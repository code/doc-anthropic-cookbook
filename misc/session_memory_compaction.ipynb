{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instant Compaction with Session Memory\n",
    "\n",
    "Traditional compaction is slow: when you hit the context limit, you wait for a summary.\n",
    "\n",
    "With **Instant compaction** the session memory is proactively generated once a soft token threshold is reached. Once the user triggers a compaction or a hard limit is reached, the summary is already available, so the user doesn't need to wait.\n",
    "\n",
    "Result: Instant compaction, no waiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "TRADITIONAL COMPACTION (slow)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Turn 1 â†’ Turn 2 â†’ Turn 3 â†’ ... â†’ Turn N â†’ CONTEXT FULL!\n",
    "                                              â”‚\n",
    "                                              â–¼\n",
    "                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                    â”‚ Generate summaryâ”‚\n",
    "                                    â”‚ ( USER WAITS !) â”‚\n",
    "                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                              â”‚\n",
    "                                              â–¼\n",
    "                                         Continue\n",
    "\n",
    "\n",
    "SESSION MEMORY COMPACTION (instant)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Turn 1 â†’ Turn 2 â†’ ... â†’ Turn K â†’ Turn K+1 â†’ ... â†’ Turn N â†’ ..  â†’ CONTEXT FULL!\n",
    "                            â”‚                         â”‚            â”‚\n",
    "                (soft threshold met:              (update          â”‚\n",
    "                   10k tokens init)                trigger)        â”‚\n",
    "                            â”‚                                      â”‚\n",
    "                            â”‚                         â”‚            â”‚\n",
    "                            â–¼                         â–¼            â”‚\n",
    "                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n",
    "                       â”‚ Update â”‚                â”‚ Update â”‚        â”‚\n",
    "                       â”‚ memory â”‚ (background)   â”‚ memory â”‚        â”‚\n",
    "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\n",
    "                            â”‚                         â”‚            â”‚\n",
    "                            â–¼                         â–¼            â–¼\n",
    "                     ğŸ“ session-memory.md â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º INSTANT SWAP!\n",
    "                       (continuously updated)\n",
    "```\n",
    "\n",
    "**Update triggers:** The first summary is generated after the initial 10k tokens. Updates can be triggered after every subsequent turn, or at periodically at natural breakpoints intervals (e.g. every ~5k tokens or 3+ tool calls)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals: writing a compaction prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a well structured session memory prompt. \n",
    "\n",
    "Some best practices include:\n",
    "- Use chain-of-thought before summarizing â€” analyze first, then output                                                                                         \n",
    "- Enumerate exactly what to preserve: file paths, code snippets, errors, user corrections                                                                      \n",
    "- Weight recency heavily â€” the end of the conversation is the active context                                                                                   \n",
    "- Require verbatim quotes for next steps to prevent task drift                                                                                                 \n",
    "- Use structured sections with token budgets per section                                                                                                       \n",
    "- Include a \"Current State\" section that always reflects the moment of compaction\n",
    "\n",
    "Some pitfalls include:\n",
    "- Vague prompts like \"summarize this conversation\" produce lossy output                                                                                        \n",
    "- Treating all messages equally loses the active working context                                                                                               \n",
    "- Paraphrasing next steps introduces subtle drift that compounds                                                                                               \n",
    "- Omitting error history causes the model to retry failed approaches                                                                                           \n",
    "- Dropping user corrections makes the model revert to old behaviors                                                                                            \n",
    "- No token limits lets one section consume the entire summary                                                                                                  \n",
    "- Summarizing for human readability instead of model continuity\n",
    "- Having the agent try to compress the results of tool calls here - this can be retrieved later if the agent needs it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "SESSION_CREATION_PROMPT = \"\"\"\n",
    "<analysis-instructions>\n",
    "Before generating your summary, analyze the transcript in <think>...</think> tags:\n",
    "1. What did the user originally request? (Exact phrasing)\n",
    "2. What actions succeeded? What failed and why?\n",
    "3. Did the user correct or redirect the assistant at any point?\n",
    "4. What was actively being worked on at the end?\n",
    "5. What tasks remain incomplete or pending?\n",
    "6. What specific details (IDs, paths, values, names) must survive compression?\n",
    "</analysis-instructions>\n",
    "\n",
    "<summary-format>\n",
    "## User Intent\n",
    "The user's original request and any refinements. Use direct quotes for key requirements.\n",
    "If the user's goal evolved during the conversation, capture that progression.\n",
    "\n",
    "## Completed Work\n",
    "Actions successfully performed. Be specific:\n",
    "- What was created, modified, or deleted\n",
    "- Exact identifiers (file paths, record IDs, URLs, names)\n",
    "- Specific values, configurations, or settings applied\n",
    "\n",
    "## Errors & Corrections\n",
    "- Problems encountered and how they were resolved\n",
    "- Approaches that failed (so they aren't retried)\n",
    "- User corrections: \"don't do X\", \"actually I meant Y\", \"that's wrong because...\"\n",
    "Capture corrections verbatimâ€”these represent learned preferences.\n",
    "\n",
    "## Active Work\n",
    "What was in progress when the session ended. Include:\n",
    "- The specific task being performed\n",
    "- Direct quotes showing exactly where work left off\n",
    "- Any partial results or intermediate state\n",
    "\n",
    "## Pending Tasks\n",
    "Remaining items the user requested that haven't been started.\n",
    "Distinguish between \"explicitly requested\" and \"implied/assumed.\"\n",
    "\n",
    "## Key References\n",
    "Important details needed to continue:\n",
    "- Identifiers: IDs, paths, URLs, names, keys\n",
    "- Values: numbers, dates, configurations, credentials (redacted)\n",
    "- Context: relevant background information, constraints, preferences\n",
    "- Citations: sources referenced during the conversation\n",
    "</summary-format>\n",
    "\n",
    "<preserve-rules>\n",
    "Always preserve when present:\n",
    "- Exact identifiers (IDs, paths, URLs, keys, names)\n",
    "- Error messages verbatim\n",
    "- User corrections and negative feedback\n",
    "- Specific values, formulas, or configurations\n",
    "- Technical constraints or requirements discovered\n",
    "- The precise state of any in-progress work\n",
    "</preserve-rules>\n",
    "\n",
    "<compression-rules>\n",
    "- Weight recent messages more heavilyâ€”the end of the transcript is the active context\n",
    "- Omit pleasantries, acknowledgments, and filler (\"Sure!\", \"Great question\")\n",
    "- Omit system context that will be re-injected separately\n",
    "- Keep each section under 500 words; condense older content to make room for recent\n",
    "- If you must cut details, preserve: user corrections > errors > active work > completed work\n",
    "</compression-rules>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional compacting example\n",
    "In traditional compaction, you generate one summary once the token threshold is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# setup, we are using haiku for demo purposes\n",
    "import anthropic\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "MODEL = \"claude-haiku-4-5-20251001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.13.11/lib/python3.13/site-packages/coconut/compiler/util.py:457: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in staticmethod() if you want to preserve the old behavior\n",
      "  result = add_action(grammar, unpack).parseWithTabs().transformString(text)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "class TraditionalCompactingChatSession:\n",
    "    \"\"\"Traditional chat session with compaction after the fact.\"\"\"\n",
    "    def __init__(self, context_limit: int = 700):\n",
    "        self.context_limit = context_limit\n",
    "        self.messages = []\n",
    "        self.current_tokens = 0\n",
    "        self.tokens_before_compaction = None  # Track for showing reduction\n",
    "        self.summary = None\n",
    "    \n",
    "    def compact(self):\n",
    "        prev_msg_count = len(self.messages)\n",
    "        self.tokens_before_compaction = self.current_tokens\n",
    "       \n",
    "        compaction_prompt = SESSION_CREATION_PROMPT + \"\\n\\nTranscript:\\n\"\n",
    "        for msg in self.messages:\n",
    "            role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "            compaction_prompt += f\"{role}: {msg['content']}\\n\"\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        response = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=1024,\n",
    "            system=\"You are a helpful assistant that summarizes conversations.\",\n",
    "            messages=[{\"role\": \"user\", \"content\": compaction_prompt}]\n",
    "        )\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        \n",
    "        # Generate new summary message\n",
    "        self.summary = response.content[0].text\n",
    "        self.messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"This session is being continued from a previous conversation. Here is the session memory: {self.summary}.Continue from where we left off.\"\"\"\n",
    "        }]\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        curr_msg_count = len(self.messages)\n",
    "        print(f\"ğŸ”„ Compaction messages: {prev_msg_count} â†’ {curr_msg_count}\")\n",
    "        print(f\"â±ï¸  Compaction time: {elapsed:.2f}s (user waiting...)\")\n",
    "    \n",
    "    def chat(self, user_message: str):\n",
    "        if self.current_tokens >= self.context_limit:\n",
    "            print(\"\\nğŸ§¹ Context limit exceeded, compacting session memory...\")\n",
    "            self.compact()\n",
    "        \n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        response = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=1024,\n",
    "            system=\"You are a helpful coding assistant. Be concise but thorough.\",\n",
    "            messages=self.messages\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.content[0].text\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "        \n",
    "        self.current_tokens = response.usage.input_tokens\n",
    "        \n",
    "        # Show token reduction if we just compacted\n",
    "        if self.tokens_before_compaction is not None:\n",
    "            reduction = self.tokens_before_compaction - self.current_tokens\n",
    "            pct = (reduction / self.tokens_before_compaction) * 100\n",
    "            print(f\"âœ… Tokens reduced: {self.tokens_before_compaction:,} â†’ {self.current_tokens:,} ({reduction:,} tokens saved, {pct:.0f}% reduction)\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "            self.tokens_before_compaction = None\n",
    "      \n",
    "        return assistant_message, response.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use of traditional compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with traditional compacting chat session...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Turn  1: Input=     48 tokens | Output=  418 tokens | Messages= 2\n",
      "\n",
      "User: Explain Python decorators with a simple example.\n",
      "Assistant: # Python Decorators Explained\n",
      "\n",
      "A **decorator** is a function that modifies or enhances another function or class without changing its source code. It wraps a function and executes code before and/or after the wrapped function runs.\n",
      "\n",
      "## Simple Example\n",
      "\n",
      "```python\n",
      "def my_decorator(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        print(\"Something before the function\")\n",
      "        result = func(*args, **kwargs)\n",
      "        print(\"Something after the function\")\n",
      "        return result\n",
      "    return wrapper\n",
      "\n",
      "@my_decorator\n",
      "def say_hello(name):\n",
      "    print(f\"Hello, {name}!\")\n",
      "\n",
      "say_hello(\"Alice\")\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "```\n",
      "Something before the function\n",
      "Hello, Alice!\n",
      "Something after the function\n",
      "```\n",
      "\n",
      "## How It Works\n",
      "\n",
      "1. `my_decorator` takes a function as input\n",
      "2. `wrapper` is a new function that:\n",
      "   - Runs code **before** calling the original function\n",
      "   - Calls the original function with `func(*args, **kwargs)`\n",
      "   - Runs code **after** the function completes\n",
      "3. The `@my_decorator` syntax is shorthand for: `say_hello = my_decorator(say_hello)`\n",
      "\n",
      "## Practical Use Case: Timing a Function\n",
      "\n",
      "```python\n",
      "import time\n",
      "\n",
      "def timer(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        start = time.time()\n",
      "        result = func(*args, **kwargs)\n",
      "        end = time.time()\n",
      "        print(f\"Executed in {end - start:.2f} seconds\")\n",
      "        return result\n",
      "    return wrapper\n",
      "\n",
      "@timer\n",
      "def slow_function():\n",
      "    time.sleep(1)\n",
      "    print(\"Done!\")\n",
      "\n",
      "slow_function()\n",
      "```\n",
      "\n",
      "Decorators are powerful for logging, authentication, timing, validation, and moreâ€”keeping your code DRY and organized.\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "============================================================\n",
      "Turn  2: Input=    479 tokens | Output=  471 tokens | Messages= 4\n",
      "\n",
      "User: Now show me a decorator that logs function arguments.\n",
      "Assistant: # Logging Arguments Decorator\n",
      "\n",
      "```python\n",
      "def log_arguments(func):\n",
      "    def wrapper(*args, **kwargs):\n",
      "        print(f\"Function: {func.__name__}\")\n",
      "        print(f\"Args: {args}\")\n",
      "        print(f\"Kwargs: {kwargs}\")\n",
      "        result = func(*args, **kwargs)\n",
      "        print(f\"Returned: {result}\\n\")\n",
      "        return result\n",
      "    return wrapper\n",
      "\n",
      "@log_arguments\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "@log_arguments\n",
      "def greet(name, greeting=\"Hello\"):\n",
      "    return f\"{greeting}, {name}!\"\n",
      "\n",
      "add(5, 3)\n",
      "greet(\"Bob\")\n",
      "greet(\"Alice\", greeting=\"Hi\")\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "```\n",
      "Function: add\n",
      "Args: (5, 3)\n",
      "Kwargs: {}\n",
      "Returned: 8\n",
      "\n",
      "Function: greet\n",
      "Args: ('Bob',)\n",
      "Kwargs: {'greeting': 'Hello'}\n",
      "Returned: Hello, Bob!\n",
      "\n",
      "Function: greet\n",
      "Args: ('Alice',)\n",
      "Kwargs: {'greeting': 'Hi'}\n",
      "Returned: Hi, Alice!\n",
      "```\n",
      "\n",
      "## Enhanced Version with functools\n",
      "\n",
      "```python\n",
      "from functools import wraps\n",
      "\n",
      "def log_arguments(func):\n",
      "    @wraps(func)  # Preserves original function metadata\n",
      "    def wrapper(*args, **kwargs):\n",
      "        print(f\"Calling {func.__name__}({args}, {kwargs})\")\n",
      "        result = func(*args, **kwargs)\n",
      "        print(f\"Result: {result}\")\n",
      "        return result\n",
      "    return wrapper\n",
      "\n",
      "@log_arguments\n",
      "def multiply(x, y):\n",
      "    \"\"\"Multiplies two numbers.\"\"\"\n",
      "    return x * y\n",
      "\n",
      "multiply(4, 5)\n",
      "print(multiply.__name__)  # Prints \"multiply\" (not \"wrapper\")\n",
      "```\n",
      "\n",
      "**Key Benefits:**\n",
      "- `@wraps(func)` preserves the original function's name, docstring, and metadata\n",
      "- Useful for debugging and understanding function calls\n",
      "- Easy to apply to multiple functions\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "============================================================\n",
      "Turn  3: Input=    963 tokens | Output=  549 tokens | Messages= 6\n",
      "\n",
      "User: How do I make a decorator that accepts parameters?\n",
      "Assistant: # Parameterized Decorators\n",
      "\n",
      "To make a decorator that accepts parameters, you need **three levels of functions**: outer function (parameters), middle function (decorator), and inner function (wrapper).\n",
      "\n",
      "## Simple Example\n",
      "\n",
      "```python\n",
      "def repeat(times):\n",
      "    def decorator(func):\n",
      "        def wrapper(*args, **kwargs):\n",
      "            results = []\n",
      "            for _ in range(times):\n",
      "                result = func(*args, **kwargs)\n",
      "                results.append(result)\n",
      "            return results\n",
      "        return wrapper\n",
      "    return decorator\n",
      "\n",
      "@repeat(times=3)\n",
      "def greet(name):\n",
      "    return f\"Hello, {name}!\"\n",
      "\n",
      "print(greet(\"Alice\"))\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "```\n",
      "['Hello, Alice!', 'Hello, Alice!', 'Hello, Alice!']\n",
      "```\n",
      "\n",
      "## How It Works\n",
      "\n",
      "1. `repeat(times=3)` is called first â†’ returns the `decorator` function\n",
      "2. `decorator` is applied to `greet` â†’ returns the `wrapper` function\n",
      "3. When `greet(\"Alice\")` is called â†’ `wrapper` executes\n",
      "\n",
      "It's equivalent to:\n",
      "```python\n",
      "greet = repeat(times=3)(greet)\n",
      "```\n",
      "\n",
      "## More Practical Example: Rate Limiter\n",
      "\n",
      "```python\n",
      "from functools import wraps\n",
      "import time\n",
      "\n",
      "def rate_limit(max_calls, time_window):\n",
      "    def decorator(func):\n",
      "        last_called = [0]\n",
      "        calls = [0]\n",
      "        \n",
      "        @wraps(func)\n",
      "        def wrapper(*args, **kwargs):\n",
      "            now = time.time()\n",
      "            if now - last_called[0] > time_window:\n",
      "                calls[0] = 0\n",
      "                last_called[0] = now\n",
      "            \n",
      "            if calls[0] >= max_calls:\n",
      "                raise Exception(f\"Rate limit exceeded: {max_calls} calls per {time_window}s\")\n",
      "            \n",
      "            calls[0] += 1\n",
      "            return func(*args, **kwargs)\n",
      "        return wrapper\n",
      "    return decorator\n",
      "\n",
      "@rate_limit(max_calls=3, time_window=10)\n",
      "def api_call():\n",
      "    print(\"API called!\")\n",
      "\n",
      "api_call()\n",
      "api_call()\n",
      "api_call()\n",
      "# api_call()  # Would raise an exception\n",
      "```\n",
      "\n",
      "**Key Point:** Always use `@wraps(func)` from `functools` to preserve metadata!\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session = TraditionalCompactingChatSession()\n",
    "\n",
    "messages = [\n",
    "    \"Explain Python decorators with a simple example.\",\n",
    "    \"Now show me a decorator that logs function arguments.\",\n",
    "    \"How do I make a decorator that accepts parameters?\",\n",
    "]\n",
    "\n",
    "print(\"Starting conversation with traditional compacting chat session...\\n\")\n",
    "\n",
    "turn_count = 0\n",
    " \n",
    "for i, message in enumerate(messages, 1):\n",
    "    response, usage = session.chat(message)\n",
    "    turn_count += 1\n",
    "    print(\n",
    "        f\"\\n{'=' * 60}\\n\"\n",
    "        f\"Turn {turn_count:2d}: Input={usage.input_tokens:7,} tokens | \"\n",
    "        f\"Output={usage.output_tokens:5,} tokens | \"\n",
    "        f\"Messages={len(session.messages):2d}\"\n",
    "    )\n",
    "    print(f\"\\nUser: {message}\\nAssistant: {response}\\n{'-'*40}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§¹ Context limit exceeded, compacting session memory...\n",
      "\n",
      "============================================================\n",
      "ğŸ”„ Compaction messages: 6 â†’ 1\n",
      "â±ï¸  Compaction time: 5.97s (user waiting...)\n",
      "âœ… Tokens reduced: 963 â†’ 721 (242 tokens saved, 25% reduction)\n",
      "============================================================\n",
      "\n",
      "Final assistant response:\n",
      "We just finished a **three-part tutorial on Python decorators**, progressing from basics to advanced:\n",
      "\n",
      "1. **Basic decorators** â€“ Simple wrapper functions using `def decorator(func)` pattern with a nested `wrapper` function\n",
      "2. **Logging decorators** â€“ Capturing function arguments and return values, introducing `@wraps(func)` from `functools` to preserve metadata\n",
      "3. **Parameterized decorators** â€“ The \"three-level nesting\" pattern where decorators themselves accept arguments:\n",
      "   - `@repeat(times=3)` â€“ calls a function multiple times\n",
      "   - `@rate_limit(max_calls, time_window)` â€“ throttles function calls\n",
      "\n",
      "The key insight was that **parameterized decorators have three nested functions**: outer (parameters) â†’ middle (decorator) â†’ inner (wrapper).\n",
      "\n",
      "---\n",
      "\n",
      "**Where would you like to go from here?** For example:\n",
      "- Practical examples of using these decorators?\n",
      "- Class-based decorators (using `__call__`)?\n",
      "- Decorators with multiple stacked decorators?\n",
      "- Real-world use cases (caching, authentication, validation)?\n",
      "- Something else?\n"
     ]
    }
   ],
   "source": [
    "response, _ = session.chat(\"What did we just talk about?\")\n",
    "print(\"\\nFinal assistant response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result the user experineces a wait time when compaction occurs. It is only a few seconds in this example, but for long context compaction, this can be must longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instant Compaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The key insight: **build the session memory in the background** so it's ready when you need it.\n",
    "\n",
    "```\n",
    "Turn 1 â†’ Turn 2 â†’ ... â†’ Turn K  â†’ Turn K+1 â†’ ... â†’ CONTEXT FULL!\n",
    "                           â”‚           â”‚                 â”‚\n",
    "                     (threshold)  (update)          INSTANT!\n",
    "                           â†“           â†“                 â†“\n",
    "                    [Background]  [Background]    [Just swap in\n",
    "                     memory init   memory update   pre-built memory]\n",
    "```\n",
    "\n",
    "The `InstantCompactingChatSession` class uses **threading** for background execution:\n",
    "1. **`threading.Thread`** - runs memory updates in background without blocking\n",
    "2. **Thread-safe state** - uses `threading.Lock` to safely update shared memory\n",
    "3. **Daemon threads** - background work doesn't prevent program exit\n",
    "4. **Instant compaction** - when context is full, just swap in the pre-built memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.13.11/lib/python3.13/site-packages/coconut/compiler/util.py:403: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in staticmethod() if you want to preserve the old behavior\n",
      "  grammar.streamline()\n",
      "/root/.pyenv/versions/3.13.11/lib/python3.13/site-packages/coconut/compiler/util.py:457: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in staticmethod() if you want to preserve the old behavior\n",
      "  result = add_action(grammar, unpack).parseWithTabs().transformString(text)\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "class InstantCompactingChatSession:\n",
    "    \"\"\"\n",
    "    Maintains session memory via incremental background updates.\n",
    "    \n",
    "    Key insight: By updating memory in the background after each turn,\n",
    "    the summary is already ready when compaction is needed - instant swap!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_limit: int = 2000,\n",
    "        min_tokens_to_init: int = 500,\n",
    "        min_tokens_between_updates: int = 300,\n",
    "    ):\n",
    "        # Thresholds\n",
    "        self.context_limit = context_limit\n",
    "        self.min_tokens_to_init = min_tokens_to_init\n",
    "        self.min_tokens_between_updates = min_tokens_between_updates\n",
    "\n",
    "        # Conversation state\n",
    "        self.messages = []\n",
    "        self.current_tokens = 0\n",
    "\n",
    "        # Session memory state\n",
    "        self.session_memory = None\n",
    "        self.last_summarized_index = 0\n",
    "        self.tokens_at_last_update = 0\n",
    "\n",
    "        # Background update tracking\n",
    "        self._update_thread: threading.Thread | None = None\n",
    "        self.last_update_time = None\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "    def chat(self, user_message: str):\n",
    "        \"\"\"Process a chat turn with background session memory updates.\"\"\"\n",
    "        if self.current_tokens >= self.context_limit:\n",
    "            self.compact()\n",
    "\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=1024,\n",
    "            system=\"You are a helpful coding assistant. Be concise but thorough.\",\n",
    "            messages=self.messages,\n",
    "        )\n",
    "\n",
    "        assistant_message = response.content[0].text\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "        self.current_tokens = response.usage.input_tokens\n",
    "\n",
    "        # KEY DIFFERENCE: Trigger background memory update if needed\n",
    "        if self._should_init_memory() or self._should_update_memory():\n",
    "            self._trigger_background_update()\n",
    "            status = \"initializing\" if self.session_memory is None else \"updating\"\n",
    "            print(f\"   [Background] Session memory {status}...\")\n",
    "\n",
    "        return assistant_message, response.usage\n",
    "    \n",
    "    # Helper methods to determine when to init/update/compact\n",
    "    def _should_init_memory(self) -> bool:\n",
    "        return (\n",
    "            self.session_memory is None\n",
    "            and self.current_tokens >= self.min_tokens_to_init\n",
    "        )\n",
    "\n",
    "    # Helper method to determine if memory should be updated\n",
    "    def _should_update_memory(self) -> bool:\n",
    "        if self.session_memory is None:\n",
    "            return False\n",
    "        tokens_since = self.current_tokens - self.tokens_at_last_update\n",
    "        return tokens_since >= self.min_tokens_between_updates\n",
    "\n",
    "    def _build_transcript(self, messages: list[dict]) -> str:\n",
    "        lines = []\n",
    "        for msg in messages:\n",
    "            role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "            lines.append(f\"{role}: {msg['content']}\")\n",
    "        return \"\\n\\n\".join(lines)\n",
    "\n",
    "    def _create_session_memory(self, messages: list[dict]) -> str:\n",
    "        \"\"\"Generate initial session memory from messages.\"\"\"\n",
    "        transcript = self._build_transcript(messages)\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=1024,\n",
    "            system=\"\"\"You are a session memory agent. Compress the conversation into a structured summary \n",
    "that preserves all information needed to continue work seamlessly. Optimize for the assistant's \n",
    "ability to continue working, not human readability.\"\"\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Conversation transcript:\n",
    "{transcript}\n",
    "\n",
    "Create session memory using these instructions:\n",
    "{SESSION_CREATION_PROMPT}\n",
    "\n",
    "First analyze in <think>...</think> tags, then output the structured summary.\"\"\",\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        return response.content[0].text\n",
    "\n",
    "    def _update_session_memory(self, new_messages: list[dict]) -> str:\n",
    "        \"\"\"Update existing session memory with new messages.\"\"\"\n",
    "        transcript = self._build_transcript(new_messages)\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=1024,\n",
    "            system=\"\"\"You are a session memory agent. Update the existing session memory with new information \n",
    "from the recent conversation. Preserve important existing details while integrating new content.\"\"\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Current session memory:\n",
    "{self.session_memory}\n",
    "\n",
    "New messages to integrate:\n",
    "{transcript}\n",
    "\n",
    "Update the session memory following these guidelines:\n",
    "{SESSION_CREATION_PROMPT}\n",
    "\n",
    "Output only the updated session memory (no analysis tags needed for updates).\"\"\",\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        return response.content[0].text\n",
    "\n",
    "    def _background_memory_update(\n",
    "        self, messages_snapshot: list[dict], snapshot_index: int, current_tokens: int\n",
    "    ):\n",
    "        \"\"\"Run session memory update in a background thread.\"\"\"\n",
    "        try:\n",
    "            if self.session_memory is None:\n",
    "                new_memory = self._create_session_memory(messages_snapshot)\n",
    "            else:\n",
    "                new_messages = messages_snapshot[self.last_summarized_index :]\n",
    "                if not new_messages:\n",
    "                    return\n",
    "                new_memory = self._update_session_memory(new_messages)\n",
    "\n",
    "            # Update state (thread-safe)\n",
    "            with self._lock:\n",
    "                self.session_memory = new_memory\n",
    "                self.last_summarized_index = snapshot_index\n",
    "                self.tokens_at_last_update = current_tokens\n",
    "                self.last_update_time = time.time()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   [Background] Error updating memory: {e}\")\n",
    "\n",
    "    def _trigger_background_update(self):\n",
    "        \"\"\"Trigger a background session memory update.\"\"\"\n",
    "        if self._update_thread is not None and self._update_thread.is_alive():\n",
    "            return\n",
    "\n",
    "        messages_snapshot = self.messages.copy()\n",
    "        snapshot_index = len(messages_snapshot)\n",
    "        current_tokens = self.current_tokens\n",
    "\n",
    "        self._update_thread = threading.Thread(\n",
    "            target=self._background_memory_update,\n",
    "            args=(messages_snapshot, snapshot_index, current_tokens),\n",
    "            daemon=True,\n",
    "        )\n",
    "        self._update_thread.start()\n",
    "\n",
    "    def wait_for_memory(self, timeout: float = 30.0):\n",
    "        \"\"\"Wait for any pending background update to complete.\"\"\"\n",
    "        if self._update_thread is not None and self._update_thread.is_alive():\n",
    "            self._update_thread.join(timeout=timeout)\n",
    "\n",
    "    def compact(self):\n",
    "        \"\"\"INSTANT compaction using pre-built session memory.\"\"\"\n",
    "        prev_msg_count = len(self.messages)\n",
    "\n",
    "        if self.session_memory is None:\n",
    "            if self._update_thread is not None and self._update_thread.is_alive():\n",
    "                print(\"   â³ Waiting for background memory update...\")\n",
    "                self._update_thread.join(timeout=30.0)\n",
    "\n",
    "            if self.session_memory is None:\n",
    "                print(\"   âš ï¸  No pre-built memory, creating synchronously...\")\n",
    "                start = time.perf_counter()\n",
    "                self.session_memory = self._create_session_memory(self.messages)\n",
    "                elapsed = time.perf_counter() - start\n",
    "                print(f\"   â±ï¸  Took {elapsed:.2f}s (but should be instant normally!)\")\n",
    "                self.last_summarized_index = len(self.messages)\n",
    "\n",
    "        unsummarized = self.messages[self.last_summarized_index :]\n",
    "\n",
    "        summary_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"This session is being continued from a previous conversation.\n",
    "\n",
    "Here is the session memory:\n",
    "{self.session_memory}\n",
    "\n",
    "Continue from where we left off.\"\"\",\n",
    "        }\n",
    "\n",
    "        self.messages = [summary_message] + unsummarized\n",
    "        self.last_summarized_index = 1\n",
    "\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"âš¡ INSTANT COMPACTION! Messages: {prev_msg_count} â†’ {len(self.messages)}\")\n",
    "        print(f\"   Kept {len(unsummarized)} unsummarized messages\")\n",
    "        print(f\"   Session memory was pre-built (no wait time!)\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use of Instant Compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INSTANT COMPACTING SESSION\n",
      "============================================================\n",
      "Session memory builds in background, so compaction is instant!\n",
      "\n",
      "\n",
      "============================================================\n",
      "Turn  1: Input=     48 tokens | Output=  393 tokens | Messages= 2\n",
      "   [Background] Session memory initializing...\n",
      "\n",
      "============================================================\n",
      "Turn  2: Input=    454 tokens | Output=  476 tokens | Messages= 4\n",
      "   [Background] Session memory initializing...\n",
      "\n",
      "============================================================\n",
      "Turn  3: Input=    943 tokens | Output=  564 tokens | Messages= 6\n"
     ]
    }
   ],
   "source": [
    "# Low thresholds for demo - in production you'd use higher values\n",
    "session = InstantCompactingChatSession(\n",
    "    context_limit=700,\n",
    "    min_tokens_to_init=200,\n",
    "    min_tokens_between_updates=150,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    \"Explain Python decorators with a simple example.\",\n",
    "    \"Now show me a decorator that logs function arguments.\",\n",
    "    \"How do I make a decorator that accepts parameters?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INSTANT COMPACTING SESSION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Session memory builds in background, so compaction is instant!\\n\")\n",
    "\n",
    "turn_count= 0\n",
    "for i, message in enumerate(messages, 1):\n",
    "    turn_count += 1\n",
    "    response, usage = session.chat(message)\n",
    "    \n",
    "    memory_status = \"ready\" if session.session_memory else \"not yet\"\n",
    "    print(\n",
    "        f\"\\n{'=' * 60}\\n\"\n",
    "        f\"Turn {turn_count:2d}: Input={usage.input_tokens:7,} tokens | \"\n",
    "        f\"Output={usage.output_tokens:5,} tokens | \"\n",
    "        f\"Messages={len(session.messages):2d}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âš¡ INSTANT COMPACTION! Messages: 6 â†’ 3\n",
      "   Kept 2 unsummarized messages\n",
      "   Session memory was pre-built (no wait time!)\n",
      "============================================================\n",
      "   [Background] Session memory updating...\n",
      "\n",
      "Final assistant response:\n",
      "We just covered **parameterized decorators** â€” how to create decorators that accept their own parameters.\n",
      "\n",
      "The key concept: you need **three levels of nesting** instead of two:\n",
      "\n",
      "1. **Outer function** â€” accepts decorator parameters (e.g., `repeat(times=3)`)\n",
      "2. **Middle function** â€” the decorator itself (takes the function to decorate)\n",
      "3. **Inner function** â€” the wrapper (executes the actual behavior)\n",
      "\n",
      "I showed two examples:\n",
      "- **`@repeat(times=3)`** â€” runs a function multiple times\n",
      "- **`@rate_limit(max_calls=3, time_window=10)`** â€” prevents function calls exceeding a rate limit\n",
      "\n",
      "This was a follow-up to our earlier conversation about Python decorators and logging decorators.\n"
     ]
    }
   ],
   "source": [
    "response, _ = session.chat(\"What did we just talk about?\")\n",
    "print(\"\\nFinal assistant response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARISON: Traditional vs Instant Compaction\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š TRADITIONAL COMPACTION:\n",
      "----------------------------------------\n",
      "  Turn 1: 48 tokens\n",
      "  Turn 2: 444 tokens\n",
      "  Turn 3: 915 tokens\n",
      "\n",
      "============================================================\n",
      "ğŸ”„ Compaction triggered! Messages: 6 â†’ 1\n",
      "â±ï¸  Compaction time: 5.69s (user waiting...)\n",
      "\n",
      "âš¡ INSTANT COMPACTION:\n",
      "----------------------------------------\n",
      "  Turn 1: 48 tokens | Memory: building...\n",
      "   [Background] Session memory initializing...\n",
      "  Turn 2: 452 tokens | Memory: building...\n",
      "   [Background] Session memory initializing...\n",
      "  Turn 3: 1,024 tokens | Memory: building...\n",
      "\n",
      "============================================================\n",
      "âš¡ INSTANT COMPACTION! Messages: 6 â†’ 3\n",
      "   Kept 2 unsummarized messages\n",
      "   Session memory was pre-built (no wait time!)\n",
      "============================================================\n",
      "\n",
      "======================================================================\n",
      "RESULTS:\n",
      "  Traditional compaction time: 5.69s (user waiting)\n",
      "  Instant compaction time:     0.0002s (just a swap!)\n",
      "  Speedup: 5692x faster\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Side-by-side comparison: Traditional vs Instant compaction\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: Traditional vs Instant Compaction\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "messages = [\n",
    "    \"Explain Python decorators with a simple example.\",\n",
    "    \"Now show me a decorator that logs function arguments.\",\n",
    "    \"How do I make a decorator that accepts parameters?\",\n",
    "]\n",
    "\n",
    "# Traditional approach\n",
    "print(\"\\nğŸ“Š TRADITIONAL COMPACTION:\")\n",
    "print(\"-\" * 40)\n",
    "traditional = TraditionalCompactingChatSession(context_limit=500)\n",
    "\n",
    "for i, msg in enumerate(messages, 1):\n",
    "    response, usage = traditional.chat(msg)\n",
    "    print(f\"  Turn {i}: {usage.input_tokens:,} tokens\")\n",
    "\n",
    "# Force a compaction to measure time\n",
    "start = time.perf_counter()\n",
    "traditional.compact()\n",
    "traditional_compaction_time = time.perf_counter() - start\n",
    "\n",
    "# Instant approach  \n",
    "print(\"\\nâš¡ INSTANT COMPACTION:\")\n",
    "print(\"-\" * 40)\n",
    "instant = InstantCompactingChatSession(\n",
    "    context_limit=500,\n",
    "    min_tokens_to_init=100,\n",
    "    min_tokens_between_updates=100,\n",
    ")\n",
    "\n",
    "for i, msg in enumerate(messages, 1):\n",
    "    response, usage = instant.chat(msg)\n",
    "    print(f\"  Turn {i}: {usage.input_tokens:,} tokens | Memory: {'ready' if instant.session_memory else 'building...'}\")\n",
    "\n",
    "# Wait for background to finish\n",
    "instant.wait_for_memory()\n",
    "\n",
    "# Measure instant compaction time\n",
    "start = time.perf_counter()\n",
    "instant.compact()\n",
    "instant_compaction_time = time.perf_counter() - start\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTS:\")\n",
    "print(f\"  Traditional compaction time: {traditional_compaction_time:.2f}s (user waiting)\")\n",
    "print(f\"  Instant compaction time:     {instant_compaction_time:.4f}s (just a swap!)\")\n",
    "print(f\"  Speedup: {traditional_compaction_time/max(instant_compaction_time, 0.001):.0f}x faster\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Adding Prompt Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The background updates can be made **~10x cheaper** by using prompt caching. The trick:\n",
    "1. Pass the **full conversation** to the background summarizer\n",
    "2. Add `cache_control` markers so subsequent requests hit the cache\n",
    "3. Only the new \"summarize this\" instruction is billed at full price\n",
    "\n",
    "```\n",
    "Main chat:         [System + Turn 1 + Turn 2 + ... + Turn N]\n",
    "                                    â†“\n",
    "                              (cached automatically)\n",
    "                              \n",
    "Background update: [System + Turn 1 + Turn 2 + ... + Turn N] + [Summarize instruction]\n",
    "                              â†‘                                        â†‘\n",
    "                         CACHE HIT! (10x cheaper)              Only this is billed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the Caching Works\n",
    "\n",
    "The key is in `_add_cache_control()` and `_create_session_memory_cached()`:\n",
    "\n",
    "```python\n",
    "# 1. Mark the last conversation message with cache_control\n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\n",
    "        \"type\": \"text\",\n",
    "        \"text\": msg[\"content\"],\n",
    "        \"cache_control\": {\"type\": \"ephemeral\"}  # <-- This creates a cache breakpoint\n",
    "    }]\n",
    "}\n",
    "\n",
    "# 2. Also mark the system prompt\n",
    "system=[{\n",
    "    \"type\": \"text\",\n",
    "    \"text\": \"You are a session memory agent...\",\n",
    "    \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "}]\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- The first background update creates a cache entry for `[System + Messages]`\n",
    "- Subsequent updates with the same message prefix get **cache hits**\n",
    "- Only the new summarization instruction is billed at full price\n",
    "- Cache entries have a 5-minute TTL, so rapid updates benefit most\n",
    "\n",
    "**Cost math:**\n",
    "- Without caching: 5,000 tokens Ã— $3.00/1M = $0.015 per update\n",
    "- With caching: 500 new tokens Ã— $3.00/1M + 4,500 cached Ã— $0.30/1M = $0.00285\n",
    "- **Savings: ~80%** on background summarization costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class CachedInstantCompactingSession:\n",
    "    \"\"\"\n",
    "    Session memory with prompt caching for cheaper background updates.\n",
    "    \n",
    "    Key optimization: By passing the full conversation with cache_control markers,\n",
    "    background summarization requests get cache hits on 90%+ of input tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_limit: int = 2000,\n",
    "        min_tokens_to_init: int = 500,\n",
    "        min_tokens_between_updates: int = 300,\n",
    "        system_prompt: str = \"You are a helpful coding assistant. Be concise but thorough.\",\n",
    "    ):\n",
    "        self.context_limit = context_limit\n",
    "        self.min_tokens_to_init = min_tokens_to_init\n",
    "        self.min_tokens_between_updates = min_tokens_between_updates\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "        self.messages = []\n",
    "        self.current_tokens = 0\n",
    "\n",
    "        self.session_memory = None\n",
    "        self.last_summarized_index = 0\n",
    "        self.tokens_at_last_update = 0\n",
    "\n",
    "        self._update_thread = None\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "        # Track cache stats\n",
    "        self.total_cache_read = 0\n",
    "        self.total_cache_created = 0\n",
    "        self.total_input_tokens = 0\n",
    "\n",
    "    def _should_init_memory(self) -> bool:\n",
    "        return self.session_memory is None and self.current_tokens >= self.min_tokens_to_init\n",
    "\n",
    "    def _should_update_memory(self) -> bool:\n",
    "        if self.session_memory is None:\n",
    "            return False\n",
    "        return (self.current_tokens - self.tokens_at_last_update) >= self.min_tokens_between_updates\n",
    "\n",
    "    def _should_compact(self) -> bool:\n",
    "        return self.current_tokens >= self.context_limit\n",
    "\n",
    "    def _add_cache_control(self, messages: list[dict]) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Add cache_control markers to messages for prompt caching.\n",
    "        \n",
    "        Strategy: Mark the last message with cache_control so the entire\n",
    "        conversation prefix gets cached for subsequent requests.\n",
    "        \"\"\"\n",
    "        if not messages:\n",
    "            return messages\n",
    "\n",
    "        cached_messages = []\n",
    "        for i, msg in enumerate(messages):\n",
    "            if i == len(messages) - 1:\n",
    "                # Last message: add cache_control marker\n",
    "                cached_messages.append({\n",
    "                    \"role\": msg[\"role\"],\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": msg[\"content\"],\n",
    "                            \"cache_control\": {\"type\": \"ephemeral\"},\n",
    "                        }\n",
    "                    ],\n",
    "                })\n",
    "            else:\n",
    "                cached_messages.append(msg)\n",
    "\n",
    "        return cached_messages\n",
    "\n",
    "    def _create_session_memory_cached(self, messages: list[dict]) -> tuple[str, dict]:\n",
    "        \"\"\"\n",
    "        Generate session memory using the FULL conversation with caching.\n",
    "        \n",
    "        This passes the entire conversation + summarize instruction, so subsequent\n",
    "        calls with the same conversation prefix will hit the cache.\n",
    "        \"\"\"\n",
    "        # Build conversation with cache marker on last message\n",
    "        cached_messages = self._add_cache_control(messages)\n",
    "\n",
    "        # Add the summarization instruction as a new user message\n",
    "        cached_messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Based on our conversation above, create a session memory summary.\n",
    "\n",
    "{SESSION_CREATION_PROMPT}\n",
    "\n",
    "Output the structured summary directly.\"\"\",\n",
    "        })\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=1024,\n",
    "            system=[\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"\"\"You are a session memory agent. When asked, compress the conversation \n",
    "into a structured summary that preserves all information needed to continue work seamlessly.\"\"\",\n",
    "                    \"cache_control\": {\"type\": \"ephemeral\"},\n",
    "                }\n",
    "            ],\n",
    "            messages=cached_messages,\n",
    "        )\n",
    "\n",
    "        # Extract cache stats\n",
    "        cache_stats = {\n",
    "            \"cache_read\": getattr(response.usage, \"cache_read_input_tokens\", 0),\n",
    "            \"cache_created\": getattr(response.usage, \"cache_creation_input_tokens\", 0),\n",
    "            \"input_tokens\": response.usage.input_tokens,\n",
    "        }\n",
    "\n",
    "        return response.content[0].text, cache_stats\n",
    "\n",
    "    def _background_memory_update(\n",
    "        self, messages_snapshot: list[dict], snapshot_index: int, current_tokens: int\n",
    "    ):\n",
    "        \"\"\"Run cached session memory update in background thread.\"\"\"\n",
    "        try:\n",
    "            new_memory, cache_stats = self._create_session_memory_cached(messages_snapshot)\n",
    "\n",
    "            with self._lock:\n",
    "                self.session_memory = new_memory\n",
    "                self.last_summarized_index = snapshot_index\n",
    "                self.tokens_at_last_update = current_tokens\n",
    "                self.total_cache_read += cache_stats[\"cache_read\"]\n",
    "                self.total_cache_created += cache_stats[\"cache_created\"]\n",
    "                self.total_input_tokens += cache_stats[\"input_tokens\"]\n",
    "\n",
    "            # Show cache performance\n",
    "            if cache_stats[\"cache_read\"] > 0:\n",
    "                pct = (cache_stats[\"cache_read\"] / cache_stats[\"input_tokens\"]) * 100\n",
    "                print(f\"   [Cache] {cache_stats['cache_read']:,} read ({pct:.0f}% hit rate)\")\n",
    "            else:\n",
    "                print(f\"   [Cache] {cache_stats['cache_created']:,} created (first request)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   [Background] Error: {e}\")\n",
    "\n",
    "    def _trigger_background_update(self):\n",
    "        if self._update_thread is not None and self._update_thread.is_alive():\n",
    "            return\n",
    "\n",
    "        self._update_thread = threading.Thread(\n",
    "            target=self._background_memory_update,\n",
    "            args=(self.messages.copy(), len(self.messages), self.current_tokens),\n",
    "            daemon=True,\n",
    "        )\n",
    "        self._update_thread.start()\n",
    "\n",
    "    def wait_for_memory(self, timeout: float = 30.0):\n",
    "        if self._update_thread is not None and self._update_thread.is_alive():\n",
    "            self._update_thread.join(timeout=timeout)\n",
    "\n",
    "    def compact(self):\n",
    "        prev_msg_count = len(self.messages)\n",
    "\n",
    "        if self.session_memory is None:\n",
    "            if self._update_thread is not None and self._update_thread.is_alive():\n",
    "                print(\"   â³ Waiting for background update...\")\n",
    "                self._update_thread.join(timeout=30.0)\n",
    "\n",
    "            if self.session_memory is None:\n",
    "                print(\"   âš ï¸  Creating memory synchronously...\")\n",
    "                self.session_memory, _ = self._create_session_memory_cached(self.messages)\n",
    "                self.last_summarized_index = len(self.messages)\n",
    "\n",
    "        unsummarized = self.messages[self.last_summarized_index :]\n",
    "\n",
    "        self.messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Session memory:\\n{self.session_memory}\\n\\nContinue from where we left off.\",\n",
    "            }\n",
    "        ] + unsummarized\n",
    "        self.last_summarized_index = 1\n",
    "\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"âš¡ INSTANT COMPACTION! Messages: {prev_msg_count} â†’ {len(self.messages)}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "\n",
    "    def chat(self, user_message: str):\n",
    "        if self._should_compact():\n",
    "            self.compact()\n",
    "\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=1024,\n",
    "            system=self.system_prompt,\n",
    "            messages=self.messages,\n",
    "        )\n",
    "\n",
    "        assistant_message = response.content[0].text\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "        self.current_tokens = response.usage.input_tokens\n",
    "\n",
    "        if self._should_init_memory() or self._should_update_memory():\n",
    "            self._trigger_background_update()\n",
    "            print(f\"   [Background] Updating session memory...\")\n",
    "\n",
    "        return assistant_message, response.usage\n",
    "\n",
    "    def get_cache_savings(self) -> dict:\n",
    "        \"\"\"Calculate cost savings from caching.\"\"\"\n",
    "        if self.total_input_tokens == 0:\n",
    "            return {\"savings_pct\": 0, \"effective_rate\": 0}\n",
    "\n",
    "        # Cache reads are 10x cheaper than regular input\n",
    "        regular_cost = self.total_input_tokens\n",
    "        actual_cost = (self.total_input_tokens - self.total_cache_read) + (self.total_cache_read * 0.1)\n",
    "        savings_pct = ((regular_cost - actual_cost) / regular_cost) * 100 if regular_cost > 0 else 0\n",
    "\n",
    "        return {\n",
    "            \"total_input\": self.total_input_tokens,\n",
    "            \"cache_read\": self.total_cache_read,\n",
    "            \"cache_created\": self.total_cache_created,\n",
    "            \"savings_pct\": savings_pct,\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coconut",
   "language": "coconut",
   "name": "coconut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".coco",
   "mimetype": "text/x-python3",
   "name": "coconut",
   "pygments_lexer": "coconut",
   "version": "3.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
