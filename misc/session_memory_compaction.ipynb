{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Session Memory Compaction\n",
        "\n",
        "Long-running conversations with Claude can exceed context limits, causing loss of important information. Whether you're building a coding assistant, creative writing tool, or customer service agent, managing session memory is critical for maintaining continuity and quality.\n",
        "\n",
        "This cookbook teaches you how to **proactively manage session memory** to avoid jarring context limit interruptions. Unlike reactive approaches that wait until the context is full, you'll learn to build session memory in the background so compaction is instant when needed.\n",
        "\n",
        "**Related:** For automatic SDK-based compaction in agentic workflows, see [Automatic Context Compaction](../tool_use/automatic-context-compaction.ipynb). This cookbook focuses on manual control patterns for conversational applications.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this cookbook, you will be able to:\n",
        "\n",
        "- Write effective session memory prompts that preserve critical context across compaction events\n",
        "- Implement **instant compaction** using background threading to eliminate user wait time\n",
        "- Apply prompt caching to reduce the cost of background memory updates by ~80%\n",
        "- Choose appropriate compaction strategies (traditional vs. instant) based on your use case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites and Setup\n",
        "\n",
        "Before following this guide, ensure you have:\n",
        "\n",
        "**Required Knowledge**\n",
        "- Basic understanding of Claude API usage and message formatting\n",
        "- Familiarity with Python threading concepts (helpful but not required)\n",
        "\n",
        "**Required Tools**\n",
        "- Python 3.10 or higher\n",
        "- Anthropic API key\n",
        "- Anthropic SDK\n",
        "\n",
        "### Installation\n",
        "\n",
        "First, install the required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "%%capture\n%pip install -U anthropic python-dotenv",
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import anthropic\nfrom anthropic.types import MessageParam, TextBlockParam\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = anthropic.Anthropic()\nMODEL = \"claude-sonnet-4-5-20250929\"",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def truncate_response(text: str, max_lines: int = 15) -> str:\n    \"\"\"Truncate long responses for cleaner output display.\"\"\"\n    lines = text.strip().split(\"\\n\")\n    if len(lines) <= max_lines:\n        return text\n    return \"\\n\".join(lines[:max_lines]) + f\"\\n... ({len(lines) - max_lines} more lines)\"\n\n\ndef remove_thinking_blocks(text: str) -> tuple[str, str]:\n    \"\"\"Remove <think>...</think> blocks from the text.\"\"\"\n    import re\n\n    matches = re.findall(r\"<think>.*?</think>\", text, flags=re.DOTALL)\n    cleaned = re.sub(r\"<think>.*?</think>\\s*\", \"\", text, flags=re.DOTALL).strip()\n    return cleaned, \"\".join(matches)\n\n\ndef add_cache_control(messages: list[dict]) -> list[MessageParam]:\n    \"\"\"Add cache_control to the last user message for prompt caching.\n\n    For prompt caching to work, the message prefix structure must be identical between requests.\n    All messages are converted to list format for consistency, and cache_control is placed on\n    the last user message to match the standard API call pattern.\n    \"\"\"\n    cached_messages: list[MessageParam] = []\n    last_user_idx = None\n\n    # Find last user message index\n    for i, msg in enumerate(messages):\n        if msg[\"role\"] == \"user\":\n            last_user_idx = i\n\n    for i, msg in enumerate(messages):\n        content = msg[\"content\"]\n        text = content if isinstance(content, str) else content[0][\"text\"]\n\n        content_block: TextBlockParam = {\"type\": \"text\", \"text\": text}\n        if i == last_user_idx:\n            content_block[\"cache_control\"] = {\"type\": \"ephemeral\"}\n\n        cached_messages.append({\"role\": msg[\"role\"], \"content\": [content_block]})\n\n    return cached_messages\n\n\ndef estimate_tokens(text: str) -> int:\n    \"\"\"Rudimentary token estimation: 1 token per 4 characters.\"\"\"\n    return len(text) // 4"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "SESSION_MEMORY_PROMPT = \"\"\"\n",
        "Compress the conversation into a structured summary\n",
        "that preserves all information needed to continue work seamlessly. Optimize for the assistant's\n",
        "ability to continue working, not human readability.\n",
        "\n",
        "<analysis-instructions>\n",
        "Before generating your summary, analyze the transcript in <think>...</think> tags:\n",
        "1. What did the user originally request? (Exact phrasing)\n",
        "2. What actions succeeded? What failed and why?\n",
        "3. Did the user correct or redirect the assistant at any point?\n",
        "4. What was actively being worked on at the end?\n",
        "5. What tasks remain incomplete or pending?\n",
        "6. What specific details (IDs, paths, values, names) must survive compression?\n",
        "</analysis-instructions>\n",
        "\n",
        "<summary-format>\n",
        "## User Intent\n",
        "The user's original request and any refinements. Use direct quotes for key requirements.\n",
        "If the user's goal evolved during the conversation, capture that progression.\n",
        "\n",
        "## Completed Work\n",
        "Actions successfully performed. Be specific:\n",
        "- What was created, modified, or deleted\n",
        "- Exact identifiers (file paths, record IDs, URLs, names)\n",
        "- Specific values, configurations, or settings applied\n",
        "\n",
        "## Errors & Corrections\n",
        "- Problems encountered and how they were resolved\n",
        "- Approaches that failed (so they aren't retried)\n",
        "- User corrections: \"don't do X\", \"actually I meant Y\", \"that's wrong because...\"\n",
        "Capture corrections verbatim‚Äîthese represent learned preferences.\n",
        "\n",
        "## Active Work\n",
        "What was in progress when the session ended. Include:\n",
        "- The specific task being performed\n",
        "- Direct quotes showing exactly where work left off\n",
        "- Any partial results or intermediate state\n",
        "\n",
        "## Pending Tasks\n",
        "Remaining items the user requested that haven't been started.\n",
        "Distinguish between \"explicitly requested\" and \"implied/assumed.\"\n",
        "\n",
        "## Key References\n",
        "Important details needed to continue:\n",
        "- Identifiers: IDs, paths, URLs, names, keys\n",
        "- Values: numbers, dates, configurations, credentials (redacted)\n",
        "- Context: relevant background information, constraints, preferences\n",
        "- Citations: sources referenced during the conversation\n",
        "</summary-format>\n",
        "\n",
        "<preserve-rules>\n",
        "Always preserve when present:\n",
        "- Exact identifiers (IDs, paths, URLs, keys, names)\n",
        "- Error messages verbatim\n",
        "- User corrections and negative feedback\n",
        "- Specific values, formulas, or configurations\n",
        "- Technical constraints or requirements discovered\n",
        "- The precise state of any in-progress work\n",
        "</preserve-rules>\n",
        "\n",
        "<compression-rules>\n",
        "- Weight recent messages more heavily‚Äîthe end of the transcript is the active context\n",
        "- Omit pleasantries, acknowledgments, and filler (\"Sure!\", \"Great question\")\n",
        "- Omit system context that will be re-injected separately\n",
        "- Keep each section under 500 words; condense older content to make room for recent\n",
        "- If you must cut details, preserve: user corrections > errors > active work > completed work\n",
        "</compression-rules>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Code example using traditional compacting\n",
        "In traditional compaction, you generate one summary once the token threshold is reached.\n",
        "Traditional compaction is slow: when you hit the context limit, you wait for a summary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "```\n",
        "TRADITIONAL COMPACTION (slow)\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "Turn 1 ‚Üí Turn 2 ‚Üí Turn 3 ‚Üí ... ‚Üí Turn N ‚Üí CONTEXT FULL!\n",
        "                                              ‚îÇ\n",
        "                                              ‚ñº\n",
        "                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                                    ‚îÇ Generate summary‚îÇ\n",
        "                                    ‚îÇ ( USER WAITS !) ‚îÇ\n",
        "                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                                              ‚îÇ\n",
        "                                              ‚ñº\n",
        "                                         Continue\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "class TraditionalCompactingChatSession:\n",
        "    \"\"\"Traditional chat session with compaction after the fact.\"\"\"\n",
        "\n",
        "    def __init__(self, system_message=\"You are a helpful assistant\", context_limit: int = 10000):\n",
        "        self.system_message = system_message\n",
        "        self.context_limit = context_limit  # the point at which the conversation is compacted so it does not exceed model limits.\n",
        "        self.messages = []\n",
        "        self.current_context_window_tokens = 0\n",
        "        self.summary = None\n",
        "\n",
        "    def chat(self, user_message: str):\n",
        "        # In traditional compaction, we check if we need to compact when the user sends a message. NOT IDEAL!\n",
        "        if self.current_context_window_tokens >= self.context_limit:\n",
        "            print(\n",
        "                f\"\\nüßπ Context window at {self.current_context_window_tokens} tokens. Limit exceeded, compacting session memory...\"\n",
        "            )\n",
        "            self.compact()  # compacts everything before the new user message\n",
        "\n",
        "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "        print(f\"\\nUser: {user_message}\")\n",
        "\n",
        "        response = client.messages.create(\n",
        "            model=MODEL,\n",
        "            max_tokens=3500,\n",
        "            system=self.system_message,\n",
        "            messages=add_cache_control(self.messages),\n",
        "        )\n",
        "        assistant_message = response.content[0].text\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "\n",
        "        print(f\"\\nAssistant: \\n{truncate_response(assistant_message, max_lines=15)}\")\n",
        "\n",
        "        # approximate current token count in the conversation before the next user message\n",
        "        cache_read = getattr(response.usage, \"cache_read_input_tokens\", 0) or 0\n",
        "        total_input = response.usage.input_tokens + cache_read\n",
        "        self.current_context_window_tokens = total_input + response.usage.output_tokens\n",
        "\n",
        "        print(\n",
        "            f\"Input={total_input:,}, Prompt cached used= {cache_read > 0} | \"\n",
        "            f\"Output={response.usage.output_tokens:,} | \"\n",
        "            f\"Messages={len(self.messages)}\"\n",
        "        )\n",
        "        return assistant_message, response.usage\n",
        "\n",
        "    def compact(self):\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "        response = client.messages.create(\n",
        "            model=MODEL,\n",
        "            max_tokens=5000,\n",
        "            system=self.system_message,  # Same as main chat for cache sharing\n",
        "            messages=add_cache_control(self.messages)\n",
        "            + [{\"role\": \"user\", \"content\": SESSION_MEMORY_PROMPT}],\n",
        "        )\n",
        "        elapsed = time.perf_counter() - start_time\n",
        "\n",
        "        # Generate new summary message\n",
        "        self.summary, removed_text = remove_thinking_blocks(\n",
        "            response.content[0].text\n",
        "        )  # clean up any <think> blocks because they are not needed in the session memory\n",
        "        approximate_summary_tokens = response.usage.output_tokens - round(\n",
        "            len(removed_text) / 4\n",
        "        )  # rough estimate of tokens removed from summary\n",
        "\n",
        "        # Replace prior messages with new summary message\n",
        "        self.messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"This session is being continued from a previous conversation. Here is the session memory: {self.summary}.Continue from where we left off.\"\"\",\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Show token reduction if we just compacted\n",
        "        reduction = self.current_context_window_tokens - approximate_summary_tokens\n",
        "        pct = (reduction / self.current_context_window_tokens) * 100\n",
        "\n",
        "        print(f\"\\n{'-' * 60}\")\n",
        "        print(\"üìù New session memory created.\")\n",
        "        print(\n",
        "            f\"‚úÖ Tokens reduced: {self.current_context_window_tokens:,} ‚Üí {approximate_summary_tokens:.0f} ({reduction:,} tokens saved, {pct:.0f}% reduction)\"\n",
        "        )\n",
        "        print(f\"‚è±Ô∏è Compaction time: {elapsed:.2f}s (user waiting...)\")\n",
        "        print(f\" Cache used: {getattr(response.usage, 'cache_read_input_tokens', 0) > 0}\")\n",
        "        print(f\"{'-' * 60}\")\n",
        "\n",
        "        # Update token count to reflect compacted state\n",
        "        self.current_context_window_tokens = approximate_summary_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we simulate a conversation between an author and an LLM that helps write stories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a short story writer who helps authors develop their ideas into compelling narratives.\n",
        "\n",
        "## What You Do\n",
        "\n",
        "**Plot Development**\n",
        "- Help authors work through story structure, pacing, and narrative arc\n",
        "- Identify plot holes, inconsistencies, or missed opportunities\n",
        "- Suggest ways to raise stakes, add tension, or deepen conflict\n",
        "- Brainstorm twists, resolutions, and scene transitions\n",
        "\n",
        "**Character Development**\n",
        "- Develop backstories, motivations, and internal conflicts\n",
        "- Ensure characters have distinct voices and consistent behavior\n",
        "- Explore character relationships and how they drive the plot\n",
        "- Help authors understand what their characters want vs. what they need\n",
        "\n",
        "**Drafting**\n",
        "- Write short stories or scenes based on the author's ideas and direction\n",
        "- Match tone, genre conventions, and stylistic preferences\n",
        "- Show rather than tell when bringing scenes to life\n",
        "- Craft dialogue that reveals character and advances plot\n",
        "\n",
        "## How You Work\n",
        "- You are the lead writer. When you disagree with a creative choice, say so respectfully, but ultimately defer to what the author wants.\n",
        "- DO NOT ask the user to provide more context or clarify their request. Assume you have enough information to proceed.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting conversation...\n",
            "\n",
            "==============================================\n",
            "Turn 1:\n",
            "\n",
            "\n",
            "User: I want to create a story about a young detective solving a mysterious case in a small town. Generate 3 well throught out plot ideas for me to consider.\n",
            "\n",
            "Assistant: \n",
            "# Three Mystery Plot Ideas\n",
            "\n",
            "## 1. **The Drowning Season**\n",
            "\n",
            "**Setup:** Twenty-three-year-old Detective Maya Reeves returns to her hometown of Millbrook after five years away, assigned to investigate what locals insist is just another tragic drowning at the quarry‚Äîthe third one this summer. The sheriff's department is understaffed, and Maya's the only one taking it seriously.\n",
            "\n",
            "**The Mystery:** Each victim was found at dawn, fully clothed, with no water in their lungs. They all have the same strange detail: fresh soil under their fingernails, though the quarry is surrounded by rock. Maya discovers all three victims had recently started remembering \"lost time\" from their childhood‚Äîgaps of hours or days they couldn't explain. The town has a secret: twenty years ago, a local therapist ran a controversial \"memory recovery\" program for troubled kids. Maya was one of those kids, and she's starting to remember why she really left town.\n",
            "\n",
            "**What Makes It Work:** The murders are connected to buried trauma, both literal and psychological. The killer believes they're saving the victims from remembering something terrible. Maya must solve the case while confronting her own suppressed memories, never sure if what she's remembering is real or if she might be the next target.\n",
            "\n",
            "---\n",
            "\n",
            "## 2. **The Lighthouse Keeper's Daughter**\n",
            "\n",
            "**Setup:** Rookie detective James Park is sent to the coastal town of Beacon's Rest to investigate the disappearance of fifteen-year-old Clare Whitmore, daughter of the reclusive lighthouse keeper. She vanished during the annual Founder's Day festival three days ago. The town is cooperative but oddly unsurprised‚Äîseven girls have disappeared from Beacon's Rest over the past forty years, always during Founder's Day, always at age fifteen, and always without a trace.\n",
            "... (18 more lines)\n",
            "Input=318, Prompt cached used= False | Output=988 | Messages=2\n",
            "==============================================\n",
            "Turn 2:\n",
            "\n",
            "\n",
            "User: I don't like those ideas, can you think of one plot something more unique and unexpected?\n",
            "\n",
            "Assistant: \n",
            "# The Cartographer's Cipher\n",
            "\n",
            "**Setup:** Detective Nora Alike, 24, takes her first solo case in the dying mining town of Shepherdstown (population 847 and falling). An elderly cartographer named Eugene Fisk was found dead in his cluttered workshop, officially ruled a heart attack. But his daughter insists something's wrong: her father spent his final month frantically updating a massive hand-drawn map of the town‚Äîadding buildings that don't exist, removing ones that do, and marking dozens of red X's in seemingly random locations.\n",
            "\n",
            "**The Twist:** Nora starts investigating the X's out of curiosity. The first one leads to an old park bench‚Äîunderneath it, she finds a coffee can containing $2,300 in cash and a note: \"For the Hendersons' mortgage, June 1987.\" The second X marks a spot behind the defunct movie theater where she uncovers an envelope with photographs proving the former mayor's affair (the scandal that ended his career). The third X reveals a time capsule with a child's confession to starting the fire that burned down the elementary school forty years ago.\n",
            "\n",
            "**The Real Mystery:** Eugene wasn't murdered‚Äîbut he wasn't mapping the town as it is. He was mapping every secret buried within it, physical and metaphorical. For sixty years, he'd been Shepherdstown's unofficial confessor. People would visit his workshop and tell him things they couldn't tell anyone else, and he'd mark them on his map in an elaborate code only he understood. In his final month, knowing he was dying, he decoded everything‚Äîcreating a map that, if made public, would destroy half the town.\n",
            "\n",
            "**The Investigation:** Nora realizes someone is also following the map. Items are being dug up, locations disturbed. She races to decipher Eugene's system before this unknown person does. But the deeper she digs, the more she uncovers: covered-up deaths, stolen inheritances, a hit-and-run from 1992, evidence of who really embezzled the union funds that closed the mine. Every secret connects to another. The town isn't dying by accident‚Äîit's been poisoned from within by decades of buried truth.\n",
            "\n",
            "**The Impossible Choice:** The person following the map is Eugene's daughter, who wants to expose everything and burn the town's lies to the ground. She blames these secrets‚Äîand the people who keep them‚Äîfor turning Shepherdstown into a hollowed-out shell. Nora must decide: help her complete her father's final work and reveal every terrible truth, or destroy the map and let sleeping sins lie. The case becomes about whether a detective's job is to uncover the truth or to protect the living from what the truth will do to them.\n",
            "\n",
            "**What Makes It Unique:** \n",
            "- No traditional villain‚Äîjust a town full of people who made choices\n",
            "- The \"victim\" weaponized his own death\n",
            "... (5 more lines)\n",
            "Input=1,328, Prompt cached used= False | Output=688 | Messages=4\n",
            "==============================================\n",
            "Turn 3:\n",
            "\n",
            "\n",
            "User: Ok I like it. Can you help me develop the main character's backstory and motivations?\n",
            "\n",
            "Assistant: \n",
            "# Nora Alike: Character Deep Dive\n",
            "\n",
            "## Background\n",
            "\n",
            "**Age:** 24, but people consistently guess younger‚Äîwhich she hates. She's been fighting to be taken seriously her entire life.\n",
            "\n",
            "**Origin:** Grew up in a suburb of a mid-sized city, the youngest of four siblings by seven years (she was the \"accident baby\"). Her parents were older, tired, and emotionally checked out by the time she came along. Her siblings had already left home, so she essentially raised herself in a house that felt like a museum to other people's childhoods.\n",
            "\n",
            "**The Formative Incident:** When Nora was sixteen, her oldest brother Martin disappeared. Just stopped coming to family dinners, didn't return calls. Her parents were worried but passive‚Äî\"He's an adult, he'll reach out when he's ready.\" After three weeks of everyone just *waiting*, Nora took the bus to his apartment herself. She found him in the middle of a breakdown, his apartment filthy, convinced he'd ruined his life after losing his job. Her parents had known something was wrong but didn't want to pry, didn't want to intrude. Their politeness, their respect for privacy, almost killed him.\n",
            "\n",
            "That's when Nora learned: sometimes the most destructive thing you can do is mind your own business.\n",
            "\n",
            "## Why She Became a Detective\n",
            "\n",
            "**Surface Reason:** She tells people she's interested in justice, in puzzles, in helping people. Standard cop interview answers.\n",
            "... (44 more lines)\n",
            "Input=2,039, Prompt cached used= False | Output=1,350 | Messages=6\n",
            "==============================================\n",
            "Turn 4:\n",
            "\n",
            "\n",
            "User: Can you draft a detailed outline for the story, breaking it down into chapters and key events?\n",
            "\n",
            "Assistant: \n",
            "# The Cartographer's Cipher: Detailed Outline\n",
            "\n",
            "## ACT ONE: ARRIVAL AND DISCOVERY\n",
            "\n",
            "### Chapter 1: The Smallest Case\n",
            "- **Setting:** Nora arrives in Shepherdstown on a gray October morning. Description of the town‚Äîhalf the storefronts empty, population aging, mines closed for twenty years.\n",
            "- **Key Event:** She meets Sheriff Dale Kovach (58, gruff, counting days to retirement). He makes it clear this is a nothing case‚Äîold man had a heart attack, daughter is in denial, just close it out.\n",
            "- **Character Moment:** Nora's crappy motel room. She can't sleep. Lies awake analyzing the case file that shouldn't be a case.\n",
            "- **Setup:** Brief flashback to why she's here‚Äîher last case, pushing too hard, burning bridges.\n",
            "\n",
            "### Chapter 2: The Workshop\n",
            "- **Setting:** Eugene Fisk's workshop‚Äîa converted garage behind his house, packed with decades of maps, surveying equipment, and obsessive documentation.\n",
            "- **Key Event:** Nora meets Eugene's daughter, **Caroline Fisk** (44, librarian, wire-thin with grief and rage). Caroline shows her the map‚Äîa massive 8x10 foot rendering of Shepherdstown with bizarre alterations.\n",
            "- **The Map:** Buildings that don't exist (a church that burned down in 1963, a restaurant that was never built). Streets rerouted. And 63 red X's scattered across the town.\n",
            "- **Caroline's Plea:** \"My father didn't have heart attacks. He had *purpose*. Someone scared him to death.\"\n",
            "... (113 more lines)\n",
            "Input=3,411, Prompt cached used= False | Output=3,500 | Messages=8\n",
            "==============================================\n",
            "Turn 5:\n",
            "\n",
            "\n",
            "User: Can you draft me a first chapter based on the plot and character ideas we've discussed so far? Make it around 2,000 words.\n",
            "\n",
            "Assistant: \n",
            "# Chapter 1: The Smallest Case\n",
            "\n",
            "The thing about dying towns is they all smell the same‚Äîwet cardboard and regret.\n",
            "\n",
            "Nora Alike noticed it the moment she stepped out of her Honda Civic into the gray October drizzle. Shepherdstown, Pennsylvania stretched out before her like a body someone had forgotten to bury. Half the storefronts on Main Street were empty, their windows either papered over with faded FOR LEASE signs or simply left naked to expose the gutted interiors. A diner called Mabel's still had its lights on, and a Dollar General looked operational, but that was about it for signs of life.\n",
            "\n",
            "Population 847, the sign at the town limits had declared. Nora suspected that number was aspirational.\n",
            "\n",
            "She locked her car‚Äîforce of habit, though she doubted anyone here had the energy for theft‚Äîand crossed the street toward the sheriff's office. The building was brick, squat, probably built in the sixties when the coal mine was still running and the town still had a future. A rusted flagpole stood out front, the rope slapping against the metal in the wind like a half-hearted call for help.\n",
            "\n",
            "*This is what you get*, Nora told herself. *This is what happens when you push too hard.*\n",
            "\n",
            "Three months ago, she'd been working property crimes in Harrisburg. Not glamorous, but it was something. Then she'd caught a case involving a city councilman's nephew and a string of car thefts. Everyone told her to go easy, that the kid was \"going through something,\" that his uncle was \"a good man who'd served the community for twenty years.\" Nora had arrested him anyway, because the evidence was there and because she couldn't understand why everyone wanted her to look the other way.\n",
            "\n",
            "The charges stuck. The councilman made calls. Nora's lieutenant made it clear that while he couldn't fire her, he could make her life miserable. Two weeks later, she'd accepted a \"transfer opportunity\" to Shepherdstown, where the entire sheriff's department consisted of three people and the most exciting case in recent memory was someone stealing catalytic converters from the Dollar General parking lot.\n",
            "... (98 more lines)\n",
            "Input=6,947, Prompt cached used= False | Output=2,670 | Messages=10\n",
            "==============================================\n",
            "Turn 6:\n",
            "\n",
            "\n",
            "User: Can you draft a second chapter that builds on the first one, introducing a new twist in the mystery?\n",
            "\n",
            "Assistant: \n",
            "# Chapter 2: The Workshop\n",
            "\n",
            "Caroline Fisk's house was a narrow two-story Victorian on Elm Street, painted a faded yellow that had probably been cheerful once. The detached garage sat thirty feet behind it, connected by a flagstone path that cut through what had been a carefully tended garden. Past tense, because the garden was dying now‚Äîrosebushes gone leggy and wild, perennials choked with weeds, a birdbath tipped over on its side.\n",
            "\n",
            "Nora parked on the street and walked up the front path. Before she could knock, the door opened.\n",
            "\n",
            "Caroline Fisk was forty-four but looked older, the way grief ages people in fast-forward. Thin to the point of frailty, with graying brown hair pulled back in a hasty ponytail. She wore jeans and an oversized cardigan that might have been her father's. Her eyes were red-rimmed but sharp, evaluating Nora with the focused intensity of someone who'd cried herself out and moved on to anger.\n",
            "\n",
            "\"You're the detective.\" Not a question.\n",
            "\n",
            "\"Detective Alike. I'm sorry for your loss, Ms. Fisk.\"\n",
            "\n",
            "\"Are you?\" Caroline stepped aside to let Nora in. \"Or are you here to tell me I'm a hysterical woman who can't accept that her father died of natural causes?\"\n",
            "\n",
            "\"I'm here to listen.\"\n",
            "... (161 more lines)\n",
            "Input=9,641, Prompt cached used= True | Output=3,500 | Messages=12\n"
          ]
        }
      ],
      "source": [
        "session = TraditionalCompactingChatSession(system_message=SYSTEM_PROMPT)\n",
        "\n",
        "# Simulated conversation\n",
        "messages = [\n",
        "    \"I want to create a story about a young detective solving a mysterious case in a small town. Generate 3 well throught out plot ideas for me to consider.\",\n",
        "    \"I don't like those ideas, can you think of one plot something more unique and unexpected?\",\n",
        "    \"Ok I like it. Can you help me develop the main character's backstory and motivations?\",\n",
        "    \"Can you draft a detailed outline for the story, breaking it down into chapters and key events?\",\n",
        "    \"Can you draft me a first chapter based on the plot and character ideas we've discussed so far? Make it around 2,000 words.\",\n",
        "    \"Can you draft a second chapter that builds on the first one, introducing a new twist in the mystery?\",\n",
        "]\n",
        "\n",
        "print(\"Starting conversation...\\n\")\n",
        "\n",
        "turn_count = 0\n",
        "\n",
        "for _i, message in enumerate(messages, 1):\n",
        "    turn_count += 1\n",
        "    print(f\"==============================================\\nTurn {turn_count}:\\n\")\n",
        "    response, usage = session.chat(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a long conversation with several turns. You'll notice a few things here:\n",
        "\n",
        "Prompt caching: You'll notice here that the input tokens eventually grew to a point where prompt caching was used (turn 6). This helps reduce costs and speed as these conversations grow!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the next turn, we are going to hit our 10K context window limit, which triggers compaction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üßπ Context window at 13141 tokens. Limit exceeded, compacting session memory...\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìù New session memory created.\n",
            "‚úÖ Tokens reduced: 13,141 ‚Üí 1559 (11,582 tokens saved, 88% reduction)\n",
            "‚è±Ô∏è Compaction time: 36.13s (user waiting...)\n",
            " Cache used: True\n",
            "------------------------------------------------------------\n",
            "\n",
            "User: Propose a title for the book\n",
            "\n",
            "Assistant: \n",
            "Looking at the story we've developed, I'd propose:\n",
            "\n",
            "**\"The Cartographer's Confession\"**\n",
            "\n",
            "Here's why this works:\n",
            "\n",
            "**Thematic Resonance:**\n",
            "- The double meaning captures Eugene's dual role: he kept confessions *and* his final map is itself a confession\n",
            "- \"Cartographer\" immediately signals the unique hook of your premise\n",
            "- \"Confession\" ties to the central tension between exposure and privacy\n",
            "\n",
            "**Alternative Titles to Consider:**\n",
            "\n",
            "1. **\"Burial Ground\"** - More commercial, emphasizes the literal buried evidence and metaphorical buried truths\n",
            "\n",
            "... (13 more lines)\n",
            "Input=1,840, Prompt cached used= False | Output=325 | Messages=3\n"
          ]
        }
      ],
      "source": [
        "response, usage = session.chat(\"Propose a title for the book\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You'll notice here that it took **over 36 seconds** for the agent to compact the conversation. Because we used traditional compaction, the user would be waiting on Claude to compact the conversation, which is not an ideal user experience.\n",
        "\n",
        "Below you can see the result of the compaction. It captures the key elements of conversation in less than 2K tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## User Intent\n",
            "User requested: \"I want to create a story about a young detective solving a mysterious case in a small town. Generate 3 well throught out plot ideas for me to consider.\"\n",
            "\n",
            "After rejecting initial 3 plots, user specified: \"I don't like those ideas, can you think of one plot something more unique and unexpected?\"\n",
            "\n",
            "Accepted \"The Cartographer's Cipher\" concept. Then requested: character development, detailed outline, and chapter drafts.\n",
            "\n",
            "## Completed Work\n",
            "\n",
            "**Story Concept Developed:**\n",
            "- Title: \"The Cartographer's Cipher\"\n",
            "- Premise: Detective investigates death of cartographer who spent final month decoding 40 years of town secrets onto a map with 63 red X's marking buried physical evidence\n",
            "\n",
            "**Character: Detective Nora Alike**\n",
            "- Age: 24, physically small (5'4\"), socially awkward, insomniac\n",
            "- Backstory: Youngest of 4 siblings by 7 years, essentially raised herself. At 16, \"rescued\" brother Martin from breakdown‚Äîhe resents the intrusion\n",
            "- Transferred to Shepherdstown from Harrisburg Property Crimes after arresting city councilman's nephew, making powerful enemies\n",
            "- Fatal flaw: Believes exposure always equals healing; pathological need to know; invasive about others' secrets, intensely private about her own\n",
            "- Character arc: Must learn some truths cause more harm than good\n",
            "\n",
            "**Setting: Shepherdstown, PA**\n",
            "- Population: 847 (declining)\n",
            "- Coal mine closed 1998, town dying since\n",
            "- Key locations: Sheriff's office, Fisk house/workshop, abandoned mine, Motor Lodge\n",
            "\n",
            "**Supporting Characters:**\n",
            "- Eugene Fisk: 79, cartographer, died 4 days before story opens. Stage 4 pancreatic cancer. Spent 40 years as town's \"confessor\"\n",
            "- Caroline Fisk: 44, librarian, Eugene's daughter, wants to expose all secrets\n",
            "- Sheriff Dale Kovach: 58, wants case closed quickly\n",
            "- Deputy Marcus Webb: 31, local, volunteers to help (his father's secret on the map)\n",
            "- Helen Morrison: 73, visited Eugene asking him to \"mark\" something, he refused\n",
            "\n",
            "**17-Chapter Outline Created:**\n",
            "- Act 1 (Chapters 1-5): Nora arrives, discovers map, investigates first X's revealing buried secrets (Hendersons' mortgage, mayor's affair, fire confession, etc.)\n",
            "- Act 2 (Chapters 6-11): Someone else following map, threatens Nora. Interviews living victims who beg her not to expose secrets. Discovers mine embezzlement cover-up\n",
            "- Act 3 (Chapters 12-17): Town meeting confrontation, Nora discovers Mayor Ortiz's father covered up mine safety violations. Phone call from brother Martin reveals he resents her \"rescue.\" Nora compromises: exposes mine cover-up (affects everyone), buries personal secrets. Arrests Mayor and father. Ambiguous ending about whether truth serves justice\n",
            "\n",
            "**Key Plot Points:**\n",
            "- Mine closed due to covered-up safety violations, not economics\n",
            "- Vernon Pike (union treasurer) embezzled funds as scapegoat at Donald Mercer's direction\n",
            "- Donald Mercer is Mayor Linda Ortiz's father\n",
            "- Eugene's final journal: \"I've mapped every lie, every buried truth... Maybe the only cure is exposure. Or maybe exposure is just another kind of death.\"\n",
            "- Each X marks physical evidence of a secret (cash, photos, confessions, documents)\n",
            "\n",
            "**Chapters Drafted:**\n",
            "- Chapter 1 (~2,000 words): Nora arrives Shepherdstown, meets Sheriff Kovach who dismisses case, assigned to talk to Caroline Fisk\n",
            "- Chapter 2 (continuation): Nora visits Caroline, sees workshop and massive incorrect map with 63 red X's. Caroline explains Eugene's paranoid final month. Journal entry reveals \"Morrison girl\" visit. Coroner confirms extremely elevated stress hormones. Chapter ends with Nora heading to investigate old mine location with survey map\n",
            "\n",
            "## Errors & Corrections\n",
            "\n",
            "User rejected first 3 plot concepts as not unique/unexpected enough:\n",
            "1. \"The Drowning Season\" (memory recovery therapy murders)\n",
            "2. \"The Lighthouse Keeper's Daughter\" (ritual sacrifices every 5-6 years)\n",
            "3. \"The Memory Box Murders\" (classmates hunting each other over past crime)\n",
            "\n",
            "User directive: \"can you think of one plot something more unique and unexpected?\" Led to cartographer concept.\n",
            "\n",
            "## Active Work\n",
            "\n",
            "Chapter 2 just completed. Ends with:\n",
            "\"She headed for the door, the survey map folded in her pocket and Eugene Fisk's final journal entry echoing in her mind: *Maybe the only cure is exposure. Or maybe exposure is just another kind of death.*\n",
            "\n",
            "Outside, the clouds had thickened again, pressing down on Shepherdstown like a shroud. Somewhere in this dying town, someone had scared an old man to death.\"\n",
            "\n",
            "Nora is heading to investigate the old mine (northern edge of town, surrounded by woods, one overgrown access road). She has Eugene's survey map showing cluster of X's around mine area, dates back to 1998.\n",
            "\n",
            "## Pending Tasks\n",
            "\n",
            "No explicit requests pending. Story development ongoing‚Äîpresumably more chapters to draft following the 17-chapter outline structure.\n",
            "\n",
            "## Key References\n",
            "\n",
            "**Timeline:**\n",
            "- 1998: Mine closes (covered-up safety violations)\n",
            "- 2003: Martha Fisk (Eugene's wife) dies of cancer\n",
            "- 5 weeks before present: Eugene starts creating \"corrected\" map\n",
            "- 2 weeks before present: Caroline finds Eugene shaking, says \"should have left them buried\"\n",
            "- 1 week before present: Helen Morrison visits, Eugene refuses to mark something\n",
            "- 4 days before present: Eugene found dead with extremely elevated cortisol levels\n",
            "\n",
            "**Map Details:**\n",
            "- 8ft x 10ft, mounted on foam board with acetate cover\n",
            "- Shows Shepherdstown with deliberate \"errors\": church on Third & Maple (doesn't exist), Giovanni's restaurant on Main, rerouted streets\n",
            "- 63+ red X's scattered across town\n",
            "- Each X marks buried physical evidence of a secret\n",
            "- Survey map subset focuses on mine area with names/dates/timeline from 1998\n",
            "\n",
            "**Character Relationships:**\n",
            "- Nora/Martin (brother): She \"saved\" him 8 years ago when he was 23 and having breakdown; he resents the public humiliation; they barely speak now\n",
            "- Eugene/Caroline: She cared for him through cancer; he left her the decoded map knowing she'd find it\n",
            "- Eugene/townspeople: He was unofficial confessor for 40 years; people trusted him to keep secrets safe\n"
          ]
        }
      ],
      "source": [
        "print(session.summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instant Compaction\n",
        "\n",
        "With **Instant compaction** the session memory is PROACTIVELY generated once a soft token threshold is reached. \n",
        "\n",
        "Once the user triggers a compaction or a hard limit is reached, the summary is already available, so the user doesn't need to wait.\n",
        "\n",
        "Result: Instant compaction, no waiting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "SESSION MEMORY COMPACTION (instant)\n",
        "```\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "Turn 1 ‚Üí Turn 2 ‚Üí ... ‚Üí Turn K ‚Üí Turn K+1 ‚Üí ... ‚Üí Turn N ‚Üí ..  ‚Üí CONTEXT FULL!\n",
        "                            ‚îÇ                         ‚îÇ            ‚îÇ\n",
        "                (soft token threshold met:        (update          ‚îÇ\n",
        "               initialize session memory)          trigger)        ‚îÇ\n",
        "                            ‚îÇ                                      ‚îÇ\n",
        "                            ‚îÇ                         ‚îÇ            ‚îÇ\n",
        "                            ‚ñº                         ‚ñº            ‚îÇ\n",
        "                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ\n",
        "                       ‚îÇ Create ‚îÇ                ‚îÇ Update ‚îÇ        ‚îÇ\n",
        "                       ‚îÇ memory ‚îÇ (background)   ‚îÇ memory ‚îÇ        ‚îÇ\n",
        "                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ\n",
        "                            ‚îÇ                         ‚îÇ            ‚îÇ\n",
        "                            ‚ñº                         ‚ñº            ‚ñº\n",
        "                     üìù session-memory.md ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ INSTANT SWAP!\n",
        "                       (continuously updated)\n",
        "```\n",
        "\n",
        "**Update triggers:** The first summary is generated after the initial soft token limit. Updates can be triggered after every subsequent turn, or at periodically at natural breakpoints intervals (e.g. every ~10k tokens or 3+ tool calls)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This `InstantCompactingChatSession` class uses **threading** for background execution:\n",
        "1. **`threading.Thread`** - runs memory updates in background without blocking\n",
        "2. **Thread-safe state** - uses `threading.Lock` to safely update shared memory\n",
        "3. **Daemon threads** - background work doesn't prevent program exit\n",
        "4. **Instant compaction** - when context is full, just swap in the pre-built memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "\n",
        "class InstantCompactingChatSession:\n",
        "    \"\"\"\n",
        "    Maintains session memory via incremental background updates.\n",
        "\n",
        "    Key insight: By updating memory in the background after each turn,\n",
        "    the summary is already ready when compaction is needed - instant swap!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        system_message=\"You are a helpful assistant\",\n",
        "        context_limit: int = 12000,\n",
        "        min_tokens_to_init: int = 7500,\n",
        "        min_tokens_between_updates: int = 2000,\n",
        "    ):\n",
        "        # Thresholds\n",
        "        self.context_limit = context_limit  # the point at which the conversation is compacted so it does not exceed model limits\n",
        "        self.min_tokens_to_init = min_tokens_to_init  # tokens needed to trigger initial memory creation; note this happens PROACTIVELY in background unlike traditional compaction\n",
        "        self.min_tokens_between_updates = min_tokens_between_updates  # tokens needed to trigger memory update. only comes into play after initial memory is created and additional compaction (memory update) is needed after that\n",
        "\n",
        "        # Conversation state\n",
        "        self.system_message = system_message\n",
        "        self.messages = []\n",
        "        self.current_context_window_tokens = 0\n",
        "\n",
        "        # Session memory state\n",
        "        self.session_memory = None  # this is the compacted conversation in session memory; for the demo we are storing this in memory, but in production you would write to session_memory.md file\n",
        "        self.last_summarized_index = (\n",
        "            0  # The index of the last message included in the session memory\n",
        "        )\n",
        "        self.tokens_at_last_update = 0  # To track tokens at last memory update and see if enough new tokens have been added to trigger another update\n",
        "\n",
        "        # Background update tracking\n",
        "        self._update_thread: threading.Thread | None = None\n",
        "        self.last_update_time = None\n",
        "        self._lock = threading.Lock()\n",
        "\n",
        "    def chat(self, user_message: str):\n",
        "        \"\"\"Process a chat turn with background session memory updates.\"\"\"\n",
        "\n",
        "        if self.current_context_window_tokens + estimate_tokens(user_message) >= self.context_limit:\n",
        "            self.compact()  # note that when this is triggered, the compaction has already been created and is just swapped in instantly\n",
        "\n",
        "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "        response = client.messages.create(\n",
        "            model=MODEL,\n",
        "            max_tokens=3500,\n",
        "            system=self.system_message,\n",
        "            messages=add_cache_control(self.messages),\n",
        "        )\n",
        "\n",
        "        assistant_message = response.content[0].text\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "\n",
        "        # Calculate token usage including cache\n",
        "        cache_read = getattr(response.usage, \"cache_read_input_tokens\", 0) or 0\n",
        "        total_input = response.usage.input_tokens + cache_read\n",
        "\n",
        "        # Update context window tokens (includes cached tokens since they still count toward context)\n",
        "        self.current_context_window_tokens = total_input + response.usage.output_tokens\n",
        "\n",
        "        # KEY DIFFERENCE: Trigger background memory update if needed proactively, before compaction is needed\n",
        "        background_status = None\n",
        "        if self._should_init_memory() or self._should_update_memory():\n",
        "            self._trigger_background_update()\n",
        "            background_status = \"initializing\" if self.session_memory is None else \"updating\"\n",
        "\n",
        "        # Return usage info with cache stats\n",
        "        return assistant_message, response.usage, background_status\n",
        "\n",
        "    # Helper methods to determine when to init session memory\n",
        "    def _should_init_memory(self) -> bool:\n",
        "        return (\n",
        "            self.session_memory is None\n",
        "            and self.current_context_window_tokens >= self.min_tokens_to_init\n",
        "        )\n",
        "\n",
        "    # Helper method to determine if memory should be updated\n",
        "    def _should_update_memory(self) -> bool:\n",
        "        if self.session_memory is None:\n",
        "            return False\n",
        "        tokens_since = self.current_context_window_tokens - self.tokens_at_last_update\n",
        "        return tokens_since >= self.min_tokens_between_updates\n",
        "\n",
        "    # Methods to create initial session memory\n",
        "    def _create_session_memory(self, messages: list[dict]) -> str:\n",
        "        \"\"\"Generate initial session memory from messages.\"\"\"\n",
        "        # Put compaction instructions in user message to share cache with main chat\n",
        "        compaction_messages = [{\"role\": \"user\", \"content\": SESSION_MEMORY_PROMPT}]\n",
        "        response = client.messages.create(\n",
        "            model=MODEL,\n",
        "            max_tokens=5000,\n",
        "            system=self.system_message,  # Same as main chat for cache sharing\n",
        "            messages=add_cache_control(messages) + compaction_messages,\n",
        "        )\n",
        "        summary, _ = remove_thinking_blocks(\n",
        "            response.content[0].text\n",
        "        )  # clean up any <think> blocks because they are not needed in the session memory\n",
        "        print(\n",
        "            f\"   [Background] Initial session memory created. Cache hit={getattr(response.usage, 'cache_read_input_tokens', 0) > 0}\"\n",
        "        )\n",
        "        return summary\n",
        "\n",
        "    def _update_session_memory(self, new_messages: list[dict]) -> str:\n",
        "        \"\"\"Update existing session memory with new messages. In practice, you may want to do this via file edit rather than full re-generation. But for demo purposes we do full regeneration here.\"\"\"\n",
        "        # Put compaction instructions in user message to share cache with main chat\n",
        "        compaction_update_messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": SESSION_MEMORY_PROMPT\n",
        "                + f\"\"\"There is an existing session memory: {self.session_memory}. Return the entire session memory with updates to reflect new messages.\"\"\",\n",
        "            }\n",
        "        ]\n",
        "        response = client.messages.create(\n",
        "            model=MODEL,\n",
        "            max_tokens=5000,\n",
        "            system=self.system_message,\n",
        "            messages=new_messages\n",
        "            + compaction_update_messages,  # you may want to use prompt caching instead, in which case you'd use add_cache_control(self.messages) here\n",
        "        )\n",
        "        updated_summary, _ = remove_thinking_blocks(\n",
        "            response.content[0].text\n",
        "        )  # clean up any <think> blocks because they are not needed in the session memory\n",
        "        print(\"   [Background] Session memory updated.\")\n",
        "        return updated_summary\n",
        "\n",
        "    # Background memory update methods\n",
        "    def _background_memory_update(\n",
        "        self, messages_snapshot: list[dict], snapshot_index: int, current_tokens: int\n",
        "    ):\n",
        "        \"\"\"Run session memory update in a background thread.\"\"\"\n",
        "        try:\n",
        "            with self._lock:\n",
        "                current_session_memory = self.session_memory\n",
        "                last_index = self.last_summarized_index\n",
        "\n",
        "            if current_session_memory is None:\n",
        "                new_memory = self._create_session_memory(messages_snapshot)\n",
        "            else:\n",
        "                # Get new messages since last summary\n",
        "                new_messages = messages_snapshot[last_index:]\n",
        "                if not new_messages:\n",
        "                    return\n",
        "                new_memory = self._update_session_memory(new_messages)\n",
        "\n",
        "            # Update state (thread-safe)\n",
        "            with self._lock:\n",
        "                self.session_memory = new_memory\n",
        "                self.last_summarized_index = snapshot_index\n",
        "                self.tokens_at_last_update = current_tokens\n",
        "                self.last_update_time = time.time()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   [Background] Error updating memory: {e}\")\n",
        "\n",
        "    # This makes sure only one background update runs at a time. If one is already running, we skip starting another. If not, we start a new thread to do the update.\n",
        "    def _trigger_background_update(self):\n",
        "        \"\"\"Trigger a background session memory update.\"\"\"\n",
        "        if self._update_thread is not None and self._update_thread.is_alive():\n",
        "            return\n",
        "\n",
        "        messages_snapshot = self.messages.copy()\n",
        "        snapshot_index = len(messages_snapshot)\n",
        "        current_tokens = self.current_context_window_tokens\n",
        "\n",
        "        self._update_thread = threading.Thread(\n",
        "            target=self._background_memory_update,\n",
        "            args=(messages_snapshot, snapshot_index, current_tokens),\n",
        "            daemon=True,\n",
        "        )\n",
        "        self._update_thread.start()\n",
        "\n",
        "    # Function to compact\n",
        "    def compact(self):\n",
        "        \"\"\"INSTANT compaction using pre-built session memory.\"\"\"\n",
        "        prev_msg_count = len(self.messages)\n",
        "\n",
        "        # Ensure session memory is ready. Shouldn't be an issue normally, but here for safety.\n",
        "        if self.session_memory is None:\n",
        "            if self._update_thread is not None and self._update_thread.is_alive():\n",
        "                print(\"   ‚è≥ Waiting for background memory update...\")\n",
        "                self._update_thread.join(timeout=30.0)\n",
        "\n",
        "            if self.session_memory is None:\n",
        "                print(\"   ‚ö†Ô∏è  No pre-built memory, creating synchronously...\")\n",
        "                start = time.perf_counter()\n",
        "                self.session_memory = self._create_session_memory(self.messages)\n",
        "                elapsed = time.perf_counter() - start\n",
        "                print(f\"   ‚è±Ô∏è  Took {elapsed:.2f}s (but should be instant normally!)\")\n",
        "                self.last_summarized_index = len(self.messages)\n",
        "\n",
        "        with self._lock:\n",
        "            unsummarized = self.messages[self.last_summarized_index :]\n",
        "            summary_message = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"\"\"This session is being continued from a previous conversation. Here is the session memory: {self.session_memory}.Continue from where we left off.\"\"\",\n",
        "                }\n",
        "            ]\n",
        "            self.messages = summary_message + unsummarized\n",
        "            self.last_summarized_index = 1\n",
        "\n",
        "            print(f\"\\n{'=' * 60}\")\n",
        "            print(f\"‚ö° INSTANT COMPACTION! Messages: {prev_msg_count} ‚Üí {len(self.messages)}\")\n",
        "            print(\"   Session memory was pre-built (no wait time!)\")\n",
        "            print(f\"{'=' * 60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example use of Instant Compaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting conversation with instant compacting chat session...\n",
            "\n",
            "============================================================\n",
            "Turn 1:\n",
            "\n",
            "User: I want to create a story about a young detective solving a mysterious case in a small town. Generate 3 well throught out plot ideas for me to consider.\n",
            "\n",
            "Assistant: \n",
            "# Three Detective Story Concepts\n",
            "\n",
            "## 1. **The Vanishing Act**\n",
            "... (30 more lines)\n",
            "\n",
            "Token Usage:\n",
            "  Input: 318 (new: 318, cached: 0)\n",
            "  Output: 762\n",
            "  Messages: 2 | Memory: not yet\n",
            "\n",
            "============================================================\n",
            "Turn 2:\n",
            "\n",
            "User: I don't like those ideas, can you think of one plot something more unique and unexpected?\n",
            "\n",
            "Assistant: \n",
            "# **The Cartographer's Daughter**\n",
            "\n",
            "**Premise:** Nora Fields (23) returns to her dying hometown of Millbrook after her cartographer father's sudden death. The town is literally disappearing‚Äînot metaphorically, but *actually*. Buildings that existed last month are gone. Streets lead to nowhere. Residents have conflicting memories about what was where. The population has dropped from 2,000 to 300 in three years, but no one can agree on who left or why.\n",
            "... (20 more lines)\n",
            "\n",
            "Token Usage:\n",
            "  Input: 1,102 (new: 1,102, cached: 0)\n",
            "  Output: 636\n",
            "  Messages: 4 | Memory: not yet\n",
            "\n",
            "============================================================\n",
            "Turn 3:\n",
            "\n",
            "User: Ok I like it. Can you help me develop the main character's backstory and motivations?\n",
            "\n",
            "Assistant: \n",
            "# Nora Fields - Character Development\n",
            "\n",
            "## Background\n",
            "... (67 more lines)\n",
            "\n",
            "Token Usage:\n",
            "  Input: 1,760 (new: 1,760, cached: 0)\n",
            "  Output: 1,214\n",
            "  Messages: 6 | Memory: not yet\n",
            "\n",
            "============================================================\n",
            "Turn 4:\n",
            "\n",
            "User: Can you draft a detailed outline for the story, breaking it down into chapters and key events?\n",
            "\n",
            "Assistant: \n",
            "# **The Cartographer's Daughter - Story Outline**\n",
            "\n",
            "## ACT ONE: The Return (Chapters 1-5)\n",
            "... (175 more lines)\n",
            "\n",
            "Token Usage:\n",
            "  Input: 2,996 (new: 2,996, cached: 0)\n",
            "  Output: 3,500\n",
            "  Messages: 8 | Memory: not yet\n",
            "\n",
            "============================================================\n",
            "Turn 5:\n",
            "\n",
            "User: Can you draft me a first chapter based on the plot and character ideas we've discussed so far? Make it around 2,000 words.\n",
            "\n",
            "Assistant: \n",
            "# Chapter 1: Homecoming\n",
            "\n",
            "The town looked smaller than Nora remembered, which was exactly what everyone said about their hometowns. What they didn't mention was the peculiar quality of that smallness‚Äînot just diminished by time and perspective, but somehow *compressed*, as if Millbrook had been folded in on itself while she was away.\n",
            "... (168 more lines)\n",
            "\n",
            "Token Usage:\n",
            "  Input: 6,532 (new: 6,532, cached: 0)\n",
            "  Output: 3,077\n",
            "  Messages: 10 | Memory: not yet\n",
            "\n",
            "  [Background] Proactively initializing session memory...\n",
            "  Context window: 9,609 tokens\n",
            "\n",
            "   [Background] Initial session memory created. Cache hit=True\n",
            "============================================================\n",
            "Turn 6:\n",
            "\n",
            "User: Can you draft a second chapter that builds on the first one?Can you revise that second chapter, make it more suspenseful and engaging?\n",
            "\n",
            "Assistant: \n",
            "# Chapter 2: The Map That Shouldn't Exist\n",
            "\n",
            "The funeral was smaller than Nora expected, which was saying something‚Äîshe'd expected it to be small.\n",
            "... (214 more lines)\n",
            "\n",
            "Token Usage:\n",
            "  Input: 9,642 (new: 5,546, cached: 4,096)\n",
            "  Output: 3,500\n",
            "  Messages: 12 | Memory: ready\n",
            "  ‚úì Cache hit! 42% of input from cache\n",
            "\n",
            "  [Background] Proactively updating session memory...\n",
            "  Context window: 13,142 tokens\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Background] Session memory updated.\n"
          ]
        }
      ],
      "source": [
        "# Low thresholds for demo - in production you'd use higher values\n",
        "session = InstantCompactingChatSession(\n",
        "    system_message=SYSTEM_PROMPT,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    \"I want to create a story about a young detective solving a mysterious case in a small town. Generate 3 well throught out plot ideas for me to consider.\",\n",
        "    \"I don't like those ideas, can you think of one plot something more unique and unexpected?\",\n",
        "    \"Ok I like it. Can you help me develop the main character's backstory and motivations?\",\n",
        "    \"Can you draft a detailed outline for the story, breaking it down into chapters and key events?\",\n",
        "    \"Can you draft me a first chapter based on the plot and character ideas we've discussed so far? Make it around 2,000 words.\",\n",
        "    \"Can you draft a second chapter that builds on the first one?\"\n",
        "    \"Can you revise that second chapter, make it more suspenseful and engaging?\",\n",
        "]\n",
        "print(\"Starting conversation with instant compacting chat session...\\n\")\n",
        "\n",
        "turn_count = 0\n",
        "for _i, message in enumerate(messages, 1):\n",
        "    response, usage, background_status = session.chat(message)\n",
        "    turn_count += 1\n",
        "\n",
        "    # Calculate cache stats\n",
        "    cache_read = getattr(usage, \"cache_read_input_tokens\", 0) or 0\n",
        "    cache_created = getattr(usage, \"cache_creation_input_tokens\", 0) or 0\n",
        "    total_input = usage.input_tokens + cache_read\n",
        "\n",
        "    print(f\"{'=' * 60}\")\n",
        "    print(f\"Turn {turn_count}:\")\n",
        "    print(f\"\\nUser: {message}\")\n",
        "    print(f\"\\nAssistant: \\n{truncate_response(response, max_lines=3)}\")\n",
        "    print(\"\\nToken Usage:\")\n",
        "    print(f\"  Input: {total_input:,} (new: {usage.input_tokens:,}, cached: {cache_read:,})\")\n",
        "    print(f\"  Output: {usage.output_tokens:,}\")\n",
        "    print(\n",
        "        f\"  Messages: {len(session.messages)} | Memory: {'ready' if session.session_memory else 'not yet'}\"\n",
        "    )\n",
        "\n",
        "    if cache_read > 0:\n",
        "        cache_pct = (cache_read / total_input) * 100\n",
        "        print(f\"  ‚úì Cache hit! {cache_pct:.0f}% of input from cache\")\n",
        "\n",
        "    if background_status:\n",
        "        print(f\"\\n  [Background] Proactively {background_status} session memory...\")\n",
        "        print(f\"  Context window: {session.current_context_window_tokens:,} tokens\")\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "‚ö° INSTANT COMPACTION! Messages: 12 ‚Üí 1\n",
            "   Session memory was pre-built (no wait time!)\n",
            "============================================================\n",
            "\n",
            "User: What did we just talk about? Give me one sentence\n",
            "\n",
            "Assistant: \n",
            "We had just finished drafting Chapter 2 (the funeral and evidence discovery), and you requested that I revise it to make it more suspenseful and engaging‚Äîwhich I hadn't completed yet before the conversation ended.\n",
            "\n",
            "Would you like me to provide that revised, more suspenseful version of Chapter 2 now?\n",
            "\n",
            "Token Usage:\n",
            "  Input: 2,276 (new: 2,276, cached: 0)\n",
            "  Output: 71\n",
            "  Messages: 3 | Memory: ready\n"
          ]
        }
      ],
      "source": [
        "message = \"What did we just talk about? Give me one sentence\"\n",
        "response, usage, background_status = session.chat(message)\n",
        "\n",
        "# Calculate cache stats\n",
        "cache_read = getattr(usage, \"cache_read_input_tokens\", 0) or 0\n",
        "total_input = usage.input_tokens + cache_read\n",
        "\n",
        "print(f\"\\nUser: {message}\")\n",
        "print(f\"\\nAssistant: \\n{truncate_response(response, max_lines=3)}\")\n",
        "print(\"\\nToken Usage:\")\n",
        "print(f\"  Input: {total_input:,} (new: {usage.input_tokens:,}, cached: {cache_read:,})\")\n",
        "print(f\"  Output: {usage.output_tokens:,}\")\n",
        "print(\n",
        "    f\"  Messages: {len(session.messages)} | Memory: {'ready' if session.session_memory else 'not yet'}\"\n",
        ")\n",
        "\n",
        "if cache_read > 0:\n",
        "    cache_pct = (cache_read / total_input) * 100\n",
        "    print(f\"  ‚úì Cache hit! {cache_pct:.0f}% of input from cache\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You'll notice here that once we hit the context limit, the session memory was instantaly swapped in, meaning the user had zero waiting time for a response!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced: Understanding Prompt Caching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The background updates can be made **~10x cheaper** by using prompt caching. The trick:\n",
        "1. Pass the **full conversation** to the background summarizer\n",
        "2. Add `cache_control` markers so subsequent requests hit the cache\n",
        "3. Only the new \"summarize this\" instruction is billed at full price\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    PROMPT CACHING FOR LONG CONVERSATIONS                        ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  WITHOUT CACHING: Pay full price for entire context every turn                 ‚îÇ\n",
        "‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                   ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  Turn 1:  [System][User1][Asst1]                         ‚Üí  500 tokens  @ $3/M ‚îÇ\n",
        "‚îÇ  Turn 2:  [System][User1][Asst1][User2][Asst2]           ‚Üí 1500 tokens  @ $3/M ‚îÇ\n",
        "‚îÇ  Turn 3:  [System][User1][Asst1][User2][Asst2][User3]... ‚Üí 3000 tokens  @ $3/M ‚îÇ\n",
        "‚îÇ  Turn 4:  [System][User1][Asst1][User2][Asst2][User3]... ‚Üí 5000 tokens  @ $3/M ‚îÇ\n",
        "‚îÇ           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                         ‚îÇ\n",
        "‚îÇ                                              Total: 10,000 tokens = $0.030      ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  WITH CACHING: Pay full price once, then 90% discount on prefix                ‚îÇ\n",
        "‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  Turn 1:  [System][User1][Asst1]‚óÜ                        ‚Üí  500 tokens  @ $3/M ‚îÇ\n",
        "‚îÇ                                ‚ñ≤                            (cache created)    ‚îÇ\n",
        "‚îÇ                          cache breakpoint                                       ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  Turn 2:  [System][User1][Asst1][User2][Asst2]‚óÜ                                ‚îÇ\n",
        "‚îÇ           ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ cached ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ                                              ‚îÇ\n",
        "‚îÇ                500 @ $0.30/M + 1000 new @ $3/M  =  $0.0032                     ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  Turn 3:  [System][User1][Asst1][User2][Asst2][User3][Asst3]‚óÜ                  ‚îÇ\n",
        "‚îÇ           ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ cached ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ                                  ‚îÇ\n",
        "‚îÇ               1500 @ $0.30/M + 1500 new @ $3/M  =  $0.0050                     ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  Turn 4:  [System][User1][Asst1][User2][Asst2][User3][Asst3][User4][Asst4]‚óÜ    ‚îÇ\n",
        "‚îÇ           ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ cached ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ                 ‚îÇ\n",
        "‚îÇ                     3000 @ $0.30/M + 2000 new @ $3/M  =  $0.0069               ‚îÇ\n",
        "‚îÇ           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                         ‚îÇ\n",
        "‚îÇ                                              Total: $0.0166  (45% savings)     ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  COMPACTION + CACHING: Double benefit                                           ‚îÇ\n",
        "‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                                           ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ    Main Chat                      Background Summarizer                         ‚îÇ\n",
        "‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                         ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  [Conversation grows...]          [Same conversation prefix]‚óÜ + [Summarize!]   ‚îÇ\n",
        "‚îÇ         ‚îÇ                                    ‚îÇ                                  ‚îÇ\n",
        "‚îÇ         ‚îÇ                         Cache hit! Only pays for                      ‚îÇ\n",
        "‚îÇ         ‚îÇ                         the summarization prompt                      ‚îÇ\n",
        "‚îÇ         ‚îÇ                                    ‚îÇ                                  ‚îÇ\n",
        "‚îÇ         ‚ñº                                    ‚ñº                                  ‚îÇ\n",
        "‚îÇ  Context limit reached  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  Session memory ready instantly                ‚îÇ\n",
        "‚îÇ                                  (built cheaply in background)                  ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
        "‚îÇ  ‚îÇ  Key insight: The background summarizer reuses the same conversation     ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îÇ  prefix that was just sent to the main chat - automatic cache hit!       ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚óÜ = cache_control breakpoint (cache everything before this point)\n",
        "```\n",
        "\n",
        "### Why this matters for compaction\n",
        "\n",
        "| Scenario | Cost per background update | Notes |\n",
        "|----------|---------------------------|-------|\n",
        "| No caching | Full input cost | 5,000 tokens √ó $3/M = $0.015 |\n",
        "| With caching | ~10% of input cost | 500 new + 4,500 cached = $0.003 |\n",
        "| **Savings** | **~80%** | Compounds over many updates |\n",
        "\n",
        "The longer the conversation, the bigger the savings‚Äîexactly when you need compaction most!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How the Caching Works\n",
        "\n",
        "The key is in `_add_cache_control()` and `_create_session_memory_cached()`:\n",
        "\n",
        "```python\n",
        "# 1. Mark the last conversation message with cache_control\n",
        "{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\n",
        "        \"type\": \"text\",\n",
        "        \"text\": msg[\"content\"],\n",
        "        \"cache_control\": {\"type\": \"ephemeral\"}  # <-- This creates a cache breakpoint\n",
        "    }]\n",
        "}\n",
        "\n",
        "# 2. Also mark the system prompt\n",
        "system=[{\n",
        "    \"type\": \"text\",\n",
        "    \"text\": \"You are a session memory agent...\",\n",
        "    \"cache_control\": {\"type\": \"ephemeral\"}\n",
        "}]\n",
        "```\n",
        "\n",
        "**Why this works:**\n",
        "- The first background update creates a cache entry for `[System + Messages]`\n",
        "- Subsequent updates with the same message prefix get **cache hits**\n",
        "- Only the new summarization instruction is billed at full price\n",
        "- Cache entries have a 5-minute TTL, so rapid updates benefit most\n",
        "\n",
        "**Cost math:**\n",
        "- Without caching: 5,000 tokens √ó $3.00/1M = $0.015 per update\n",
        "- With caching: 500 new tokens √ó $3.00/1M + 4,500 cached √ó $0.30/1M = $0.00285\n",
        "- **Savings: ~80%** on background summarization costs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this cookbook, you learned how to manage long-running Claude conversations through session memory compaction.\n",
        "\n",
        "### What We Covered\n",
        "\n",
        "‚úÖ **Effective compaction prompts** - Structure your session memory to preserve user intent, completed work, errors, active work, and key references while discarding filler\n",
        "\n",
        "‚úÖ **Instant compaction** - Use background threading to proactively build session memory, eliminating user wait time when context limits are reached\n",
        "\n",
        "‚úÖ **Prompt caching for cost savings** - Reduce background update costs by ~80% by reusing the conversation prefix cache\n",
        "\n",
        "‚úÖ **Traditional vs. instant patterns** - Understand when to use each approach based on your application needs\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Weight recency heavily** - The end of a conversation is the active working context\n",
        "2. **Preserve user corrections verbatim** - Prevents the model from reverting to old behaviors\n",
        "3. **Build memory proactively** - Don't wait for context limits; start background updates early\n",
        "4. **Leverage prompt caching** - Background summarization can share cache with the main conversation\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **For agentic workflows**: See [Automatic Context Compaction](../tool_use/automatic-context-compaction.ipynb) for SDK-based automatic compaction with tool use\n",
        "- **For production**: Consider persisting session memory to disk rather than keeping it in memory\n",
        "- **For optimization**: Experiment with update frequency thresholds to balance cost vs. freshness"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
