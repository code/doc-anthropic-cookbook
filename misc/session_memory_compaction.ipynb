{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instant Compaction with Session Memory\n",
    "\n",
    "Traditional compaction is slow: when you hit the context limit, you wait for a summary.\n",
    "\n",
    "With **Instant compaction** the session memory is proactively generated once a soft token threshold is reached. Once the user triggers a compaction or a hard limit is reached, the summary is already available, so the user doesn't need to wait.\n",
    "\n",
    "Result: Instant compaction, no waiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "TRADITIONAL COMPACTION (slow)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Turn 1 â†’ Turn 2 â†’ Turn 3 â†’ ... â†’ Turn N â†’ CONTEXT FULL!\n",
    "                                              â”‚\n",
    "                                              â–¼\n",
    "                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                    â”‚ Generate summaryâ”‚\n",
    "                                    â”‚ ( USER WAITS !) â”‚\n",
    "                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                              â”‚\n",
    "                                              â–¼\n",
    "                                         Continue\n",
    "\n",
    "\n",
    "SESSION MEMORY COMPACTION (instant)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Turn 1 â†’ Turn 2 â†’ ... â†’ Turn K â†’ Turn K+1 â†’ ... â†’ Turn N â†’ ..  â†’ CONTEXT FULL!\n",
    "                            â”‚                         â”‚            â”‚\n",
    "                (soft threshold met:              (update          â”‚\n",
    "                   10k tokens init)                trigger)        â”‚\n",
    "                            â”‚                                      â”‚\n",
    "                            â”‚                         â”‚            â”‚\n",
    "                            â–¼                         â–¼            â”‚\n",
    "                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n",
    "                       â”‚ Update â”‚                â”‚ Update â”‚        â”‚\n",
    "                       â”‚ memory â”‚ (background)   â”‚ memory â”‚        â”‚\n",
    "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\n",
    "                            â”‚                         â”‚            â”‚\n",
    "                            â–¼                         â–¼            â–¼\n",
    "                     ğŸ“ session-memory.md â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º INSTANT SWAP!\n",
    "                       (continuously updated)\n",
    "```\n",
    "\n",
    "**Update triggers:** The first summary is generated after the initial 10k tokens. Updates can be triggered after every subsequent turn, or at periodically at natural breakpoints intervals (e.g. every ~5k tokens or 3+ tool calls)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals: writing a compaction prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a well structured session memory prompt. \n",
    "\n",
    "Some best practices include:\n",
    "- Use chain-of-thought before summarizing â€” analyze first, then output                                                                                         \n",
    "- Enumerate exactly what to preserve: file paths, code snippets, errors, user corrections                                                                      \n",
    "- Weight recency heavily â€” the end of the conversation is the active context                                                                                   \n",
    "- Require verbatim quotes for next steps to prevent task drift                                                                                                 \n",
    "- Use structured sections with token budgets per section                                                                                                       \n",
    "- Include a \"Current State\" section that always reflects the moment of compaction\n",
    "\n",
    "Some pitfalls include:\n",
    "- Vague prompts like \"summarize this conversation\" produce lossy output                                                                                        \n",
    "- Treating all messages equally loses the active working context                                                                                               \n",
    "- Paraphrasing next steps introduces subtle drift that compounds                                                                                               \n",
    "- Omitting error history causes the model to retry failed approaches                                                                                           \n",
    "- Dropping user corrections makes the model revert to old behaviors                                                                                            \n",
    "- No token limits lets one section consume the entire summary                                                                                                  \n",
    "- Summarizing for human readability instead of model continuity\n",
    "- Having the agent try to compress the results of tool calls here - this can be retrieved later if the agent needs it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "SESSION_CREATION_PROMPT = \"\"\"\n",
    "<analysis-instructions>\n",
    "Before generating your summary, analyze the transcript in <think>...</think> tags:\n",
    "1. What did the user originally request? (Exact phrasing)\n",
    "2. What actions succeeded? What failed and why?\n",
    "3. Did the user correct or redirect the assistant at any point?\n",
    "4. What was actively being worked on at the end?\n",
    "5. What tasks remain incomplete or pending?\n",
    "6. What specific details (IDs, paths, values, names) must survive compression?\n",
    "</analysis-instructions>\n",
    "\n",
    "<summary-format>\n",
    "## User Intent\n",
    "The user's original request and any refinements. Use direct quotes for key requirements.\n",
    "If the user's goal evolved during the conversation, capture that progression.\n",
    "\n",
    "## Completed Work\n",
    "Actions successfully performed. Be specific:\n",
    "- What was created, modified, or deleted\n",
    "- Exact identifiers (file paths, record IDs, URLs, names)\n",
    "- Specific values, configurations, or settings applied\n",
    "\n",
    "## Errors & Corrections\n",
    "- Problems encountered and how they were resolved\n",
    "- Approaches that failed (so they aren't retried)\n",
    "- User corrections: \"don't do X\", \"actually I meant Y\", \"that's wrong because...\"\n",
    "Capture corrections verbatimâ€”these represent learned preferences.\n",
    "\n",
    "## Active Work\n",
    "What was in progress when the session ended. Include:\n",
    "- The specific task being performed\n",
    "- Direct quotes showing exactly where work left off\n",
    "- Any partial results or intermediate state\n",
    "\n",
    "## Pending Tasks\n",
    "Remaining items the user requested that haven't been started.\n",
    "Distinguish between \"explicitly requested\" and \"implied/assumed.\"\n",
    "\n",
    "## Key References\n",
    "Important details needed to continue:\n",
    "- Identifiers: IDs, paths, URLs, names, keys\n",
    "- Values: numbers, dates, configurations, credentials (redacted)\n",
    "- Context: relevant background information, constraints, preferences\n",
    "- Citations: sources referenced during the conversation\n",
    "</summary-format>\n",
    "\n",
    "<preserve-rules>\n",
    "Always preserve when present:\n",
    "- Exact identifiers (IDs, paths, URLs, keys, names)\n",
    "- Error messages verbatim\n",
    "- User corrections and negative feedback\n",
    "- Specific values, formulas, or configurations\n",
    "- Technical constraints or requirements discovered\n",
    "- The precise state of any in-progress work\n",
    "</preserve-rules>\n",
    "\n",
    "<compression-rules>\n",
    "- Weight recent messages more heavilyâ€”the end of the transcript is the active context\n",
    "- Omit pleasantries, acknowledgments, and filler (\"Sure!\", \"Great question\")\n",
    "- Omit system context that will be re-injected separately\n",
    "- Keep each section under 500 words; condense older content to make room for recent\n",
    "- If you must cut details, preserve: user corrections > errors > active work > completed work\n",
    "</compression-rules>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional compacting example\n",
    "In traditional compaction, you generate one summary once the token threshold is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# setup, we are using haiku for demo purposes\n",
    "import anthropic\n",
    "import warnings\n",
    "\n",
    "# Suppress noisy FutureWarning from coconut compiler\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"coconut\")\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "MODEL = \"claude-haiku-4-5-20251001\"\n",
    "\n",
    "# helper functions:\n",
    "\n",
    "def truncate_response(text: str, max_lines: int = 8) -> str:\n",
    "    \"\"\"Truncate long responses for cleaner output display.\"\"\"\n",
    "    lines = text.strip().split(\"\\n\")\n",
    "    if len(lines) <= max_lines:\n",
    "        return text\n",
    "    return \"\\n\".join(lines[:max_lines]) + f\"\\n... ({len(lines) - max_lines} more lines)\"\n",
    "\n",
    "def build_transcript(messages: list[dict]) -> str:\n",
    "    lines = []\n",
    "    for msg in messages:\n",
    "        role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "        lines.append(f\"{role}: {msg['content']}\")\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "def remove_thinking_blocks(text: str):                                                                                       \n",
    "    \"\"\"Remove <think>...</think> blocks from the text.\"\"\"                                                                    \n",
    "    import re                                                                                                                \n",
    "    matches = re.findall(r\"<think>.*?</think>\", text, flags=re.DOTALL)                                                       \n",
    "    cleaned = re.sub(r\"<think>.*?</think>\\s*\", \"\", text, flags=re.DOTALL).strip()                                            \n",
    "    return cleaned, \"\".join(matches)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class TraditionalCompactingChatSession:\n",
    "    \"\"\"Traditional chat session with compaction after the fact.\"\"\"\n",
    "    def __init__(self, context_limit: int = 1500):\n",
    "        self.context_limit = context_limit # the point at which the conversation is compacted so it does not exceed model limits. You would set this based on your model's context window size with a buffer for response tokens.\n",
    "        self.messages = []\n",
    "        self.current_context_window_tokens = 0\n",
    "        self.summary = None\n",
    "    \n",
    "    def chat(self, user_message: str):\n",
    "        # In traditional compaction, we check if we need to compact when the user sends a message. NOT IDEAL!\n",
    "        if self.current_context_window_tokens >= self.context_limit:\n",
    "            print(f\"\\nğŸ§¹ Context window at {self.current_context_window_tokens} tokens. Limit exceeded, compacting session memory...\")\n",
    "            self.compact() # compacts everything before the new user message\n",
    "        \n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        response = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=1024,\n",
    "            system=\"You are a helpful coding assistant.\",\n",
    "            messages=self.messages\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.content[0].text\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "        \n",
    "        # approximate current token count in the conversation before the next user message\n",
    "        self.current_context_window_tokens = response.usage.input_tokens + response.usage.output_tokens\n",
    "\n",
    "        return assistant_message, response.usage\n",
    "    \n",
    "    def compact(self):\n",
    "        prev_msg_count = len(self.messages)\n",
    "        \n",
    "        compaction_prompt = SESSION_CREATION_PROMPT + \"\\n\\nTranscript:\\n\" + build_transcript(self.messages)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        response = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=800, # note that some of this will be eaten up by the thinking blocks we remove later\n",
    "            system=\"\"\"You are a session memory agent. Compress the conversation into a structured summary \n",
    "that preserves all information needed to continue work seamlessly. Optimize for the assistant's \n",
    "ability to continue working, not human readability\"\"\",\n",
    "            messages=[{\"role\": \"user\", \"content\": compaction_prompt}]\n",
    "        )\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        \n",
    "        # Generate new summary message\n",
    "        self.summary, removed_text = remove_thinking_blocks(response.content[0].text) # clean up any <think> blocks because they are not needed in the session memory\n",
    "        approximate_summary_tokens = response.usage.output_tokens - round(len(removed_text) / 4)  # rough estimate of tokens removed from summary\n",
    "       \n",
    "        # Replace prior messages with new summary message\n",
    "        self.messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"This session is being continued from a previous conversation. Here is the session memory: {self.summary}.Continue from where we left off.\"\"\"\n",
    "        }]\n",
    "\n",
    "        # Show stats on compaction\n",
    "        curr_msg_count = len(self.messages)\n",
    "        \n",
    "        # Show token reduction if we just compacted\n",
    "        reduction = self.current_context_window_tokens - approximate_summary_tokens\n",
    "        pct = (reduction / self.current_context_window_tokens) * 100\n",
    "        \n",
    "        print(f\"\\n{'-' * 60}\")\n",
    "        print(f\"âœ… Tokens reduced: {self.current_context_window_tokens:,} â†’ {approximate_summary_tokens:.0f} ({reduction:,} tokens saved, {pct:.0f}% reduction)\")\n",
    "        print(f\"ğŸ“ New session memory created.\")\n",
    "        print(f\"ğŸ”„ Compaction messages: {prev_msg_count} â†’ {curr_msg_count}\")\n",
    "        print(f\"â±ï¸ Compaction time: {elapsed:.2f}s (user waiting...)\")\n",
    "        print(f\"{'-' * 60}\")\n",
    "        \n",
    "        # Update token count to reflect compacted state\n",
    "        self.current_context_window_tokens = approximate_summary_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use of traditional compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with traditional compacting chat session...\n",
      "\n",
      "==============================================\n",
      "Turn 1: Input=41 | Output=450 | Messages=2\n",
      "\n",
      "User: Explain Python decorators with a simple example.\n",
      "\n",
      "Assistant: \n",
      "# Python Decorators Explained\n",
      "\n",
      "A **decorator** is a function that modifies or enhances another function or class without permanently changing its source code.\n",
      "... (69 more lines)\n",
      "\n",
      "==============================================\n",
      "Turn 2: Input=504 | Output=871 | Messages=4\n",
      "\n",
      "User: Now show me a decorator that logs function arguments.\n",
      "\n",
      "Assistant: \n",
      "# Decorator that Logs Function Arguments\n",
      "\n",
      "Here's a practical logging decorator:\n",
      "... (122 more lines)\n",
      "\n",
      "==============================================\n",
      "Turn 3: Input=1,388 | Output=1,024 | Messages=6\n",
      "\n",
      "User: How do I make a decorator that accepts parameters?\n",
      "\n",
      "Assistant: \n",
      "# Decorators with Parameters\n",
      "\n",
      "To make a decorator that accepts parameters, you need an **extra layer of nesting**.\n",
      "... (144 more lines)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session = TraditionalCompactingChatSession()\n",
    "\n",
    "messages = [\n",
    "    \"Explain Python decorators with a simple example.\",\n",
    "    \"Now show me a decorator that logs function arguments.\",\n",
    "    \"How do I make a decorator that accepts parameters?\",\n",
    "]\n",
    "\n",
    "print(\"Starting conversation with traditional compacting chat session...\\n\")\n",
    "\n",
    "turn_count = 0\n",
    "\n",
    "for i, message in enumerate(messages, 1):\n",
    "    response, usage = session.chat(message)\n",
    "    turn_count += 1\n",
    "    print(\n",
    "        f\"==============================================\\n\"\n",
    "        f\"Turn {turn_count}: Input={usage.input_tokens:,} | \"\n",
    "        f\"Output={usage.output_tokens:,} | \"\n",
    "        f\"Messages={len(session.messages)}\"\n",
    "    )\n",
    "    print(f\"\\nUser: {message}\")\n",
    "    print(f\"\\nAssistant: \\n{truncate_response(response, max_lines=3)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§¹ Context window at 2412 tokens. Limit exceeded, compacting session memory...\n",
      "\n",
      "------------------------------------------------------------\n",
      "âœ… Tokens reduced: 2,412 â†’ 391 (2,021 tokens saved, 84% reduction)\n",
      "ğŸ“ New session memory created.\n",
      "ğŸ”„ Compaction messages: 6 â†’ 1\n",
      "â±ï¸ Compaction time: 6.93s (user waiting...)\n",
      "------------------------------------------------------------\n",
      "Final response after compaction:\n",
      "# Continuing: Complete the `validate_types` Decorator\n",
      "\n",
      "Let me finish that incomplete example from Response 3:\n",
      "\n",
      "```python\n",
      "from functools import wraps\n",
      "\n",
      "def validate_types(**type_checks):\n",
      "    \"\"\"Decorator that validates argument types before execution\"\"\"\n",
      "    def decorator(func):\n",
      "        @wraps(func)\n",
      "        def wrapper(*args, **kwargs):\n",
      "            # Get function signature to match args to parameter names\n",
      "            import inspect\n",
      "            sig = inspect.signature(func)\n",
      "            bound_args = sig.bind(*args, **kwargs)\n",
      "            bound_args.apply_defaults()\n",
      "            \n",
      "            # Validate types\n",
      "            for param_name, expected_type in type_checks.items():\n",
      "                if param_name in bound_args.arguments:\n",
      "                    actual_value = bound_args.arguments[param_name]\n",
      "                    if not isinstance(actual_value, expected_type):\n",
      "                        raise TypeError(\n",
      "                            f\"Parameter '{param_name}' expected {expected_type.__name__}, \"\n",
      "                            f\"got {type(actual_value).__name__}\"\n",
      "                        )\n",
      "            \n",
      "            return func(*args, **kwargs)\n",
      "        return wrapper\n",
      "    return decorator\n",
      "\n",
      "# Usage example:\n",
      "@validate_types(name=str, age=int, email=str)\n",
      "def create_user(name, age, email):\n",
      "    return f\"User {name} ({age}) created with email {email}\"\n",
      "\n",
      "# âœ… Success case\n",
      "print(create_user(\"Alice\", 30, \"alice@example.com\"))\n",
      "# Output: User Alice (30) created with email alice@example.com\n",
      "\n",
      "# âŒ Error case - wrong type for 'age'\n",
      "try:\n",
      "    print(create_user(\"Bob\", \"twenty-five\", \"bob@example.com\"))\n",
      "except TypeError as e:\n",
      "    print(f\"âŒ Error: {e}\")\n",
      "    # Output: âŒ Error: Parameter 'age' expected int, got str\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Related Advanced Topics to Cover Next\n",
      "\n",
      "Based on your progression, here are natural next steps:\n",
      "\n",
      "| Topic | Difficulty | Why It Matters |\n",
      "|-------|-----------|----------------|\n",
      "| **Class-based Decorators** | ğŸŸ¡ Intermediate | Stateful decorators using `__call__`, persisting data between calls |\n",
      "| **Stacking Decorators** | ğŸŸ¡ Intermediate | Order matters! `@decorator1 @decorator2 def func()` |\n",
      "| **Async Decorators** | ğŸ”´ Advanced | `async def` decorators for async/await functions |\n",
      "| **Decorator Composition** | ğŸ”´ Advanced | Building decorators from other decorators (meta-decorating) |\n",
      "| **Context Managers vs Decorators** | ğŸŸ¡ Intermediate | When to use `with` statements instead |\n",
      "| **Real-world patterns** | ğŸŸ¢ Beginner-friendly | Flask/Django route decorators, authentication, timing |\n",
      "\n",
      "**My recommendation**: Pick one:\n",
      "- ğŸŸ¢ **\"Show me real-world decorator patterns from Flask/Django\"** â€” immediately practical\n",
      "- ğŸŸ¡ **\"Explain class-based decorators\"** â€” bridges to OOP patterns\n",
      "- ğŸŸ¡ **\"What happens when I stack multiple decorators?\"** â€” common mistake area\n",
      "\n",
      "Which interests you? ğŸ¯\n"
     ]
    }
   ],
   "source": [
    "response, _ = session.chat(\"What other related topics should we cover?\")\n",
    "print(\"Final response after compaction:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## User Intent\n",
      "User requested progressive education on Python decorators:\n",
      "1. \"Explain Python decorators with a simple example\"\n",
      "2. \"Now show me a decorator that logs function arguments\"\n",
      "3. \"How do I make a decorator that accepts parameters?\"\n",
      "\n",
      "User is learning decorator progression from basic to advanced (parameterized).\n",
      "\n",
      "## Completed Work\n",
      "- **Response 1**: Basic decorator explanation with gift-wrapping analogy, simple example (`my_decorator`), example with arguments (`*args, **kwargs`), and real-world use cases\n",
      "- **Response 2**: Logging decorator (`log_arguments`) in three versions:\n",
      "  - Basic version logging args, kwargs, and result\n",
      "  - Enhanced version with formatted output and emoji\n",
      "  - Professional version using `functools.wraps` to preserve function metadata\n",
      "  - Applied to example functions: `greet()`, `multiply()`, `calculate()`, `divide()`\n",
      "- **Response 3**: Parameterized decorators with multiple practical examples:\n",
      "  - `repeat(times)` - repeats function execution N times\n",
      "  - `rate_limit(max_calls, time_window)` - rate limiting with time window tracking\n",
      "  - `log_with_prefix(prefix)` - logging with custom prefix parameter\n",
      "  - `validate_types(**type_checks)` - type validation decorator (incompleteâ€”cut off mid-example)\n",
      "\n",
      "## Errors & Corrections\n",
      "None identified. User progression was linear and organic.\n",
      "\n",
      "## Active Work\n",
      "**Incomplete**: Third response cut off mid-execution. Last example was `validate_types(**type_checks)` decorator demonstrating keyword argument type validation. The final test case `print(create_user` was incomplete (no closing parenthesis). This should be resumed with the complete example showing both successful call and error case.\n",
      "\n",
      "## Pending Tasks\n",
      "- Complete/deliver the final `\n"
     ]
    }
   ],
   "source": [
    "print(session.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result the user experineces a wait time when compaction occurs. It is only a few seconds in this example, but for long context compaction, this can be must longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instant Compaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The key insight: **build the session memory in the background** so it's ready when you need it.\n",
    "\n",
    "```\n",
    "Turn 1 â†’ Turn 2 â†’ ... â†’ Turn K  â†’ Turn K+1 â†’ ... â†’ CONTEXT FULL!\n",
    "                           â”‚           â”‚                 â”‚\n",
    "                     (threshold)  (update)          INSTANT!\n",
    "                           â†“           â†“                 â†“\n",
    "                    [Background]  [Background]    [Just swap in\n",
    "                     memory init   memory update   pre-built memory]\n",
    "```\n",
    "\n",
    "This `InstantCompactingChatSession` class uses **threading** for background execution:\n",
    "1. **`threading.Thread`** - runs memory updates in background without blocking\n",
    "2. **Thread-safe state** - uses `threading.Lock` to safely update shared memory\n",
    "3. **Daemon threads** - background work doesn't prevent program exit\n",
    "4. **Instant compaction** - when context is full, just swap in the pre-built memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "class InstantCompactingChatSession:\n",
    "    \"\"\"\n",
    "    Maintains session memory via incremental background updates.\n",
    "    \n",
    "    Key insight: By updating memory in the background after each turn,\n",
    "    the summary is already ready when compaction is needed - instant swap!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_limit: int = 1500,\n",
    "        min_tokens_to_init: int = 700,\n",
    "        min_tokens_between_updates: int = 300,\n",
    "    ):\n",
    "        # Thresholds\n",
    "        self.context_limit = context_limit # the point at which the conversation is compacted so it does not exceed model limits\n",
    "        self.min_tokens_to_init = min_tokens_to_init # tokens needed to trigger initial memory creation; note this happens PROACTIVELY in background unlike traditional compaction\n",
    "        self.min_tokens_between_updates = min_tokens_between_updates # tokens needed to trigger memory update. only comes into play after initial memory is created and additional compaction (memory update) is needed after that\n",
    "\n",
    "        # Conversation state\n",
    "        self.messages = []\n",
    "        self.current_context_window_tokens = 0\n",
    "\n",
    "        # Session memory state\n",
    "        self.session_memory = None # this is the compacted conversation in session memory; for the demo we are storing this in memory, but in production you would write to session_memory.md file\n",
    "        self.last_summarized_index = 0 # The index of the last message included in the session memory\n",
    "        self.tokens_at_last_update = 0 # TBD if I need this\n",
    "\n",
    "        # Background update tracking\n",
    "        self._update_thread: threading.Thread | None = None\n",
    "        self.last_update_time = None\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "    def chat(self, user_message: str):\n",
    "        \"\"\"Process a chat turn with background session memory updates.\"\"\"\n",
    "        if self.current_context_window_tokens >= self.context_limit:\n",
    "            self.compact() # note that when this is triggered, the compaction has already been created and is just swapped in instantly\n",
    "\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=1024,\n",
    "            system=\"You are a helpful coding assistant.\",\n",
    "            messages=self.messages,\n",
    "        )\n",
    "\n",
    "        assistant_message = response.content[0].text\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "        # approximate current token count in the conversation before the next user message\n",
    "        self.current_context_window_tokens = response.usage.input_tokens + response.usage.output_tokens\n",
    "\n",
    "        # KEY DIFFERENCE: Trigger background memory update if needed proactively, before compaction is needed\n",
    "        background_status = None\n",
    "        if self._should_init_memory() or self._should_update_memory():\n",
    "            self._trigger_background_update()\n",
    "            background_status = \"initializing\" if self.session_memory is None else \"updating\"\n",
    "\n",
    "        return assistant_message, response.usage, background_status\n",
    "    \n",
    "    # Helper methods to determine when to init/update/compact\n",
    "    def _should_init_memory(self) -> bool:\n",
    "        return (\n",
    "            self.session_memory is None\n",
    "            and self.current_context_window_tokens >= self.min_tokens_to_init\n",
    "        )\n",
    "\n",
    "    # Helper method to determine if memory should be updated\n",
    "    def _should_update_memory(self) -> bool:\n",
    "        if self.session_memory is None:\n",
    "            return False\n",
    "        tokens_since = self.current_context_window_tokens - self.tokens_at_last_update\n",
    "        return tokens_since >= self.min_tokens_between_updates\n",
    "\n",
    "    # Methods to create initial session memory\n",
    "    def _create_session_memory(self, messages: list[dict]) -> str:\n",
    "        \"\"\"Generate initial session memory from messages.\"\"\"\n",
    "        transcript = build_transcript(messages)\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=1024,\n",
    "            system=\"\"\"You are a session memory agent. Compress the conversation into a structured summary \n",
    "that preserves all information needed to continue work seamlessly. Optimize for the assistant's \n",
    "ability to continue working, not human readability.\"\"\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Conversation transcript:\n",
    "{transcript}\n",
    "\n",
    "Create session memory using these instructions:\n",
    "{SESSION_CREATION_PROMPT}\n",
    "\n",
    "First analyze in <think>...</think> tags, then output the structured summary.\"\"\",\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        summary, _ = remove_thinking_blocks(response.content[0].text)  # clean up any <think> blocks because they are not needed in the session memory\n",
    "        return summary\n",
    "\n",
    "    def _update_session_memory(self, new_messages: list[dict]) -> str:\n",
    "        \"\"\"Update existing session memory with new messages. In practice, you may want to do this via file edit rather than full re-generation. But for demo purposes we do full regeneration here.\"\"\"\n",
    "        transcript = build_transcript(new_messages)\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=1024,\n",
    "            system=\"\"\"You are a session memory agent. Update the existing session memory with new information \n",
    "from the recent conversation. Preserve important existing details while integrating new content.\"\"\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Current session memory:\n",
    "{self.session_memory}\n",
    "\n",
    "New messages to integrate:\n",
    "{transcript}\n",
    "\n",
    "Update the session memory following these guidelines:\n",
    "{SESSION_CREATION_PROMPT}\n",
    "\n",
    "Output only the updated session memory (no analysis tags needed for updates).\n",
    "\n",
    "First analyze in <think>...</think> tags, then output the updated structured summary.\"\"\",\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        updated_summary, _ = remove_thinking_blocks(response.content[0].text)  # clean up any <think> blocks because they are not needed in the session memory\n",
    "        return updated_summary\n",
    "\n",
    "    # Background memory update methods\n",
    "    def _background_memory_update(\n",
    "        self, messages_snapshot: list[dict], snapshot_index: int, current_tokens: int\n",
    "    ):\n",
    "        \"\"\"Run session memory update in a background thread.\"\"\"\n",
    "        try:\n",
    "            with self._lock:\n",
    "                current_session_memory = self.session_memory\n",
    "                last_index = self.last_summarized_index\n",
    "\n",
    "            if current_session_memory is None:\n",
    "                new_memory = self._create_session_memory(messages_snapshot)\n",
    "            else:\n",
    "                # Get new messages since last summary\n",
    "                new_messages = messages_snapshot[last_index :]\n",
    "                if not new_messages:\n",
    "                    return\n",
    "                new_memory = self._update_session_memory(new_messages)\n",
    "\n",
    "            # Update state (thread-safe)\n",
    "            with self._lock:\n",
    "                self.session_memory = new_memory\n",
    "                self.last_summarized_index = snapshot_index\n",
    "                self.tokens_at_last_update = current_tokens\n",
    "                self.last_update_time = time.time()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   [Background] Error updating memory: {e}\")\n",
    "\n",
    "    # This makes sure only one background update runs at a time. If one is already running, we skip starting another. If not, we start a new thread to do the update.\n",
    "    def _trigger_background_update(self):\n",
    "        \"\"\"Trigger a background session memory update.\"\"\"\n",
    "        if self._update_thread is not None and self._update_thread.is_alive():\n",
    "            return\n",
    "\n",
    "        messages_snapshot = self.messages.copy()\n",
    "        snapshot_index = len(messages_snapshot)\n",
    "        current_tokens = self.current_context_window_tokens\n",
    "\n",
    "        self._update_thread = threading.Thread(\n",
    "            target=self._background_memory_update,\n",
    "            args=(messages_snapshot, snapshot_index, current_tokens),\n",
    "            daemon=True,\n",
    "        )\n",
    "        self._update_thread.start()\n",
    "\n",
    "    # Function to compact\n",
    "    def compact(self):\n",
    "        \"\"\"INSTANT compaction using pre-built session memory.\"\"\"\n",
    "        prev_msg_count = len(self.messages)\n",
    "\n",
    "        # Ensure session memory is ready. Shouldn't be an issue normally, but here for safety.\n",
    "        if self.session_memory is None:\n",
    "            if self._update_thread is not None and self._update_thread.is_alive():\n",
    "                print(\"   â³ Waiting for background memory update...\")\n",
    "                self._update_thread.join(timeout=30.0)\n",
    "\n",
    "            if self.session_memory is None:\n",
    "                print(\"   âš ï¸  No pre-built memory, creating synchronously...\")\n",
    "                start = time.perf_counter()\n",
    "                self.session_memory = self._create_session_memory(self.messages)\n",
    "                elapsed = time.perf_counter() - start\n",
    "                print(f\"   â±ï¸  Took {elapsed:.2f}s (but should be instant normally!)\")\n",
    "                self.last_summarized_index = len(self.messages)\n",
    "\n",
    "        with self._lock:\n",
    "            unsummarized = self.messages[self.last_summarized_index :]\n",
    "\n",
    "            summary_message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"This session is being continued from a previous conversation.\n",
    "\n",
    "    Here is the session memory:\n",
    "    {self.session_memory}\n",
    "\n",
    "    Continue from where we left off.\"\"\",\n",
    "            }\n",
    "\n",
    "            self.messages = [summary_message] + unsummarized\n",
    "            self.last_summarized_index = 1\n",
    "\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(f\"âš¡ INSTANT COMPACTION! Messages: {prev_msg_count} â†’ {len(self.messages)}\")\n",
    "            print(f\"   Session memory was pre-built (no wait time!)\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use of Instant Compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with instant compacting chat session...\n",
      "\n",
      "==============================================\n",
      "Turn 1: \n",
      "\n",
      "User: Explain Python decorators with a simple example.\n",
      "\n",
      "Assistant: \n",
      "# Python Decorators Explained\n",
      "\n",
      "A **decorator** is a function that modifies or enhances another function or class without permanently changing its source code. It wraps a function with additional functionality.\n",
      "... (66 more lines)\n",
      "\n",
      " \n",
      "Turn end state: \n",
      "Input=41 |Output=418 | Messages=2 | Memory: no memory created yet\n",
      "\n",
      "==============================================\n",
      "Turn 2: \n",
      "\n",
      "User: Now show me a decorator that logs function arguments.\n",
      "\n",
      "Assistant: \n",
      "# Function Argument Logging Decorator\n",
      "\n",
      "Here are several approaches, from simple to advanced:\n",
      "... (104 more lines)\n",
      "\n",
      " \n",
      "Turn end state: \n",
      "Input=472 |Output=750 | Messages=4 | Memory: no memory created yet\n",
      "\n",
      "On the next response, the current conversation tokens will be: 1222\n",
      "   [Background] Proactively initializing session memory...\n",
      "==============================================\n",
      "Turn 3: \n",
      "\n",
      "User: How do I make a decorator that accepts parameters?\n",
      "\n",
      "Assistant: \n",
      "# Decorators with Parameters\n",
      "\n",
      "To create a decorator that accepts parameters, you need an extra layer of nesting. Here's how:\n",
      "... (143 more lines)\n",
      "\n",
      " \n",
      "Turn end state: \n",
      "Input=1,235 |Output=1,024 | Messages=6 | Memory: ready\n",
      "\n",
      "On the next response, the current conversation tokens will be: 2259\n",
      "   [Background] Proactively updating session memory...\n"
     ]
    }
   ],
   "source": [
    "# Low thresholds for demo - in production you'd use higher values\n",
    "session = InstantCompactingChatSession(\n",
    "    context_limit=1500,\n",
    "    min_tokens_to_init=700,\n",
    "    min_tokens_between_updates=300,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    \"Explain Python decorators with a simple example.\",\n",
    "    \"Now show me a decorator that logs function arguments.\",\n",
    "    \"How do I make a decorator that accepts parameters?\",\n",
    "]\n",
    "print(\"Starting conversation with instant compacting chat session...\\n\")\n",
    "\n",
    "turn_count = 0\n",
    "for i, message in enumerate(messages, 1):\n",
    "    response, usage, background_status = session.chat(message)\n",
    "    turn_count += 1\n",
    "    print(\n",
    "        f\"==============================================\\n\"\n",
    "        f\"Turn {turn_count}: \"\n",
    "    )\n",
    "    print(f\"\\nUser: {message}\")\n",
    "    print(f\"\\nAssistant: \\n{truncate_response(response, max_lines=3)}\")\n",
    "    print(f\"\\n \\nTurn end state: \"\n",
    "        f\"\\nInput={usage.input_tokens:,} |\"\n",
    "        f\"Output={usage.output_tokens:,} | \"\n",
    "        f\"Messages={len(session.messages)} | \"\n",
    "        f\"Memory: {'ready' if session.session_memory else 'no memory created yet'}\\n\"\n",
    "    )\n",
    "    if background_status:\n",
    "        print(\"On the next response, the current conversation tokens will be:\", session.current_context_window_tokens)\n",
    "        print(f\"   [Background] Proactively {background_status} session memory...\")\n",
    "    # Sleep to allow background updates to complete for demo purposes\n",
    "    if i < len(messages):\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âš¡ INSTANT COMPACTION! Messages: 6 â†’ 1\n",
      "   Kept 0 unsummarized messages\n",
      "   Session memory was pre-built (no wait time!)\n",
      "============================================================\n",
      "\n",
      "User: What did we just talk about?\n",
      "\n",
      "Assistant: \n",
      "# Session Summary\n",
      "\n",
      "We just completed a comprehensive discussion on **parameterized decorators** (your third request in this session).\n",
      "... (33 more lines)\n",
      "\n",
      " \n",
      "Turn end state: \n",
      "Input=778 |Output=386 | Messages=3 | Memory: ready\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message = \"What did we just talk about?\"\n",
    "response, usage, background_status = session.chat(message)\n",
    "print(f\"\\nUser: {message}\")\n",
    "print(f\"\\nAssistant: \\n{truncate_response(response, max_lines=3)}\")\n",
    "print(f\"\\n \\nTurn end state: \"\n",
    "    f\"\\nInput={usage.input_tokens:,} |\"\n",
    "    f\"Output={usage.output_tokens:,} | \"\n",
    "    f\"Messages={len(session.messages)} | \"\n",
    "    f\"Memory: {'ready' if session.session_memory else 'no memory created yet'}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: What are some good follow up topics we should cover?\n",
      "\n",
      "Assistant: \n",
      "# Recommended Follow-Up Topics\n",
      "\n",
      "Based on your progression through decorator fundamentals â†’ logging â†’ parameterized decorators, here are the most logical next steps:\n",
      "... (63 more lines)\n",
      "\n",
      " \n",
      "Turn end state: \n",
      "Input=1,178 |Output=723 | Messages=5 | Memory: ready\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message = \"What are some good follow up topics we should cover?\"\n",
    "response, usage, background_status = session.chat(message)\n",
    "print(f\"\\nUser: {message}\")\n",
    "print(f\"\\nAssistant: \\n{truncate_response(response, max_lines=3)}\")\n",
    "print(f\"\\n \\nTurn end state: \"\n",
    "    f\"\\nInput={usage.input_tokens:,} |\"\n",
    "    f\"Output={usage.output_tokens:,} | \"\n",
    "    f\"Messages={len(session.messages)} | \"\n",
    "    f\"Memory: {'ready' if session.session_memory else 'no memory created yet'}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Adding Prompt Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\nThe background updates can be made **~10x cheaper** by using prompt caching. The trick:\n1. Pass the **full conversation** to the background summarizer\n2. Add `cache_control` markers so subsequent requests hit the cache\n3. Only the new \"summarize this\" instruction is billed at full price\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    PROMPT CACHING FOR LONG CONVERSATIONS                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                                 â”‚\nâ”‚  WITHOUT CACHING: Pay full price for entire context every turn                 â”‚\nâ”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                   â”‚\nâ”‚                                                                                 â”‚\nâ”‚  Turn 1:  [System][User1][Asst1]                         â†’  500 tokens  @ $3/M â”‚\nâ”‚  Turn 2:  [System][User1][Asst1][User2][Asst2]           â†’ 1500 tokens  @ $3/M â”‚\nâ”‚  Turn 3:  [System][User1][Asst1][User2][Asst2][User3]... â†’ 3000 tokens  @ $3/M â”‚\nâ”‚  Turn 4:  [System][User1][Asst1][User2][Asst2][User3]... â†’ 5000 tokens  @ $3/M â”‚\nâ”‚           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚\nâ”‚                                              Total: 10,000 tokens = $0.030      â”‚\nâ”‚                                                                                 â”‚\nâ”‚                                                                                 â”‚\nâ”‚  WITH CACHING: Pay full price once, then 90% discount on prefix                â”‚\nâ”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                â”‚\nâ”‚                                                                                 â”‚\nâ”‚  Turn 1:  [System][User1][Asst1]â—†                        â†’  500 tokens  @ $3/M â”‚\nâ”‚                                â–²                            (cache created)    â”‚\nâ”‚                          cache breakpoint                                       â”‚\nâ”‚                                                                                 â”‚\nâ”‚  Turn 2:  [System][User1][Asst1][User2][Asst2]â—†                                â”‚\nâ”‚           â•°â”€â”€â”€â”€â”€â”€â”€ cached â”€â”€â”€â”€â”€â”€â•¯                                              â”‚\nâ”‚                500 @ $0.30/M + 1000 new @ $3/M  =  $0.0032                     â”‚\nâ”‚                                                                                 â”‚\nâ”‚  Turn 3:  [System][User1][Asst1][User2][Asst2][User3][Asst3]â—†                  â”‚\nâ”‚           â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ cached â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯                                  â”‚\nâ”‚               1500 @ $0.30/M + 1500 new @ $3/M  =  $0.0050                     â”‚\nâ”‚                                                                                 â”‚\nâ”‚  Turn 4:  [System][User1][Asst1][User2][Asst2][User3][Asst3][User4][Asst4]â—†    â”‚\nâ”‚           â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ cached â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯                 â”‚\nâ”‚                     3000 @ $0.30/M + 2000 new @ $3/M  =  $0.0069               â”‚\nâ”‚           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚\nâ”‚                                              Total: $0.0166  (45% savings)     â”‚\nâ”‚                                                                                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                                 â”‚\nâ”‚  COMPACTION + CACHING: Double benefit                                           â”‚\nâ”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                           â”‚\nâ”‚                                                                                 â”‚\nâ”‚    Main Chat                      Background Summarizer                         â”‚\nâ”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚\nâ”‚                                                                                 â”‚\nâ”‚  [Conversation grows...]          [Same conversation prefix]â—† + [Summarize!]   â”‚\nâ”‚         â”‚                                    â”‚                                  â”‚\nâ”‚         â”‚                         Cache hit! Only pays for                      â”‚\nâ”‚         â”‚                         the summarization prompt                      â”‚\nâ”‚         â”‚                                    â”‚                                  â”‚\nâ”‚         â–¼                                    â–¼                                  â”‚\nâ”‚  Context limit reached  â”€â”€â”€â”€â”€â”€â–º  Session memory ready instantly                â”‚\nâ”‚                                  (built cheaply in background)                  â”‚\nâ”‚                                                                                 â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚  Key insight: The background summarizer reuses the same conversation     â”‚  â”‚\nâ”‚  â”‚  prefix that was just sent to the main chat - automatic cache hit!       â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ—† = cache_control breakpoint (cache everything before this point)\n```\n\n### Why this matters for compaction\n\n| Scenario | Cost per background update | Notes |\n|----------|---------------------------|-------|\n| No caching | Full input cost | 5,000 tokens Ã— $3/M = $0.015 |\n| With caching | ~10% of input cost | 500 new + 4,500 cached = $0.003 |\n| **Savings** | **~80%** | Compounds over many updates |\n\nThe longer the conversation, the bigger the savingsâ€”exactly when you need compaction most!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the Caching Works\n",
    "\n",
    "The key is in `_add_cache_control()` and `_create_session_memory_cached()`:\n",
    "\n",
    "```python\n",
    "# 1. Mark the last conversation message with cache_control\n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\n",
    "        \"type\": \"text\",\n",
    "        \"text\": msg[\"content\"],\n",
    "        \"cache_control\": {\"type\": \"ephemeral\"}  # <-- This creates a cache breakpoint\n",
    "    }]\n",
    "}\n",
    "\n",
    "# 2. Also mark the system prompt\n",
    "system=[{\n",
    "    \"type\": \"text\",\n",
    "    \"text\": \"You are a session memory agent...\",\n",
    "    \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "}]\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- The first background update creates a cache entry for `[System + Messages]`\n",
    "- Subsequent updates with the same message prefix get **cache hits**\n",
    "- Only the new summarization instruction is billed at full price\n",
    "- Cache entries have a 5-minute TTL, so rapid updates benefit most\n",
    "\n",
    "**Cost math:**\n",
    "- Without caching: 5,000 tokens Ã— $3.00/1M = $0.015 per update\n",
    "- With caching: 500 new tokens Ã— $3.00/1M + 4,500 cached Ã— $0.30/1M = $0.00285\n",
    "- **Savings: ~80%** on background summarization costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "SMARTER_MODEL = \"claude-sonnet-4-5-20250929\"\n",
    "class CachedInstantCompactingChatSession(InstantCompactingChatSession):\n",
    "    \"\"\"Instant compacting session with prompt caching enabled.\"\"\"\n",
    "    \n",
    "    def _add_cache_control(self, messages: list[dict]) -> list[dict]:\n",
    "        \"\"\"Convert all messages to list format for consistent structure, with cache_control on the last message.\n",
    "\n",
    "        For prompt caching to work, the message prefix structure must be identical between requests.\n",
    "        If we only convert the last message to list format, previous messages change from listâ†’string\n",
    "        on the next turn, breaking the cache match.\n",
    "        \"\"\"\n",
    "        if not messages:\n",
    "            return messages\n",
    "\n",
    "        cached_messages = []\n",
    "        for i, msg in enumerate(messages):\n",
    "            is_last = (i == len(messages) - 1)\n",
    "            content_block = {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": msg[\"content\"],\n",
    "            }\n",
    "            if is_last:\n",
    "                content_block[\"cache_control\"] = {\"type\": \"ephemeral\"}\n",
    "\n",
    "            cached_messages.append({\n",
    "                \"role\": msg[\"role\"],\n",
    "                \"content\": [content_block],\n",
    "            })\n",
    "\n",
    "        return cached_messages\n",
    "\n",
    "    def chat(self, user_message: str):\n",
    "        if self.current_context_window_tokens >= self.context_limit:\n",
    "            self.compact()\n",
    "\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=SMARTER_MODEL,\n",
    "            max_tokens=1024,\n",
    "            system=\"You are a helpful coding assistant.\",\n",
    "            messages=self._add_cache_control(self.messages),\n",
    "        )\n",
    "\n",
    "        assistant_message = response.content[0].text\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "        self.current_context_window_tokens = response.usage.input_tokens + response.usage.output_tokens\n",
    "\n",
    "        background_status = None\n",
    "        if self._should_init_memory() or self._should_update_memory():\n",
    "            self._trigger_background_update()\n",
    "            background_status = \"initializing\" if self.session_memory is None else \"updating\"\n",
    "\n",
    "        return assistant_message, response.usage, background_status\n",
    "\n",
    "    def _create_session_memory(self, messages: list[dict]) -> str:\n",
    "        transcript = build_transcript(messages)\n",
    "\n",
    "        prompt_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"\"\"Conversation transcript:\n",
    "{transcript}\n",
    "\n",
    "Create session memory using these instructions:\n",
    "{SESSION_CREATION_PROMPT}\n",
    "\n",
    "First analyze in <think>...</think> tags, then output the structured summary.\"\"\",\n",
    "                        \"cache_control\": {\"type\": \"ephemeral\"},\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=SMARTER_MODEL,\n",
    "            max_tokens=1024,\n",
    "            system=\"\"\"You are a session memory agent. Compress the conversation into a structured summary \n",
    "that preserves all information needed to continue work seamlessly. Optimize for the assistant's \n",
    "ability to continue working, not human readability.\"\"\",\n",
    "            messages=prompt_messages,\n",
    "        )\n",
    "        summary, _ = remove_thinking_blocks(response.content[0].text)\n",
    "        return summary\n",
    "\n",
    "    def _update_session_memory(self, new_messages: list[dict]) -> str:\n",
    "        transcript = build_transcript(new_messages)\n",
    "\n",
    "        prompt_messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"\"\"Current session memory:\n",
    "{self.session_memory}\n",
    "\n",
    "New messages to integrate:\n",
    "{transcript}\n",
    "\n",
    "Update the session memory following these guidelines:\n",
    "{SESSION_CREATION_PROMPT}\n",
    "\n",
    "Output only the updated session memory (no analysis tags needed for updates).\n",
    "\n",
    "First analyze in <think>...</think> tags, then output the updated structured summary.\"\"\",\n",
    "                        \"cache_control\": {\"type\": \"ephemeral\"},\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=SMARTER_MODEL,\n",
    "            max_tokens=1024,\n",
    "            system=\"\"\"You are a session memory agent. Update the existing session memory with new information \n",
    "from the recent conversation. Preserve important existing details while integrating new content.\"\"\",\n",
    "            messages=prompt_messages,\n",
    "        )\n",
    "        updated_summary, _ = remove_thinking_blocks(response.content[0].text)\n",
    "        return updated_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with CACHED instant compacting chat session...\n",
      "\n",
      "==============================================\n",
      "Turn 1:\n",
      "\n",
      "User: Explain Python decorators with a simple example.\n",
      "\n",
      "Assistant: \n",
      "# Python Decorators Explained\n",
      "\n",
      "## What is a Decorator?\n",
      "... (66 more lines)\n",
      "\n",
      "Turn end state:\n",
      "  Input=41 | Output=422\n",
      "  Cache: 0 read, 0 created (0% hit rate)\n",
      "  Messages=2 | Memory: not yet\n",
      "==============================================\n",
      "Turn 2:\n",
      "\n",
      "User: Now show me a decorator that logs function arguments.\n",
      "\n",
      "Assistant: \n",
      "# Decorator that Logs Function Arguments\n",
      "\n",
      "## Basic Logging Decorator\n",
      "... (124 more lines)\n",
      "\n",
      "Turn end state:\n",
      "  Input=476 | Output=1,024\n",
      "  Cache: 0 read, 0 created (0% hit rate)\n",
      "  Messages=4 | Memory: not yet\n",
      "\n",
      "  [Background] Proactively initializing session memory...\n",
      "==============================================\n",
      "Turn 3:\n",
      "\n",
      "User: How do I make a decorator that accepts parameters?\n",
      "\n",
      "Assistant: \n",
      "# Decorators with Parameters\n",
      "\n",
      "## The Pattern: Three Levels of Functions\n",
      "... (144 more lines)\n",
      "\n",
      "Turn end state:\n",
      "  Input=1,516 | Output=1,024\n",
      "  Cache: 0 read, 0 created (0% hit rate)\n",
      "  Messages=6 | Memory: not yet\n",
      "\n",
      "  [Background] Proactively initializing session memory...\n"
     ]
    }
   ],
   "source": [
    "# Low thresholds for demo - in production you'd use higher values\n",
    "session = CachedInstantCompactingChatSession(\n",
    "    context_limit=2500,\n",
    "    min_tokens_to_init=1000,\n",
    "    min_tokens_between_updates=500,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    \"Explain Python decorators with a simple example.\",\n",
    "    \"Now show me a decorator that logs function arguments.\",\n",
    "    \"How do I make a decorator that accepts parameters?\",\n",
    "]\n",
    "print(\"Starting conversation with CACHED instant compacting chat session...\\n\")\n",
    "\n",
    "turn_count = 0\n",
    "for i, message in enumerate(messages, 1):\n",
    "    response, usage, bg_status = session.chat(message)\n",
    "    turn_count += 1\n",
    "    \n",
    "    # Cache stats\n",
    "    cache_created = getattr(usage, 'cache_creation_input_tokens', 0) or 0\n",
    "    cache_read = getattr(usage, 'cache_read_input_tokens', 0) or 0\n",
    "    cache_hit_pct = (cache_read / usage.input_tokens * 100) if usage.input_tokens > 0 else 0\n",
    "    \n",
    "    print(\n",
    "        f\"==============================================\\n\"\n",
    "        f\"Turn {turn_count}:\"\n",
    "    )\n",
    "    print(f\"\\nUser: {message}\")\n",
    "    print(f\"\\nAssistant: \\n{truncate_response(response, max_lines=3)}\")\n",
    "    print(\n",
    "        f\"\\nTurn end state:\"\n",
    "        f\"\\n  Input={usage.input_tokens:,} | Output={usage.output_tokens:,}\"\n",
    "        f\"\\n  Cache: {cache_read:,} read, {cache_created:,} created ({cache_hit_pct:.0f}% hit rate)\"\n",
    "        f\"\\n  Messages={len(session.messages)} | Memory: {'ready' if session.session_memory else 'not yet'}\"\n",
    "    )\n",
    "    \n",
    "    if bg_status:\n",
    "        print(f\"\\n  [Background] Proactively {bg_status} session memory...\")\n",
    "    \n",
    "    if i < len(messages):\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âš¡ INSTANT COMPACTION! Messages: 6 â†’ 3\n",
      "   Session memory was pre-built (no wait time!)\n",
      "============================================================\n",
      "\n",
      "User: What did we just talk about?\n",
      "\n",
      "Assistant: \n",
      "We just talked about **decorators with parameters** in Python!\n",
      "\n",
      "## Quick Summary:\n",
      "... (28 more lines)\n",
      "\n",
      " \n",
      "Turn end state: \n",
      "Input=1,780 |Output=347 | Messages=5 | Memory: ready\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message = \"What did we just talk about?\"\n",
    "response, usage, background_status = session.chat(message)\n",
    "print(f\"\\nUser: {message}\")\n",
    "print(f\"\\nAssistant: \\n{truncate_response(response, max_lines=3)}\")\n",
    "print(f\"\\n \\nTurn end state: \"\n",
    "    f\"\\nInput={usage.input_tokens:,} |\"\n",
    "    f\"Output={usage.output_tokens:,} | \"\n",
    "    f\"Messages={len(session.messages)} | \"\n",
    "    f\"Memory: {'ready' if session.session_memory else 'no memory created yet'}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Print the structure of _add_cache_control output\n",
    "import json\n",
    "\n",
    "cached_messages = session._add_cache_control(session.messages)\n",
    "print(\"Number of messages:\", len(cached_messages))\n",
    "print(\"\\nStructure of cached messages:\")\n",
    "for i, msg in enumerate(cached_messages):\n",
    "    print(f\"\\n--- Message {i} (role: {msg['role']}) ---\")\n",
    "    if isinstance(msg.get('content'), list):\n",
    "        print(f\"Content is a list with {len(msg['content'])} item(s)\")\n",
    "        for j, block in enumerate(msg['content']):\n",
    "            print(f\"  Block {j}: type={block.get('type')}, has cache_control={('cache_control' in block)}\")\n",
    "            if 'cache_control' in block:\n",
    "                print(f\"    cache_control: {block['cache_control']}\")\n",
    "    else:\n",
    "        content_preview = str(msg.get('content', ''))[:100]\n",
    "        print(f\"Content is string: {content_preview}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coconut",
   "language": "coconut",
   "name": "coconut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".coco",
   "mimetype": "text/x-python3",
   "name": "coconut",
   "pygments_lexer": "coconut",
   "version": "3.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}