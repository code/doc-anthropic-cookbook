{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session memory compaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cookbook covers three main topics:\n",
    "1. Writing a quality prompt to compact session chat history\n",
    "2. Utlizing instant compacting to improve user chat experience\n",
    "3. Managing costs and latency for long context conversations using prompt caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals: writing a compaction prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a well structured session memory prompt. \n",
    "\n",
    "Some best practices include:\n",
    "- Use chain-of-thought before summarizing ‚Äî analyze first, then output                                                                                         \n",
    "- Enumerate exactly what to preserve: file paths, code snippets, errors, user corrections                                                                      \n",
    "- Weight recency heavily ‚Äî the end of the conversation is the active context                                                                                   \n",
    "- Require verbatim quotes for next steps to prevent task drift                                                                                                 \n",
    "- Use structured sections with token budgets per section                                                                                                       \n",
    "- Include a \"Current State\" section that always reflects the moment of compaction\n",
    "\n",
    "Some pitfalls include:\n",
    "- Vague prompts like \"summarize this conversation\" produce lossy output                                                                                        \n",
    "- Treating all messages equally loses the active working context                                                                                               \n",
    "- Paraphrasing next steps introduces subtle drift that compounds                                                                                               \n",
    "- Omitting error history causes the model to retry failed approaches                                                                                           \n",
    "- Dropping user corrections makes the model revert to old behaviors                                                                                            \n",
    "- No token limits lets one section consume the entire summary                                                                                                  \n",
    "- Summarizing for human readability instead of model continuity\n",
    "- Having the agent try to compress the results of tool calls here - this can be retrieved later if the agent needs it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "SESSION_MEMORY_PROMPT = \"\"\"\n",
    "You are a session memory agent. Compress the conversation into a structured summary \n",
    "that preserves all information needed to continue work seamlessly. Optimize for the assistant's \n",
    "ability to continue working, not human readability.\n",
    "\n",
    "<analysis-instructions>\n",
    "Before generating your summary, analyze the transcript in <think>...</think> tags:\n",
    "1. What did the user originally request? (Exact phrasing)\n",
    "2. What actions succeeded? What failed and why?\n",
    "3. Did the user correct or redirect the assistant at any point?\n",
    "4. What was actively being worked on at the end?\n",
    "5. What tasks remain incomplete or pending?\n",
    "6. What specific details (IDs, paths, values, names) must survive compression?\n",
    "</analysis-instructions>\n",
    "\n",
    "<summary-format>\n",
    "## User Intent\n",
    "The user's original request and any refinements. Use direct quotes for key requirements.\n",
    "If the user's goal evolved during the conversation, capture that progression.\n",
    "\n",
    "## Completed Work\n",
    "Actions successfully performed. Be specific:\n",
    "- What was created, modified, or deleted\n",
    "- Exact identifiers (file paths, record IDs, URLs, names)\n",
    "- Specific values, configurations, or settings applied\n",
    "\n",
    "## Errors & Corrections\n",
    "- Problems encountered and how they were resolved\n",
    "- Approaches that failed (so they aren't retried)\n",
    "- User corrections: \"don't do X\", \"actually I meant Y\", \"that's wrong because...\"\n",
    "Capture corrections verbatim‚Äîthese represent learned preferences.\n",
    "\n",
    "## Active Work\n",
    "What was in progress when the session ended. Include:\n",
    "- The specific task being performed\n",
    "- Direct quotes showing exactly where work left off\n",
    "- Any partial results or intermediate state\n",
    "\n",
    "## Pending Tasks\n",
    "Remaining items the user requested that haven't been started.\n",
    "Distinguish between \"explicitly requested\" and \"implied/assumed.\"\n",
    "\n",
    "## Key References\n",
    "Important details needed to continue:\n",
    "- Identifiers: IDs, paths, URLs, names, keys\n",
    "- Values: numbers, dates, configurations, credentials (redacted)\n",
    "- Context: relevant background information, constraints, preferences\n",
    "- Citations: sources referenced during the conversation\n",
    "</summary-format>\n",
    "\n",
    "<preserve-rules>\n",
    "Always preserve when present:\n",
    "- Exact identifiers (IDs, paths, URLs, keys, names)\n",
    "- Error messages verbatim\n",
    "- User corrections and negative feedback\n",
    "- Specific values, formulas, or configurations\n",
    "- Technical constraints or requirements discovered\n",
    "- The precise state of any in-progress work\n",
    "</preserve-rules>\n",
    "\n",
    "<compression-rules>\n",
    "- Weight recent messages more heavily‚Äîthe end of the transcript is the active context\n",
    "- Omit pleasantries, acknowledgments, and filler (\"Sure!\", \"Great question\")\n",
    "- Omit system context that will be re-injected separately\n",
    "- Keep each section under 500 words; condense older content to make room for recent\n",
    "- If you must cut details, preserve: user corrections > errors > active work > completed work\n",
    "</compression-rules>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional compacting\n",
    "In traditional compaction, you generate one summary once the token threshold is reached.\n",
    "Traditional compaction is slow: when you hit the context limit, you wait for a summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "TRADITIONAL COMPACTION (slow)\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Turn 1 ‚Üí Turn 2 ‚Üí Turn 3 ‚Üí ... ‚Üí Turn N ‚Üí CONTEXT FULL!\n",
    "                                              ‚îÇ\n",
    "                                              ‚ñº\n",
    "                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                                    ‚îÇ Generate summary‚îÇ\n",
    "                                    ‚îÇ ( USER WAITS !) ‚îÇ\n",
    "                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                              ‚îÇ\n",
    "                                              ‚ñº\n",
    "                                         Continue\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": "# setup, we are using haiku for demo purposes\nimport anthropic\nfrom anthropic.types import MessageParam, TextBlockParam\nimport warnings\nimport os\n\n# Suppress noisy FutureWarning from coconut compiler\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"coconut\")\n\nclient = anthropic.Anthropic()\nMODEL = \"claude-sonnet-4-5\"\n                                                                                     \nimport pandas as pd                                                                                                                         \npd.set_option('display.max_rows', None)                                                                                                     \npd.set_option('display.max_columns', None)                                                                                                  \n                                                                                                                                                                                                                                               \nfrom IPython.display import display, HTML                                                                                                   \ndisplay(HTML('<style>div.output_scroll { height: unset; }</style>'))        \n\n# helper functions:\ndef truncate_response(text: str, max_lines: int = 15) -> str:\n    \"\"\"Truncate long responses for cleaner output display.\"\"\"\n    lines = text.strip().split(\"\\n\")\n    if len(lines) <= max_lines:\n        return text\n    return \"\\n\".join(lines[:max_lines]) + f\"\\n... ({len(lines) - max_lines} more lines)\"\n\ndef build_transcript(messages: list[dict]) -> str:\n    lines = []\n    for msg in messages:\n        role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n        lines.append(f\"{role}: {msg['content']}\")\n    return \"\\n\\n\".join(lines)\n\ndef remove_thinking_blocks(text: str):\n    \"\"\"Remove <think>...</think> blocks from the text.\"\"\"\n    import re\n\n    matches = re.findall(r\"<think>.*?</think>\", text, flags=re.DOTALL)\n    cleaned = re.sub(r\"<think>.*?</think>\\s*\", \"\", text, flags=re.DOTALL).strip()\n    return cleaned, \"\".join(matches)\n\ndef add_cache_control(messages: list[dict]) -> list[MessageParam]:\n    \"\"\"Add cache_control to the last user message for prompt caching.\n\n    For prompt caching to work, the message prefix structure must be identical between requests.\n    All messages are converted to list format for consistency, and cache_control is placed on\n    the last user message to match the standard API call pattern.\n    \"\"\"\n    cached_messages: list[MessageParam] = []\n    last_user_idx = None\n\n    # Find last user message index\n    for i, msg in enumerate(messages):\n        if msg[\"role\"] == \"user\":\n            last_user_idx = i\n\n    for i, msg in enumerate(messages):\n        content = msg[\"content\"]\n        text = content if isinstance(content, str) else content[0][\"text\"]\n\n        content_block: TextBlockParam = {\"type\": \"text\", \"text\": text}\n        if i == last_user_idx:\n            content_block[\"cache_control\"] = {\"type\": \"ephemeral\"}\n\n        cached_messages.append({\"role\": msg[\"role\"], \"content\": [content_block]})\n\n    return cached_messages"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example use of traditional compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": "import time\n\nclass TraditionalCompactingChatSession:\n    \"\"\"Traditional chat session with compaction after the fact.\"\"\"\n    def __init__(self, system_message=\"You are a helpful assistant\", context_limit: int = 10000):\n        self.system_message = system_message\n        self.context_limit = context_limit # the point at which the conversation is compacted so it does not exceed model limits.\n        self.messages = []\n        self.current_context_window_tokens = 0\n        self.summary = None\n    \n    def chat(self, user_message: str):\n        # In traditional compaction, we check if we need to compact when the user sends a message. NOT IDEAL!\n        if self.current_context_window_tokens >= self.context_limit:\n            print(f\"\\nüßπ Context window at {self.current_context_window_tokens} tokens. Limit exceeded, compacting session memory...\")\n            self.compact() # compacts everything before the new user message\n        \n        self.messages.append({\"role\": \"user\", \"content\": user_message})      \n        print(f\"\\nUser: {user_message}\")                                                                                        \n\n        response = client.messages.create(\n            model=MODEL,\n            max_tokens=3500,\n            system=self.system_message,\n            messages=add_cache_control(self.messages)\n        )\n        assistant_message = response.content[0].text\n        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n       \n        print(f\"\\nAssistant: \\n{truncate_response(assistant_message, max_lines=15)}\")\n        \n        # approximate current token count in the conversation before the next user message\n        cache_read = getattr(response.usage, \"cache_read_input_tokens\", 0) or 0                                                                                                                          \n        total_input = response.usage.input_tokens + cache_read                                                                                                                                           \n        self.current_context_window_tokens = total_input + response.usage.output_tokens                                                     \n       \n        print(\n            f\"Input={total_input:,}, Prompt cached used= {cache_read > 0} | \"\n            f\"Output={response.usage.output_tokens:,} | \"\n            f\"Messages={len(self.messages)}\"\n        )\n        return assistant_message, response.usage\n    \n    def compact(self): \n        start_time = time.perf_counter()\n        \n        # Put compaction instructions in user message to share cache with main chat\n        compaction_instruction = f\"\"\"\nFor this task, act as a session memory agent. Your previous role is paused.\n\n{SESSION_MEMORY_PROMPT}\n\nThe full transcript of our conversation so far is included above. Compress it now.\n\"\"\"\n        self.messages.append({\"role\": \"user\", \"content\": compaction_instruction})\n        response = client.messages.create(\n            model=MODEL,\n            max_tokens=5000,\n            system=self.system_message,  # Same as main chat for cache sharing\n            messages=add_cache_control(self.messages)\n        )\n        print(response)\n        elapsed = time.perf_counter() - start_time\n        \n        # Generate new summary message\n        self.summary, removed_text = remove_thinking_blocks(response.content[0].text) # clean up any <think> blocks because they are not needed in the session memory\n        approximate_summary_tokens = response.usage.output_tokens - round(len(removed_text) / 4)  # rough estimate of tokens removed from summary\n       \n        # Replace prior messages with new summary message\n        self.messages = [{\n            \"role\": \"user\",\n            \"content\": f\"\"\"This session is being continued from a previous conversation. Here is the session memory: {self.summary}.Continue from where we left off.\"\"\"\n        }]\n        \n        # Show token reduction if we just compacted\n        reduction = self.current_context_window_tokens - approximate_summary_tokens\n        pct = (reduction / self.current_context_window_tokens) * 100\n        \n        print(f\"\\n{'-' * 60}\")\n        print(f\"üìù New session memory created.\")\n        print(f\"‚úÖ Tokens reduced: {self.current_context_window_tokens:,} ‚Üí {approximate_summary_tokens:.0f} ({reduction:,} tokens saved, {pct:.0f}% reduction)\")\n        print(f\"‚è±Ô∏è Compaction time: {elapsed:.2f}s (user waiting...)\")\n        print(f\"{'-' * 60}\")\n        \n        # Update token count to reflect compacted state\n        self.current_context_window_tokens = approximate_summary_tokens"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a short story writer who helps authors develop their ideas into compelling narratives.\n",
    "\n",
    "## What You Do\n",
    "\n",
    "**Plot Development**\n",
    "- Help authors work through story structure, pacing, and narrative arc\n",
    "- Identify plot holes, inconsistencies, or missed opportunities\n",
    "- Suggest ways to raise stakes, add tension, or deepen conflict\n",
    "- Brainstorm twists, resolutions, and scene transitions\n",
    "\n",
    "**Character Development**\n",
    "- Develop backstories, motivations, and internal conflicts\n",
    "- Ensure characters have distinct voices and consistent behavior\n",
    "- Explore character relationships and how they drive the plot\n",
    "- Help authors understand what their characters want vs. what they need\n",
    "\n",
    "**Drafting**\n",
    "- Write short stories or scenes based on the author's ideas and direction\n",
    "- Match tone, genre conventions, and stylistic preferences\n",
    "- Show rather than tell when bringing scenes to life\n",
    "- Craft dialogue that reveals character and advances plot\n",
    "\n",
    "## How You Work\n",
    "- You are the lead writer. When you disagree with a creative choice, say so respectfully, but ultimately defer to what the author wants.\n",
    "- DO NOT ask the user to provide more context or clarify their request. Assume you have enough information to proceed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation...\n",
      "\n",
      "==============================================\n",
      "Turn 1:\n",
      "\n",
      "\n",
      "User: I want to create a story about a young detective solving a mysterious case in a small town. Generate 3 well throught out plot ideas for me to consider.\n",
      "\n",
      "Assistant: \n",
      "# Three Mystery Plot Ideas\n",
      "\n",
      "## 1. **The Silence Pact**\n",
      "\n",
      "**Setup:** A beloved high school teacher dies in what appears to be a tragic accident during a school field trip to nearby caves. Our detective, fresh out of the academy and assigned to this sleepy town as a \"soft landing,\" is told it's an open-and-shut case. But something feels wrong‚Äîthe students who were there have all gone eerily quiet, even with each other.\n",
      "\n",
      "**The Mystery:** As our detective digs deeper, they discover the students aren't protecting a person‚Äîthey're protecting a secret about the town itself. Twenty years ago, another teacher disappeared during a similar field trip, and the current town leadership (mayor, police chief, school board) were all students on *that* trip. The dead teacher had discovered what really happened and was killed to keep the past buried.\n",
      "\n",
      "**The Complication:** The detective's own mentor‚Äîthe retiring detective they're replacing‚Äîwas the lead investigator who closed the original case as a \"runaway.\" Solving this means destroying the man who gave them their career, and possibly exposing that he was complicit in the cover-up all along.\n",
      "\n",
      "**Emotional Core:** Justice vs. loyalty; the cost of truth in a community built on comfortable lies.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. **The Inheritance Game**\n",
      "... (24 more lines)\n",
      "Input=318, Prompt cached used= False | Output=1,004 | Messages=2\n",
      "==============================================\n",
      "Turn 2:\n",
      "\n",
      "\n",
      "User: I don't like those ideas, can you think of one plot something more unique and unexpected?\n",
      "\n",
      "Assistant: \n",
      "# **The Cartographer's Daughter**\n",
      "\n",
      "**Setup:** Our young detective arrives in a mountain town of 3,000 people to investigate a truly bizarre crime: someone has been systematically stealing *maps*. Not valuable antiques‚Äîjust maps. From the library, the historical society, the ranger station, even GPS units from parked cars. The thefts escalate to break-ins, and finally, the town's oldest resident‚Äîa retired cartographer‚Äîis murdered in her home. All her maps, a lifetime of work documenting the region, are gone.\n",
      "\n",
      "**The Mystery:** The detective discovers the cartographer had been deliberately mapping things *wrong* for sixty years. Roads that don't quite connect. Trails that dead-end where her maps say they continue. Property lines slightly off. She'd been systematically introducing tiny errors into every official map of the region, and her personal maps‚Äîthe stolen ones‚Äîshow what's *really* there.\n",
      "\n",
      "What's really there: an entire ghost neighborhood. Eight families have been living on what they believed was public land for three generations, building homes, raising children, completely off-grid. They thought they were just deeply rural. But the cartographer's accurate maps prove they're squatting on land owned by a development company that's just been sold. The new owners want to build a resort. The thefts weren't about the maps‚Äîthey were about erasing proof that this land is occupied, so the families can be removed as \"trespassers\" rather than residents with potential legal claims.\n",
      "\n",
      "**The Complication:** The detective finds the hidden community and realizes the murderer is one of the eight families‚Äîsomeone who killed the cartographer to *protect* everyone else, believing her death would end the investigation and the maps would never surface. But now the detective's investigation is doing exactly what the murder was meant to prevent: shining a spotlight on this vulnerable community.\n",
      "\n",
      "The second complication: The detective discovers they're not the only one who knows. A surveyor has been hired by the development company and is systematically re-mapping the area. They'll \"discover\" the squatters any day now, and without the cartographer's maps showing 60 years of continuous occupation, the families have no legal standing.\n",
      "\n",
      "**The Choice:** The detective can solve the murder‚Äîwhich means exposing the community and likely forcing eight families from their homes. Or they can bury the case, let a murderer go free, and give these families time to establish legal residency claims. Justice for one person, or protection for dozens.\n",
      "\n",
      "**The Twist:** The cartographer's granddaughter, who hired the detective and has been helping the investigation, already knows about the hidden community‚Äîshe grew up visiting them. Her grandmother was protecting them her whole career. The granddaughter wants the maps found not to solve the murder, but to establish the families' legal claims before the developers strike. She's been manipulating the investigation from the start, using the detective as an unwitting advocate.\n",
      "... (8 more lines)\n",
      "Input=1,344, Prompt cached used= False | Output=764 | Messages=4\n",
      "==============================================\n",
      "Turn 3:\n",
      "\n",
      "\n",
      "User: Ok I like it. Can you help me develop the main character's backstory and motivations?\n",
      "\n",
      "Assistant: \n",
      "# The Detective: Building from the Ground Up\n",
      "\n",
      "Let me develop a character whose personal history makes this particular case land like a gut punch.\n",
      "\n",
      "---\n",
      "\n",
      "## **MAYA KOVAƒå** (27)\n",
      "\n",
      "**The Surface:**\n",
      "Maya is a newly minted detective who specifically requested assignment to a small, rural department. On paper, this looks like someone who couldn't handle the pressure of city police work. She's meticulous to the point of rigidity, keeps people at arm's length, and has a reputation for being \"by the book\" in a way that even her academy instructors found excessive.\n",
      "\n",
      "**The Backstory:**\n",
      "Maya grew up in a multi-generational household in a tight-knit immigrant community‚Äîher grandmother escaped war in the Balkans with false papers, her parents were undocumented for the first decade in America. She was eight years old when ICE raided their neighborhood. Her family wasn't taken (her mother had just gotten her green card), but her best friend's entire family was deported overnight. The girl was at Maya's house for a sleepover. In the morning, she had no home to return to.\n",
      "\n",
      "Maya became *obsessed* with rules after that. If her family followed every rule perfectly, kept every paper in order, never gave anyone a reason to look twice‚Äîthey'd be safe. She color-coded her parents' documents. She learned immigration law as a teenager. She became a cop because she believed that being on the \"inside\" of the system meant understanding how to protect people from it.\n",
      "... (41 more lines)\n",
      "Input=2,131, Prompt cached used= False | Output=1,323 | Messages=6\n",
      "==============================================\n",
      "Turn 4:\n",
      "\n",
      "\n",
      "User: Can you draft a detailed outline for the story, breaking it down into chapters and key events?\n",
      "\n",
      "Assistant: \n",
      "# **THE CARTOGRAPHER'S DAUGHTER**\n",
      "## Detailed Story Outline\n",
      "\n",
      "---\n",
      "\n",
      "## **ACT ONE: THE WRONG KIND OF CRIME**\n",
      "\n",
      "### **Chapter 1: Arrival**\n",
      "Maya drives into Millbrook (pop. 2,847) on a gray October morning. She's been a detective for exactly four months. The police station is attached to the town hall‚Äîthree officers total. She meets CHIEF WARREN HAYES (58), who's retiring in six weeks. He's skeptical of her city credentials and thinks the map thefts are \"kids being weird.\"\n",
      "\n",
      "**Key beats:**\n",
      "- Establish Maya's rigid professionalism‚Äîshe's already read every case file from the past five years\n",
      "- Warren is warm but dismissive: \"You'll find small-town crime is mostly boredom punctuated by stupidity\"\n",
      "- Maya tours the town: tourist-dependent, surrounded by national forest, economically struggling\n",
      "- She notices something: the town feels *smaller* than the maps suggest, like there's negative space at the edges\n",
      "... (250 more lines)\n",
      "Input=3,476, Prompt cached used= False | Output=3,500 | Messages=8\n",
      "==============================================\n",
      "Turn 5:\n",
      "\n",
      "\n",
      "User: Can you draft me a first chapter based on the plot and character ideas we've discussed so far? Make it around 2,000 words.\n",
      "\n",
      "Assistant: \n",
      "# THE CARTOGRAPHER'S DAUGHTER\n",
      "## Chapter One: Arrival\n",
      "\n",
      "The Welcome to Millbrook sign had been hand-painted, Maya noticed. Population 2,847 in careful serif letters, with a little pine tree border that someone had taken time to get right. She'd passed a dozen mountain towns on the drive from Denver, each one bleeding into the next‚Äîsame gas stations, same chain restaurants at the highway exits‚Äîbut Millbrook had stayed small enough, or stubborn enough, to remain itself.\n",
      "\n",
      "Her GPS had lost signal twenty minutes ago. Maya was navigating by the printed directions Chief Hayes had emailed her, which felt oddly appropriate. A return to analog methods for analog policing.\n",
      "\n",
      "She'd been a detective for four months and six days.\n",
      "\n",
      "The town proper appeared around a curve in the two-lane highway: a main street of old brick buildings, half of them with \"For Lease\" signs in dark windows, the other half trying hard with flower boxes and fresh paint. The police station was attached to the town hall, a squat concrete building with an American flag snapping in the October wind. Maya parked her Civic between two pickup trucks that made her sedan look like a child's toy.\n",
      "\n",
      "She checked her reflection in the rearview mirror. Professional. Serious. The blazer she'd ironed this morning was still crisp despite four hours in the car. Her dark hair was pulled into a bun so tight it gave her a headache, but that was intentional‚Äîshe'd learned early that looking severe made people take her more seriously. At twenty-seven, with her small frame and what her academy instructors had diplomatically called \"youthful features,\" she needed every advantage.\n",
      "\n",
      "Maya pulled out her phone and opened the document where she'd compiled her notes on Millbrook: population demographics, crime statistics for the past five years, economic indicators, even a topographical analysis of the surrounding terrain. She'd color-coded everything. Green for low-risk factors, yellow for moderate concern, red for‚Äî\n",
      "\n",
      "... (121 more lines)\n",
      "Input=7,012, Prompt cached used= False | Output=2,778 | Messages=10\n",
      "==============================================\n",
      "Turn 6:\n",
      "\n",
      "\n",
      "User: Can you draft a second chapter that builds on the first one, introducing a new twist in the mystery?\n",
      "\n",
      "Assistant: \n",
      "# THE CARTOGRAPHER'S DAUGHTER\n",
      "## Chapter Two: The Crime Scene\n",
      "\n",
      "Maya woke at 5:47 AM, thirteen minutes before her first alarm. She'd slept in twenty-three minute intervals, jerking awake each time the radiator clanked or a car passed on the street below. Her body still expected city noise‚Äîsirens, garbage trucks, the couple in the apartment next door who fought at 3 AM. This rural silence kept tricking her brain into thinking something was wrong.\n",
      "\n",
      "She was showered, dressed, and reviewing her notes by 6:15. The coffee maker she'd brought from Denver burbled on the counter, filling the small apartment with the smell of something that wasn't burned sludge. Small victories.\n",
      "\n",
      "At 6:45, her phone rang. The screen showed: CHIEF HAYES.\n",
      "\n",
      "\"Kovaƒç,\" she answered, already reaching for her blazer.\n",
      "\n",
      "\"Maya. Sorry to call so early, but we've got a situation. Elena Farkas‚Äîshe's a local woman, lives out on Ridgeline Road‚Äîher neighbor called it in about twenty minutes ago. She's dead, looks like a home invasion.\"\n",
      "\n",
      "Maya's pulse quickened. She pulled her notebook toward her, pen already moving. \"I'm on my way. Have you secured the scene?\"\n",
      "\n",
      "... (171 more lines)\n",
      "Input=9,814, Prompt cached used= True | Output=3,500 | Messages=12\n"
     ]
    }
   ],
   "source": [
    "session = TraditionalCompactingChatSession(system_message=SYSTEM_PROMPT)\n",
    "\n",
    "messages = [\n",
    "    \"I want to create a story about a young detective solving a mysterious case in a small town. Generate 3 well throught out plot ideas for me to consider.\",\n",
    "    \"I don't like those ideas, can you think of one plot something more unique and unexpected?\",\n",
    "    \"Ok I like it. Can you help me develop the main character's backstory and motivations?\",\n",
    "    \"Can you draft a detailed outline for the story, breaking it down into chapters and key events?\",\n",
    "    \"Can you draft me a first chapter based on the plot and character ideas we've discussed so far? Make it around 2,000 words.\", \n",
    "    \"Can you draft a second chapter that builds on the first one, introducing a new twist in the mystery?\"                   \n",
    "]\n",
    "\n",
    "print(\"Starting conversation...\\n\")\n",
    "\n",
    "turn_count = 0\n",
    "\n",
    "for i, message in enumerate(messages, 1):\n",
    "    turn_count += 1\n",
    "    print((\n",
    "        f\"==============================================\\n\"\n",
    "        f\"Turn {turn_count}:\\n\"\n",
    "    ))\n",
    "    response, usage = session.chat(message)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a long conversation with several turns. You'll notice a few things here:\n",
    "\n",
    "Prompt caching: You'll notice here that the input tokens eventually grew to a point where prompt caching was used (turn 6). This helps reduce costs and speed as these conversations grow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the next turn, we are going to hit our 10K context window limit, which triggers compaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßπ Context window at 13314 tokens. Limit exceeded, compacting session memory...\n",
      "Message(id='msg_01CesB6prqUDQyH556365iJw', content=[TextBlock(citations=None, text='<think>\\nLet me analyze this conversation systematically:\\n\\n1. **User\\'s original request**: Generate 3 plot ideas for a story about a young detective solving a mysterious case in a small town.\\n\\n2. **What succeeded**: \\n   - I provided 3 initial plot ideas\\n   - User rejected them as not unique enough\\n   - I created \"The Cartographer\\'s Daughter\" plot, which user liked\\n   - I developed Maya Kovaƒç\\'s character backstory successfully\\n   - I created a detailed chapter-by-chapter outline\\n   - I drafted Chapter 1 (arrival scene)\\n   - I drafted Chapter 2 (crime scene discovery)\\n\\n3. **User corrections/redirections**: \\n   - \"I don\\'t like those ideas, can you think of one plot something more unique and unexpected?\" - User wanted something less conventional\\n   - User accepted the cartographer concept and asked to continue developing it\\n\\n4. **Active work at end**: Just completed drafting Chapter 2, which introduces Elena\\'s murder and the discrepancies in her maps. The chapter ends with Iris Farkas arriving at the station.\\n\\n5. **Incomplete/pending tasks**: None explicitly stated. The user appears satisfied with the progression and hasn\\'t requested additional chapters or edits.\\n\\n6. **Critical details to preserve**:\\n   - Story title: \"The Cartographer\\'s Daughter\"\\n   - Main character: Maya Kovaƒç, 27, new detective, 4 months experience\\n   - Setting: Millbrook (pop. 2,847), mountain town\\n   - Mystery: Maps being stolen, cartographer Elena Farkas murdered, she deliberately falsified maps for 60 years to hide a community of 8 families living on private land\\n   - Key characters: Chief Warren Hayes (retiring), Iris Farkas (granddaughter), Daniel Reeves (surveyor), Scott Chen (ranger)\\n   - Maya\\'s backstory: immigrant family background, childhood friend\\'s family deported, became cop to work within the system\\n   - Central conflict: Maya must choose between solving murder (exposing hidden community) or protecting vulnerable people\\n</think>\\n\\n## User Intent\\nCreate a mystery story about a young detective in a small town. User rejected initial conventional plots as not unique enough, specifically requesting \"something more unique and unexpected.\" Accepted \"The Cartographer\\'s Daughter\" concept and proceeded to develop it fully.\\n\\n## Completed Work\\n**Story concept created**: \"The Cartographer\\'s Daughter\" - A young detective investigates map thefts and a murder in a mountain town, discovering the victim falsified maps for 60 years to hide a community of families living on privately-owned land they believed was public forest.\\n\\n**Main character developed**: Maya Kovaƒç, 27, new detective (4 months experience). Immigrant family background‚Äîgrandmother escaped with false papers, childhood friend\\'s family deported by ICE when Maya was 8. Became obsessive about rules/systems as protection mechanism. Requested small-town assignment seeking \"simpler\" justice, avoiding moral complexity. Character arc: must learn that protecting people sometimes requires working against the system, not within it.\\n\\n**Supporting cast**: Chief Warren Hayes (58, retiring in 6 weeks, will recommend Maya as chief); Iris Farkas (32, art teacher, Elena\\'s granddaughter, secretly manipulating investigation); Elena Farkas (89, murdered cartographer); Thomas Wade (52, hidden community leader, will be revealed as killer); Daniel Reeves (41, corporate surveyor); Scott Chen (35, ranger); 8 families in hidden settlement \"Millbrook Heights.\"\\n\\n**Detailed outline created**: 13-chapter structure spanning 3 acts:\\n- Act 1: Map thefts lead to Elena\\'s murder, Maya discovers discrepancies in maps\\n- Act 2: Maya finds hidden community, learns Iris has been manipulating her, discovers Elena\\'s accurate maps in storage unit\\n- Act 3: Thomas confesses to accidental killing, Maya must choose between justice and protection\\n\\n**Two complete chapters drafted**:\\n- Chapter 1 (~2,000 words): Maya arrives in Millbrook, meets Hayes, learns about map thefts, given case as \"orientation,\" promoted to future chief (unexpected), notices town feels \"smaller than maps suggest\"\\n- Chapter 2 (~2,500 words): Elena\\'s murder discovered, crime scene investigation, learns surveyor Reeves found GPS discrepancies with Elena\\'s maps, Margaret Yates (neighbor) mentions development company visit, chapter ends with Iris arriving at station\\n\\n## Errors & Corrections\\nUser\\'s only correction: rejected first three plot ideas as not unique enough. Required more unexpected/unconventional mystery concept.\\n\\n## Active Work\\nJust completed Chapter 2. The chapter ends mid-scene with Iris Farkas entering the police station to meet Maya for the first time. This is the moment where Maya begins interacting with the character who will manipulate the entire investigation. The scene was left at: \"Detective Kovaƒç? I\\'m Iris Farkas. Please, sit down.\"\\n\\n## Pending Tasks\\nNone explicitly requested. User has not asked for Chapter 3 or further development, but the natural continuation would be drafting subsequent chapters.\\n\\n## Key References\\n\\n**Story title**: \"The Cartographer\\'s Daughter\"\\n\\n**Setting**: Millbrook, population 2,847, mountain town in Colorado (near Denver, ~4 hour drive), surrounded by national forest, economically struggling, tourist-dependent\\n\\n**Central mystery elements**:\\n- 6 weeks of map thefts before murder (library, historical society, ranger station, 4 GPS units from cars)\\n- Elena Farkas: 89-year-old cartographer, murdered in her home studio, all maps stolen, she was burning maps before death\\n- Hidden community: 8 families, settlement called \"Millbrook Heights\" (ironic name), living 3 miles into forest on unmapped trails, on land owned by Consolidated Mountain Properties\\n- Elena falsified maps for 60 years, showing empty forest where community exists\\n- Thomas Wade will be revealed as accidental killer (pushed Elena during argument, she hit drafting table)\\n- Consolidated Mountain Properties: bought 400 acres, hired Daniel Reeves to survey for resort development\\n- Elena\\'s accurate maps hidden in storage unit by Iris, showing continuous occupation since 1962\\n\\n**Character details**:\\n- Maya Kovaƒç: 27, detective for \"four months and six days\" at start, from Denver PD, requested Millbrook assignment, drives Honda Civic, color-codes everything, obsessively organized, immigrant parents (Balkan grandmother with false papers), childhood friend deported at age 8\\n- Warren Hayes: retiring in 6 weeks to Scottsdale, will recommend Maya as chief\\n- Iris Farkas: 32, high school art teacher, paint-stained flannel shirt, has been manipulating investigation to get maps into evidence\\n- Elena Farkas: lived on Ridgeline Road, had studio attached to house, made all official maps for region for 60 years, killed ~10 PM with head trauma (drafting table corner)\\n- Margaret Yates: 70s, neighbor who discovered body during morning dog walk\\n- Scott Chen: 35, forest ranger, father Leon Chen (67) lives in hidden community (estranged)\\n- Daniel Reeves: 41, corporate surveyor for Consolidated Mountain Properties\\n- Thomas Wade: 52, hidden community leader, meeting scheduled with Elena \"T.W. - 9 PM\" night of murder\\n- Grace Wade: 17, Thomas\\'s daughter\\n- Sofia Hernandez: 28, community member with children\\n\\n**Timeline established in chapters**:\\n- Maya arrives October (gray morning)\\n- First alarm set for 6:00 AM Day 2\\n- Hayes calls 6:45 AM: Elena\\'s body discovered ~6:25 AM by Margaret Yates\\n- Death occurred previous evening ~10 PM (10-14 hours before discovery)\\n- Map thefts began 6 weeks prior\\n- Consolidated purchased land \"last month\" (one month before current timeline)\\n\\n**Physical descriptions from chapters**:\\n- Police station: attached to town hall, concrete building, smells like \"burned coffee and printer toner\"\\n- Maya\\'s apartment: above hardware store on Main Street, one bedroom, shower from \"Reagan administration,\" two-burner stove\\n- Elena\\'s house: ranch-style, weathered cedar siding, workshop attached, Ridgeline Road (10 minutes from town)\\n- Elena\\'s studio: north-facing windows, flat files, drafting table, fireplace in corner with metal waste bin\\n\\n**Key quotes to preserve tone**:\\n- Maya on rules: \"safety came from being beyond reproach, from giving the system no reason to notice you\"\\n- Hayes on Elena: \"If Elena said something existed, it existed. And if her maps said something wasn\\'t there...\"\\n- Scott on Elena\\'s accuracy: \"Elena\\'s maps are gospel\"\\n- Hayes cryptically: \"sometimes the maps we draw aren\\'t just about geography. They\\'re about protecting what we love\"', type='text')], model='claude-sonnet-4-5-20250929', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation=CacheCreation(ephemeral_1h_input_tokens=0, ephemeral_5m_input_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=13723, output_tokens=2021, server_tool_use=None, service_tier='standard'))\n",
      "\n",
      "------------------------------------------------------------\n",
      "üìù New session memory created.\n",
      "‚úÖ Tokens reduced: 13,314 ‚Üí 1536 (11,778 tokens saved, 88% reduction)\n",
      "‚è±Ô∏è Compaction time: 44.49s (user waiting...)\n",
      "------------------------------------------------------------\n",
      "\n",
      "User: Propose a title for the book\n",
      "\n",
      "Assistant: \n",
      "Looking at the story's core themes and the path Maya will travel, I'd propose:\n",
      "\n",
      "**\"The Unmapped\"**\n",
      "\n",
      "It works on multiple levels:\n",
      "- The literal hidden community that doesn't appear on any map\n",
      "- The moral territory Maya must navigate that her rule-based worldview never prepared her for\n",
      "- Elena's secret‚Äîthe truth she kept unmapped for 60 years\n",
      "- Maya herself, who arrives thinking she understands justice but discovers she's been operating with an incomplete map of what protection really means\n",
      "\n",
      "The single-word title has that literary mystery feel (think *Gone Girl*, *The Searcher*) while being evocative rather than explanatory.\n",
      "\n",
      "Alternative if you want something that hints more at the cartography angle: **\"False Survey\"** or **\"The Cartographer's Secret\"**\n",
      "\n",
      "But I think **\"The Unmapped\"** captures both the mystery and Maya's internal journey most effectively.\n",
      "... (6 more lines)\n",
      "Input=1,864, Prompt cached used= False | Output=235 | Messages=3\n"
     ]
    }
   ],
   "source": [
    "response, usage = session.chat(\"Propose a title for the book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You'll notice here that it took xx seconds for the agent to compact the conversation. Because we used traditional compaction, the user would be waiting on Claude to compact the conversation, which is not an ideal user experience.\n",
    "\n",
    "Below you can see the result of the compaction. It captures the key elements of conversation in less than 2K tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## User Intent\n",
      "Create a mystery story about a young detective in a small town. User rejected initial conventional plots as not unique enough, specifically requesting \"something more unique and unexpected.\" Accepted \"The Cartographer's Daughter\" concept and proceeded to develop it fully.\n",
      "\n",
      "## Completed Work\n",
      "**Story concept created**: \"The Cartographer's Daughter\" - A young detective investigates map thefts and a murder in a mountain town, discovering the victim falsified maps for 60 years to hide a community of families living on privately-owned land they believed was public forest.\n",
      "\n",
      "**Main character developed**: Maya Kovaƒç, 27, new detective (4 months experience). Immigrant family background‚Äîgrandmother escaped with false papers, childhood friend's family deported by ICE when Maya was 8. Became obsessive about rules/systems as protection mechanism. Requested small-town assignment seeking \"simpler\" justice, avoiding moral complexity. Character arc: must learn that protecting people sometimes requires working against the system, not within it.\n",
      "\n",
      "**Supporting cast**: Chief Warren Hayes (58, retiring in 6 weeks, will recommend Maya as chief); Iris Farkas (32, art teacher, Elena's granddaughter, secretly manipulating investigation); Elena Farkas (89, murdered cartographer); Thomas Wade (52, hidden community leader, will be revealed as killer); Daniel Reeves (41, corporate surveyor); Scott Chen (35, ranger); 8 families in hidden settlement \"Millbrook Heights.\"\n",
      "\n",
      "**Detailed outline created**: 13-chapter structure spanning 3 acts:\n",
      "- Act 1: Map thefts lead to Elena's murder, Maya discovers discrepancies in maps\n",
      "- Act 2: Maya finds hidden community, learns Iris has been manipulating her, discovers Elena's accurate maps in storage unit\n",
      "- Act 3: Thomas confesses to accidental killing, Maya must choose between justice and protection\n",
      "\n",
      "**Two complete chapters drafted**:\n",
      "- Chapter 1 (~2,000 words): Maya arrives in Millbrook, meets Hayes, learns about map thefts, given case as \"orientation,\" promoted to future chief (unexpected), notices town feels \"smaller than maps suggest\"\n",
      "- Chapter 2 (~2,500 words): Elena's murder discovered, crime scene investigation, learns surveyor Reeves found GPS discrepancies with Elena's maps, Margaret Yates (neighbor) mentions development company visit, chapter ends with Iris arriving at station\n",
      "\n",
      "## Errors & Corrections\n",
      "User's only correction: rejected first three plot ideas as not unique enough. Required more unexpected/unconventional mystery concept.\n",
      "\n",
      "## Active Work\n",
      "Just completed Chapter 2. The chapter ends mid-scene with Iris Farkas entering the police station to meet Maya for the first time. This is the moment where Maya begins interacting with the character who will manipulate the entire investigation. The scene was left at: \"Detective Kovaƒç? I'm Iris Farkas. Please, sit down.\"\n",
      "\n",
      "## Pending Tasks\n",
      "None explicitly requested. User has not asked for Chapter 3 or further development, but the natural continuation would be drafting subsequent chapters.\n",
      "\n",
      "## Key References\n",
      "\n",
      "**Story title**: \"The Cartographer's Daughter\"\n",
      "\n",
      "**Setting**: Millbrook, population 2,847, mountain town in Colorado (near Denver, ~4 hour drive), surrounded by national forest, economically struggling, tourist-dependent\n",
      "\n",
      "**Central mystery elements**:\n",
      "- 6 weeks of map thefts before murder (library, historical society, ranger station, 4 GPS units from cars)\n",
      "- Elena Farkas: 89-year-old cartographer, murdered in her home studio, all maps stolen, she was burning maps before death\n",
      "- Hidden community: 8 families, settlement called \"Millbrook Heights\" (ironic name), living 3 miles into forest on unmapped trails, on land owned by Consolidated Mountain Properties\n",
      "- Elena falsified maps for 60 years, showing empty forest where community exists\n",
      "- Thomas Wade will be revealed as accidental killer (pushed Elena during argument, she hit drafting table)\n",
      "- Consolidated Mountain Properties: bought 400 acres, hired Daniel Reeves to survey for resort development\n",
      "- Elena's accurate maps hidden in storage unit by Iris, showing continuous occupation since 1962\n",
      "\n",
      "**Character details**:\n",
      "- Maya Kovaƒç: 27, detective for \"four months and six days\" at start, from Denver PD, requested Millbrook assignment, drives Honda Civic, color-codes everything, obsessively organized, immigrant parents (Balkan grandmother with false papers), childhood friend deported at age 8\n",
      "- Warren Hayes: retiring in 6 weeks to Scottsdale, will recommend Maya as chief\n",
      "- Iris Farkas: 32, high school art teacher, paint-stained flannel shirt, has been manipulating investigation to get maps into evidence\n",
      "- Elena Farkas: lived on Ridgeline Road, had studio attached to house, made all official maps for region for 60 years, killed ~10 PM with head trauma (drafting table corner)\n",
      "- Margaret Yates: 70s, neighbor who discovered body during morning dog walk\n",
      "- Scott Chen: 35, forest ranger, father Leon Chen (67) lives in hidden community (estranged)\n",
      "- Daniel Reeves: 41, corporate surveyor for Consolidated Mountain Properties\n",
      "- Thomas Wade: 52, hidden community leader, meeting scheduled with Elena \"T.W. - 9 PM\" night of murder\n",
      "- Grace Wade: 17, Thomas's daughter\n",
      "- Sofia Hernandez: 28, community member with children\n",
      "\n",
      "**Timeline established in chapters**:\n",
      "- Maya arrives October (gray morning)\n",
      "- First alarm set for 6:00 AM Day 2\n",
      "- Hayes calls 6:45 AM: Elena's body discovered ~6:25 AM by Margaret Yates\n",
      "- Death occurred previous evening ~10 PM (10-14 hours before discovery)\n",
      "- Map thefts began 6 weeks prior\n",
      "- Consolidated purchased land \"last month\" (one month before current timeline)\n",
      "\n",
      "**Physical descriptions from chapters**:\n",
      "- Police station: attached to town hall, concrete building, smells like \"burned coffee and printer toner\"\n",
      "- Maya's apartment: above hardware store on Main Street, one bedroom, shower from \"Reagan administration,\" two-burner stove\n",
      "- Elena's house: ranch-style, weathered cedar siding, workshop attached, Ridgeline Road (10 minutes from town)\n",
      "- Elena's studio: north-facing windows, flat files, drafting table, fireplace in corner with metal waste bin\n",
      "\n",
      "**Key quotes to preserve tone**:\n",
      "- Maya on rules: \"safety came from being beyond reproach, from giving the system no reason to notice you\"\n",
      "- Hayes on Elena: \"If Elena said something existed, it existed. And if her maps said something wasn't there...\"\n",
      "- Scott on Elena's accuracy: \"Elena's maps are gospel\"\n",
      "- Hayes cryptically: \"sometimes the maps we draw aren't just about geography. They're about protecting what we love\"\n"
     ]
    }
   ],
   "source": [
    "print(session.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instant Compaction\n",
    "\n",
    "With **Instant compaction** the session memory is PROACTIVELY generated once a soft token threshold is reached. \n",
    "\n",
    "Once the user triggers a compaction or a hard limit is reached, the summary is already available, so the user doesn't need to wait.\n",
    "\n",
    "Result: Instant compaction, no waiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SESSION MEMORY COMPACTION (instant)\n",
    "```\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Turn 1 ‚Üí Turn 2 ‚Üí ... ‚Üí Turn K ‚Üí Turn K+1 ‚Üí ... ‚Üí Turn N ‚Üí ..  ‚Üí CONTEXT FULL!\n",
    "                            ‚îÇ                         ‚îÇ            ‚îÇ\n",
    "                (soft threshold met:              (update          ‚îÇ\n",
    "                   5k tokens init)                trigger)        ‚îÇ\n",
    "                            ‚îÇ                                      ‚îÇ\n",
    "                            ‚îÇ                         ‚îÇ            ‚îÇ\n",
    "                            ‚ñº                         ‚ñº            ‚îÇ\n",
    "                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ\n",
    "                       ‚îÇ Update ‚îÇ                ‚îÇ Update ‚îÇ        ‚îÇ\n",
    "                       ‚îÇ memory ‚îÇ (background)   ‚îÇ memory ‚îÇ        ‚îÇ\n",
    "                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ\n",
    "                            ‚îÇ                         ‚îÇ            ‚îÇ\n",
    "                            ‚ñº                         ‚ñº            ‚ñº\n",
    "                     üìù session-memory.md ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ INSTANT SWAP!\n",
    "                       (continuously updated)\n",
    "```\n",
    "\n",
    "**Update triggers:** The first summary is generated after the initial 5k tokens. Updates can be triggered after every subsequent turn, or at periodically at natural breakpoints intervals (e.g. every ~5k tokens or 3+ tool calls)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `InstantCompactingChatSession` class uses **threading** for background execution:\n",
    "1. **`threading.Thread`** - runs memory updates in background without blocking\n",
    "2. **Thread-safe state** - uses `threading.Lock` to safely update shared memory\n",
    "3. **Daemon threads** - background work doesn't prevent program exit\n",
    "4. **Instant compaction** - when context is full, just swap in the pre-built memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": "import threading\nimport time\n\n\nclass InstantCompactingChatSession:\n    \"\"\"\n    Maintains session memory via incremental background updates.\n    \n    Key insight: By updating memory in the background after each turn,\n    the summary is already ready when compaction is needed - instant swap!\n    \"\"\"\n\n    def __init__(\n        self,\n        system_message=\"You are a helpful assistant\",\n        context_limit: int = 10000,\n        min_tokens_to_init: int = 5000,\n        min_tokens_between_updates: int = 2000,\n    ):\n        # Thresholds\n        self.context_limit = context_limit # the point at which the conversation is compacted so it does not exceed model limits\n        self.min_tokens_to_init = min_tokens_to_init # tokens needed to trigger initial memory creation; note this happens PROACTIVELY in background unlike traditional compaction\n        self.min_tokens_between_updates = min_tokens_between_updates # tokens needed to trigger memory update. only comes into play after initial memory is created and additional compaction (memory update) is needed after that\n\n        # Conversation state\n        self.system_message = system_message\n        self.messages = []\n        self.current_context_window_tokens = 0\n\n        # Session memory state\n        self.session_memory = None # this is the compacted conversation in session memory; for the demo we are storing this in memory, but in production you would write to session_memory.md file\n        self.last_summarized_index = 0 # The index of the last message included in the session memory\n        self.tokens_at_last_update = 0 # TBD if I need this\n\n        # Background update tracking\n        self._update_thread: threading.Thread | None = None\n        self.last_update_time = None\n        self._lock = threading.Lock()\n\n    def chat(self, user_message: str):\n        \"\"\"Process a chat turn with background session memory updates.\"\"\"\n        if self.current_context_window_tokens >= self.context_limit:\n            self.compact() # note that when this is triggered, the compaction has already been created and is just swapped in instantly\n\n        self.messages.append({\"role\": \"user\", \"content\": user_message})\n\n        response = client.messages.create(\n            model=MODEL,\n            max_tokens=3500,\n            system=self.system_message,\n            messages=add_cache_control(self.messages),\n        )\n\n        assistant_message = response.content[0].text\n        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n\n        # Calculate token usage including cache\n        cache_read = getattr(response.usage, \"cache_read_input_tokens\", 0) or 0\n        total_input = response.usage.input_tokens + cache_read\n        \n        # Update context window tokens (includes cached tokens since they still count toward context)\n        self.current_context_window_tokens = total_input + response.usage.output_tokens\n\n        # KEY DIFFERENCE: Trigger background memory update if needed proactively, before compaction is needed\n        background_status = None\n        if self._should_init_memory() or self._should_update_memory():\n            self._trigger_background_update()\n            background_status = \"initializing\" if self.session_memory is None else \"updating\"\n\n        # Return usage info with cache stats\n        return assistant_message, response.usage, background_status\n    \n    # Helper methods to determine when to init/update/compact\n    def _should_init_memory(self) -> bool:\n        return (\n            self.session_memory is None\n            and self.current_context_window_tokens >= self.min_tokens_to_init\n        )\n\n    # Helper method to determine if memory should be updated\n    def _should_update_memory(self) -> bool:\n        if self.session_memory is None:\n            return False\n        tokens_since = self.current_context_window_tokens - self.tokens_at_last_update\n        return tokens_since >= self.min_tokens_between_updates\n\n    # Methods to create initial session memory\n    def _create_session_memory(self, messages: list[dict]) -> str:\n        \"\"\"Generate initial session memory from messages.\"\"\"\n        # Put compaction instructions in user message to share cache with main chat\n        compaction_instruction = f\"\"\"\nFor this task, act as a session memory agent. Your previous role is paused.\n\n{SESSION_MEMORY_PROMPT}\n\nThe full transcript of our conversation so far is included above. Compress it now.\n\"\"\"\n        messages = messages + [{\"role\": \"user\", \"content\": compaction_instruction}]\n        response = client.messages.create(\n            model=MODEL,\n            max_tokens=5000,\n            system=self.system_message,  # Same as main chat for cache sharing\n            messages=add_cache_control(messages)\n        )\n        summary, _ = remove_thinking_blocks(response.content[0].text)  # clean up any <think> blocks because they are not needed in the session memory\n        return summary\n\n    def _update_session_memory(self, new_messages: list[dict]) -> str:\n        \"\"\"Update existing session memory with new messages. In practice, you may want to do this via file edit rather than full re-generation. But for demo purposes we do full regeneration here.\"\"\"\n        # Put compaction instructions in user message to share cache with main chat\n        compaction_instruction = f\"\"\"\nFor this task, act as a session memory agent. Your previous role is paused.\n\n{SESSION_MEMORY_PROMPT}\n\nCurrent session memory:\n{self.session_memory}\n\nNew messages to integrate into the session memory are included above. Update the session memory now.\n\"\"\"\n        new_messages = new_messages + [{\"role\": \"user\", \"content\": compaction_instruction}]\n        response = client.messages.create(\n            model=MODEL,\n            max_tokens=5000,\n            system=self.system_message,  # Same as main chat for cache sharing\n            messages=add_cache_control(new_messages)\n        )\n        updated_summary, _ = remove_thinking_blocks(response.content[0].text)  # clean up any <think> blocks because they are not needed in the session memory\n        return updated_summary\n\n    # Background memory update methods\n    def _background_memory_update(\n        self, messages_snapshot: list[dict], snapshot_index: int, current_tokens: int\n    ):\n        \"\"\"Run session memory update in a background thread.\"\"\"\n        try:\n            with self._lock:\n                current_session_memory = self.session_memory\n                last_index = self.last_summarized_index\n\n            if current_session_memory is None:\n                new_memory = self._create_session_memory(messages_snapshot)\n            else:\n                # Get new messages since last summary\n                new_messages = messages_snapshot[last_index :]\n                if not new_messages:\n                    return\n                new_memory = self._update_session_memory(new_messages)\n\n            # Update state (thread-safe)\n            with self._lock:\n                self.session_memory = new_memory\n                self.last_summarized_index = snapshot_index\n                self.tokens_at_last_update = current_tokens\n                self.last_update_time = time.time()\n\n        except Exception as e:\n            print(f\"   [Background] Error updating memory: {e}\")\n\n    # This makes sure only one background update runs at a time. If one is already running, we skip starting another. If not, we start a new thread to do the update.\n    def _trigger_background_update(self):\n        \"\"\"Trigger a background session memory update.\"\"\"\n        if self._update_thread is not None and self._update_thread.is_alive():\n            return\n\n        messages_snapshot = self.messages.copy()\n        snapshot_index = len(messages_snapshot)\n        current_tokens = self.current_context_window_tokens\n\n        self._update_thread = threading.Thread(\n            target=self._background_memory_update,\n            args=(messages_snapshot, snapshot_index, current_tokens),\n            daemon=True,\n        )\n        self._update_thread.start()\n\n    # Function to compact\n    def compact(self):\n        \"\"\"INSTANT compaction using pre-built session memory.\"\"\"\n        prev_msg_count = len(self.messages)\n\n        # Ensure session memory is ready. Shouldn't be an issue normally, but here for safety.\n        if self.session_memory is None:\n            if self._update_thread is not None and self._update_thread.is_alive():\n                print(\"   ‚è≥ Waiting for background memory update...\")\n                self._update_thread.join(timeout=30.0)\n\n            if self.session_memory is None:\n                print(\"   ‚ö†Ô∏è  No pre-built memory, creating synchronously...\")\n                start = time.perf_counter()\n                self.session_memory = self._create_session_memory(self.messages)\n                elapsed = time.perf_counter() - start\n                print(f\"   ‚è±Ô∏è  Took {elapsed:.2f}s (but should be instant normally!)\")\n                self.last_summarized_index = len(self.messages)\n\n        with self._lock:\n            unsummarized = self.messages[self.last_summarized_index :]\n            summary_message = [{\n                \"role\": \"user\",\n                \"content\": f\"\"\"This session is being continued from a previous conversation. Here is the session memory: {self.session_memory}.Continue from where we left off.\"\"\"\n            }]\n            self.messages = summary_message + unsummarized\n            self.last_summarized_index = 1\n\n            print(f\"\\n{'=' * 60}\")\n            print(f\"‚ö° INSTANT COMPACTION! Messages: {prev_msg_count} ‚Üí {len(self.messages)}\")\n            print(f\"   Session memory was pre-built (no wait time!)\")\n            print(f\"{'=' * 60}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use of Instant Compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with instant compacting chat session...\n",
      "\n",
      "============================================================\n",
      "Turn 1:\n",
      "\n",
      "User: I want to create a story about a young detective solving a mysterious case in a small town. Generate 3 well throught out plot ideas for me to consider.\n",
      "\n",
      "Assistant: \n",
      "# Three Mystery Plot Ideas\n",
      "\n",
      "## 1. **The Drowning Season**\n",
      "... (24 more lines)\n",
      "\n",
      "Token Usage:\n",
      "  Input: 318 (new: 318, cached: 0)\n",
      "  Output: 659\n",
      "  Messages: 2 | Memory: not yet\n",
      "\n",
      "============================================================\n",
      "Turn 2:\n",
      "\n",
      "User: I don't like those ideas, can you think of one plot something more unique and unexpected?\n",
      "\n",
      "Assistant: \n",
      "# **The Cartographer's Error**\n",
      "\n",
      "A detective takes a job in a declining mining town where she's assigned what seems like busywork: investigating why survey maps keep disappearing from the county records office. It's treated as petty theft, probably teenagers.\n",
      "... (10 more lines)\n",
      "\n",
      "Token Usage:\n",
      "  Input: 999 (new: 999, cached: 0)\n",
      "  Output: 411\n",
      "  Messages: 4 | Memory: not yet\n",
      "\n",
      "============================================================\n",
      "Turn 3:\n",
      "\n",
      "User: Ok I like it. Can you help me develop the main character's backstory and motivations?\n",
      "\n",
      "Assistant: \n",
      "# The Detective: **Nora Hayward**\n",
      "\n",
      "## Background\n",
      "... (40 more lines)\n",
      "\n",
      "Token Usage:\n",
      "  Input: 1,433 (new: 1,433, cached: 0)\n",
      "  Output: 833\n",
      "  Messages: 6 | Memory: not yet\n",
      "\n",
      "============================================================\n",
      "Turn 4:\n",
      "\n",
      "User: Can you draft a detailed outline for the story, breaking it down into chapters and key events?\n",
      "\n",
      "Assistant: \n",
      "# **The Cartographer's Error** - Story Outline\n",
      "\n",
      "## ACT ONE: The Impossible Geography\n",
      "... (136 more lines)\n",
      "\n",
      "Token Usage:\n",
      "  Input: 2,288 (new: 2,288, cached: 0)\n",
      "  Output: 3,035\n",
      "  Messages: 8 | Memory: not yet\n",
      "\n",
      "  [Background] Proactively initializing session memory...\n",
      "  Context window: 5,323 tokens\n",
      "\n",
      "============================================================\n",
      "Turn 5:\n",
      "\n",
      "User: Can you draft me a first chapter based on the plot and character ideas we've discussed so far? Make it around 2,000 words.\n",
      "\n",
      "Assistant: \n",
      "# Chapter One: Arrival\n",
      "\n",
      "The town appeared through the rain like a photograph developing in reverse‚Äîcolor leaching into gray, details softening into suggestion. Nora Hayward's wipers beat a rhythm against the October drizzle as she followed Route 34 down the mountain pass, watching Millstone materialize in the valley below.\n",
      "... (136 more lines)\n",
      "\n",
      "Token Usage:\n",
      "  Input: 5,356 (new: 5,356, cached: 0)\n",
      "  Output: 2,778\n",
      "  Messages: 10 | Memory: ready\n",
      "\n",
      "  [Background] Proactively updating session memory...\n",
      "  Context window: 8,134 tokens\n",
      "\n",
      "============================================================\n",
      "Turn 6:\n",
      "\n",
      "User: Can you draft a second chapter that builds on the first one, introducing a new twist in the mystery?\n",
      "\n",
      "Assistant: \n",
      "# Chapter Two: The Missing Maps\n",
      "\n",
      "Nora spent the weekend in her new apartment‚Äîa one-bedroom above a closed insurance office on what the lease said was Pine Street but the street sign claimed was Birch. She told herself the discrepancy was charming, in a declining-town sort of way. Small places were idiosyncratic. That was part of their appeal.\n",
      "... (210 more lines)\n",
      "\n",
      "Token Usage:\n",
      "  Input: 8,158 (new: 4,062, cached: 4,096)\n",
      "  Output: 3,500\n",
      "  Messages: 12 | Memory: ready\n",
      "  ‚úì Cache hit! 50% of input from cache\n",
      "\n",
      "  [Background] Proactively updating session memory...\n",
      "  Context window: 11,658 tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Low thresholds for demo - in production you'd use higher values\n",
    "session = InstantCompactingChatSession(\n",
    "    system_message=SYSTEM_PROMPT,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    \"I want to create a story about a young detective solving a mysterious case in a small town. Generate 3 well throught out plot ideas for me to consider.\",\n",
    "    \"I don't like those ideas, can you think of one plot something more unique and unexpected?\",\n",
    "    \"Ok I like it. Can you help me develop the main character's backstory and motivations?\",\n",
    "    \"Can you draft a detailed outline for the story, breaking it down into chapters and key events?\",\n",
    "    \"Can you draft me a first chapter based on the plot and character ideas we've discussed so far? Make it around 2,000 words.\", \n",
    "    \"Can you draft a second chapter that builds on the first one, introducing a new twist in the mystery?\"                   \n",
    "]\n",
    "print(\"Starting conversation with instant compacting chat session...\\n\")\n",
    "\n",
    "turn_count = 0\n",
    "for i, message in enumerate(messages, 1):\n",
    "    response, usage, background_status = session.chat(message)\n",
    "    turn_count += 1\n",
    "    \n",
    "    # Calculate cache stats\n",
    "    cache_read = getattr(usage, \"cache_read_input_tokens\", 0) or 0\n",
    "    cache_created = getattr(usage, \"cache_creation_input_tokens\", 0) or 0\n",
    "    total_input = usage.input_tokens + cache_read\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Turn {turn_count}:\")\n",
    "    print(f\"\\nUser: {message}\")\n",
    "    print(f\"\\nAssistant: \\n{truncate_response(response, max_lines=3)}\")\n",
    "    print(f\"\\nToken Usage:\")\n",
    "    print(f\"  Input: {total_input:,} (new: {usage.input_tokens:,}, cached: {cache_read:,})\")\n",
    "    print(f\"  Output: {usage.output_tokens:,}\")\n",
    "    print(f\"  Messages: {len(session.messages)} | Memory: {'ready' if session.session_memory else 'not yet'}\")\n",
    "    \n",
    "    if cache_read > 0:\n",
    "        cache_pct = (cache_read / total_input) * 100\n",
    "        print(f\"  ‚úì Cache hit! {cache_pct:.0f}% of input from cache\")\n",
    "    \n",
    "    if background_status:\n",
    "        print(f\"\\n  [Background] Proactively {background_status} session memory...\")\n",
    "        print(f\"  Context window: {session.current_context_window_tokens:,} tokens\")\n",
    "    \n",
    "    print()\n",
    "    # Sleep to allow background updates to complete for demo purposes\n",
    "    if i < len(messages):\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚ö° INSTANT COMPACTION! Messages: 12 ‚Üí 3\n",
      "   Session memory was pre-built (no wait time!)\n",
      "============================================================\n",
      "\n",
      "User: What did we just talk about? Give me one sentence\n",
      "\n",
      "Assistant: \n",
      "I just drafted Chapter Two where Nora investigates stolen survey maps from the 1950s and discovers that GPS can't reliably locate buildings and street intersections don't match between historical maps and current reality, confirming something is physically wrong with Millstone's geography.\n",
      "\n",
      "Token Usage:\n",
      "  Input: 5,286 (new: 5,286, cached: 0)\n",
      "  Output: 60\n",
      "  Messages: 5 | Memory: ready\n"
     ]
    }
   ],
   "source": [
    "message = \"What did we just talk about? Give me one sentence\"\n",
    "response, usage, background_status = session.chat(message)\n",
    "\n",
    "# Calculate cache stats\n",
    "cache_read = getattr(usage, \"cache_read_input_tokens\", 0) or 0\n",
    "total_input = usage.input_tokens + cache_read\n",
    "\n",
    "print(f\"\\nUser: {message}\")\n",
    "print(f\"\\nAssistant: \\n{truncate_response(response, max_lines=3)}\")\n",
    "print(f\"\\nToken Usage:\")\n",
    "print(f\"  Input: {total_input:,} (new: {usage.input_tokens:,}, cached: {cache_read:,})\")\n",
    "print(f\"  Output: {usage.output_tokens:,}\")\n",
    "print(f\"  Messages: {len(session.messages)} | Memory: {'ready' if session.session_memory else 'not yet'}\")\n",
    "\n",
    "if cache_read > 0:\n",
    "    cache_pct = (cache_read / total_input) * 100\n",
    "    print(f\"  ‚úì Cache hit! {cache_pct:.0f}% of input from cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "message = \"What about a sequel?\"\n",
    "response, usage, background_status = session.chat(message)\n",
    "\n",
    "# Calculate cache stats\n",
    "cache_read = getattr(usage, \"cache_read_input_tokens\", 0) or 0\n",
    "total_input = usage.input_tokens + cache_read\n",
    "\n",
    "print(f\"\\nUser: {message}\")\n",
    "print(f\"\\nAssistant: \\n{truncate_response(response, max_lines=3)}\")\n",
    "print(f\"\\nToken Usage:\")\n",
    "print(f\"  Input: {total_input:,} (new: {usage.input_tokens:,}, cached: {cache_read:,})\")\n",
    "print(f\"  Output: {usage.output_tokens:,}\")\n",
    "print(f\"  Messages: {len(session.messages)} | Memory: {'ready' if session.session_memory else 'not yet'}\")\n",
    "\n",
    "if cache_read > 0:\n",
    "    cache_pct = (cache_read / total_input) * 100\n",
    "    print(f\"  ‚úì Cache hit! {cache_pct:.0f}% of input from cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Understanding Prompt Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The background updates can be made **~10x cheaper** by using prompt caching. The trick:\n",
    "1. Pass the **full conversation** to the background summarizer\n",
    "2. Add `cache_control` markers so subsequent requests hit the cache\n",
    "3. Only the new \"summarize this\" instruction is billed at full price\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    PROMPT CACHING FOR LONG CONVERSATIONS                        ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                                 ‚îÇ\n",
    "‚îÇ  WITHOUT CACHING: Pay full price for entire context every turn                 ‚îÇ\n",
    "‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                   ‚îÇ\n",
    "‚îÇ                                                                                 ‚îÇ\n",
    "‚îÇ  Turn 1:  [System][User1][Asst1]                         ‚Üí  500 tokens  @ $3/M ‚îÇ\n",
    "‚îÇ  Turn 2:  [System][User1][Asst1][User2][Asst2]           ‚Üí 1500 tokens  @ $3/M ‚îÇ\n",
    "‚îÇ  Turn 3:  [System][User1][Asst1][User2][Asst2][User3]... ‚Üí 3000 tokens  @ $3/M ‚îÇ\n",
    "‚îÇ  Turn 4:  [System][User1][Asst1][User2][Asst2][User3]... ‚Üí 5000 tokens  @ $3/M ‚îÇ\n",
    "‚îÇ           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                         ‚îÇ\n",
    "‚îÇ                                              Total: 10,000 tokens = $0.030      ‚îÇ\n",
    "‚îÇ                                                                                 ‚îÇ\n",
    "‚îÇ                                                                                 ‚îÇ\n",
    "‚îÇ  WITH CACHING: Pay full price once, then 90% discount on prefix                ‚îÇ\n",
    "‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                ‚îÇ\n",
    "‚îÇ                                                                                 ‚îÇ\n",
    "‚îÇ  Turn 1:  [System][User1][Asst1]‚óÜ                        ‚Üí  500 tokens  @ $3/M ‚îÇ\n",
    "‚îÇ                                ‚ñ≤                            (cache created)    ‚îÇ\n",
    "‚îÇ                          cache breakpoint                                       ‚îÇ\n",
    "‚îÇ                                                                                 ‚îÇ\n",
    "‚îÇ  Turn 2:  [System][User1][Asst1][User2][Asst2]‚óÜ                                ‚îÇ\n",
    "‚îÇ           ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ cached ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ                                              ‚îÇ\n",
    "‚îÇ                500 @ $0.30/M + 1000 new @ $3/M  =  $0.0032                     ‚îÇ\n",
    "‚îÇ                                                                                 ‚îÇ\n",
    "‚îÇ  Turn 3:  [System][User1][Asst1][User2][Asst2][User3][Asst3]‚óÜ                  ‚îÇ\n",
    "‚îÇ           ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ cached ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ                                  ‚îÇ\n",
    "‚îÇ               1500 @ $0.30/M + 1500 new @ $3/M  =  $0.0050                     ‚îÇ\n",
    "‚îÇ                                                                                 ‚îÇ\n",
    "‚îÇ  Turn 4:  [System][User1][Asst1][User2][Asst2][User3][Asst3][User4][Asst4]‚óÜ    ‚îÇ\n",
    "‚îÇ           ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ cached ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ                 ‚îÇ\n",
    "‚îÇ                     3000 @ $0.30/M + 2000 new @ $3/M  =  $0.0069               ‚îÇ\n",
    "‚îÇ           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                         ‚îÇ\n",
    "‚îÇ                                              Total: $0.0166  (45% savings)     ‚îÇ\n",
    "‚îÇ                                                                                 ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                                 ‚îÇ\n",
    "‚îÇ  COMPACTION + CACHING: Double benefit                                           ‚îÇ\n",
    "‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                                           ‚îÇ\n",
    "‚îÇ                                                                                 ‚îÇ\n",
    "‚îÇ    Main Chat                      Background Summarizer                         ‚îÇ\n",
    "‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                         ‚îÇ\n",
    "‚îÇ                                                                                 ‚îÇ\n",
    "‚îÇ  [Conversation grows...]          [Same conversation prefix]‚óÜ + [Summarize!]   ‚îÇ\n",
    "‚îÇ         ‚îÇ                                    ‚îÇ                                  ‚îÇ\n",
    "‚îÇ         ‚îÇ                         Cache hit! Only pays for                      ‚îÇ\n",
    "‚îÇ         ‚îÇ                         the summarization prompt                      ‚îÇ\n",
    "‚îÇ         ‚îÇ                                    ‚îÇ                                  ‚îÇ\n",
    "‚îÇ         ‚ñº                                    ‚ñº                                  ‚îÇ\n",
    "‚îÇ  Context limit reached  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  Session memory ready instantly                ‚îÇ\n",
    "‚îÇ                                  (built cheaply in background)                  ‚îÇ\n",
    "‚îÇ                                                                                 ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ  Key insight: The background summarizer reuses the same conversation     ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ  prefix that was just sent to the main chat - automatic cache hit!       ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îÇ                                                                                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚óÜ = cache_control breakpoint (cache everything before this point)\n",
    "```\n",
    "\n",
    "### Why this matters for compaction\n",
    "\n",
    "| Scenario | Cost per background update | Notes |\n",
    "|----------|---------------------------|-------|\n",
    "| No caching | Full input cost | 5,000 tokens √ó $3/M = $0.015 |\n",
    "| With caching | ~10% of input cost | 500 new + 4,500 cached = $0.003 |\n",
    "| **Savings** | **~80%** | Compounds over many updates |\n",
    "\n",
    "The longer the conversation, the bigger the savings‚Äîexactly when you need compaction most!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the Caching Works\n",
    "\n",
    "The key is in `_add_cache_control()` and `_create_session_memory_cached()`:\n",
    "\n",
    "```python\n",
    "# 1. Mark the last conversation message with cache_control\n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\n",
    "        \"type\": \"text\",\n",
    "        \"text\": msg[\"content\"],\n",
    "        \"cache_control\": {\"type\": \"ephemeral\"}  # <-- This creates a cache breakpoint\n",
    "    }]\n",
    "}\n",
    "\n",
    "# 2. Also mark the system prompt\n",
    "system=[{\n",
    "    \"type\": \"text\",\n",
    "    \"text\": \"You are a session memory agent...\",\n",
    "    \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "}]\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- The first background update creates a cache entry for `[System + Messages]`\n",
    "- Subsequent updates with the same message prefix get **cache hits**\n",
    "- Only the new summarization instruction is billed at full price\n",
    "- Cache entries have a 5-minute TTL, so rapid updates benefit most\n",
    "\n",
    "**Cost math:**\n",
    "- Without caching: 5,000 tokens √ó $3.00/1M = $0.015 per update\n",
    "- With caching: 500 new tokens √ó $3.00/1M + 4,500 cached √ó $0.30/1M = $0.00285\n",
    "- **Savings: ~80%** on background summarization costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coconut",
   "language": "coconut",
   "name": "coconut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".coco",
   "mimetype": "text/x-python3",
   "name": "coconut",
   "pygments_lexer": "coconut",
   "version": "3.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}