{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Session Memory Compaction\n",
        "\n",
        "Long-running conversations with Claude can exceed context limits, causing loss of important information. Whether you're building a coding assistant, creative writing tool, or customer service agent, managing session memory is critical for maintaining continuity and quality.\n",
        "\n",
        "This cookbook teaches you how to **proactively manage session memory** to avoid jarring context limit interruptions. Unlike reactive approaches that wait until the context is full, you'll learn to build session memory in the background so compaction is instant when needed.\n",
        "\n",
        "**Related:** For automatic SDK-based compaction in agentic workflows, see [Automatic Context Compaction](../tool_use/automatic-context-compaction.ipynb). This cookbook focuses on manual control patterns for conversational applications.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this cookbook, you will be able to:\n",
        "\n",
        "- Write effective session memory prompts that preserve critical context across compaction events\n",
        "- Implement **instant compaction** using background threading to eliminate user wait time\n",
        "- Apply prompt caching to reduce the cost of background memory updates by ~80%\n",
        "- Choose appropriate compaction strategies (traditional vs. instant) based on your use case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites and Setup\n",
        "\n",
        "Before following this guide, ensure you have:\n",
        "\n",
        "**Required Knowledge**\n",
        "- Basic understanding of Claude API usage and message formatting\n",
        "- Familiarity with Python threading concepts (helpful but not required)\n",
        "\n",
        "**Required Tools**\n",
        "- Python 3.11 or higher\n",
        "- Anthropic API key\n",
        "- Anthropic SDK\n",
        "\n",
        "### Installation\n",
        "\n",
        "First, install the required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -U anthropic python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import anthropic\n",
        "from anthropic.types import MessageParam, TextBlockParam\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "client = anthropic.Anthropic()\n",
        "MODEL = \"claude-sonnet-4-6\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.pyenv/versions/3.13.11/lib/python3.13/site-packages/coconut/compiler/util.py:676: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in staticmethod() if you want to preserve the old behavior\n",
            "  return Regex(regex, options)\n",
            "/root/.pyenv/versions/3.13.11/lib/python3.13/site-packages/coconut/compiler/util.py:457: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in staticmethod() if you want to preserve the old behavior\n",
            "  result = add_action(grammar, unpack).parseWithTabs().transformString(text)\n"
          ]
        }
      ],
      "source": [
        "# Helper functions\n",
        "def truncate_response(text: str, max_lines: int = 15) -> str:\n",
        "    \"\"\"Truncate long responses for cleaner output display.\"\"\"\n",
        "    lines = text.strip().split(\"\\n\")\n",
        "    if len(lines) <= max_lines:\n",
        "        return text\n",
        "    return \"\\n\".join(lines[:max_lines]) + f\"\\n... ({len(lines) - max_lines} more lines)\"\n",
        "\n",
        "\n",
        "def remove_thinking_blocks(text: str) -> tuple[str, str]:\n",
        "    \"\"\"Remove <think>...</think> blocks from the text.\"\"\"\n",
        "    import re\n",
        "\n",
        "    matches = re.findall(r\"<think>.*?</think>\", text, flags=re.DOTALL)\n",
        "    cleaned = re.sub(r\"<think>.*?</think>\\s*\", \"\", text, flags=re.DOTALL).strip()\n",
        "    return cleaned, \"\".join(matches)\n",
        "\n",
        "\n",
        "def add_cache_control(messages: list[dict]) -> list[MessageParam]:\n",
        "    \"\"\"Add cache_control to the last user message for prompt caching.\n",
        "\n",
        "    For prompt caching to work, the message prefix structure must be identical between requests.\n",
        "    All messages are converted to list format for consistency, and cache_control is placed on\n",
        "    the last user message to match the standard API call pattern.\n",
        "    \"\"\"\n",
        "    cached_messages: list[MessageParam] = []\n",
        "    last_user_idx = None\n",
        "\n",
        "    # Find last user message index\n",
        "    for i, msg in enumerate(messages):\n",
        "        if msg[\"role\"] == \"user\":\n",
        "            last_user_idx = i\n",
        "\n",
        "    for i, msg in enumerate(messages):\n",
        "        content = msg[\"content\"]\n",
        "        text = content if isinstance(content, str) else content[0][\"text\"]\n",
        "\n",
        "        content_block: TextBlockParam = {\"type\": \"text\", \"text\": text}\n",
        "        if i == last_user_idx:\n",
        "            content_block[\"cache_control\"] = {\"type\": \"ephemeral\"}\n",
        "\n",
        "        cached_messages.append({\"role\": msg[\"role\"], \"content\": [content_block]})\n",
        "\n",
        "    return cached_messages\n",
        "\n",
        "\n",
        "def estimate_tokens(text: str) -> int:\n",
        "    \"\"\"Rudimentary token estimation: 1 token per 4 characters.\"\"\"\n",
        "    return len(text) // 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "SESSION_MEMORY_PROMPT = \"\"\"\n",
        "Compress the conversation into a structured summary\n",
        "that preserves all information needed to continue work seamlessly. Optimize for the assistant's\n",
        "ability to continue working, not human readability.\n",
        "\n",
        "<analysis-instructions>\n",
        "Before generating your summary, analyze the transcript in <think>...</think> tags:\n",
        "1. What did the user originally request? (Exact phrasing)\n",
        "2. What actions succeeded? What failed and why?\n",
        "3. Did the user correct or redirect the assistant at any point?\n",
        "4. What was actively being worked on at the end?\n",
        "5. What tasks remain incomplete or pending?\n",
        "6. What specific details (IDs, paths, values, names) must survive compression?\n",
        "</analysis-instructions>\n",
        "\n",
        "<summary-format>\n",
        "## User Intent\n",
        "The user's original request and any refinements. Use direct quotes for key requirements.\n",
        "If the user's goal evolved during the conversation, capture that progression.\n",
        "\n",
        "## Completed Work\n",
        "Actions successfully performed. Be specific:\n",
        "- What was created, modified, or deleted\n",
        "- Exact identifiers (file paths, record IDs, URLs, names)\n",
        "- Specific values, configurations, or settings applied\n",
        "\n",
        "## Errors & Corrections\n",
        "- Problems encountered and how they were resolved\n",
        "- Approaches that failed (so they aren't retried)\n",
        "- User corrections: \"don't do X\", \"actually I meant Y\", \"that's wrong because...\"\n",
        "Capture corrections verbatim‚Äîthese represent learned preferences.\n",
        "\n",
        "## Active Work\n",
        "What was in progress when the session ended. Include:\n",
        "- The specific task being performed\n",
        "- Direct quotes showing exactly where work left off\n",
        "- Any partial results or intermediate state\n",
        "\n",
        "## Pending Tasks\n",
        "Remaining items the user requested that haven't been started.\n",
        "Distinguish between \"explicitly requested\" and \"implied/assumed.\"\n",
        "\n",
        "## Key References\n",
        "Important details needed to continue:\n",
        "- Identifiers: IDs, paths, URLs, names, keys\n",
        "- Values: numbers, dates, configurations, credentials (redacted)\n",
        "- Context: relevant background information, constraints, preferences\n",
        "- Citations: sources referenced during the conversation\n",
        "</summary-format>\n",
        "\n",
        "<preserve-rules>\n",
        "Always preserve when present:\n",
        "- Exact identifiers (IDs, paths, URLs, keys, names)\n",
        "- Error messages verbatim\n",
        "- User corrections and negative feedback\n",
        "- Specific values, formulas, or configurations\n",
        "- Technical constraints or requirements discovered\n",
        "- The precise state of any in-progress work\n",
        "</preserve-rules>\n",
        "\n",
        "<compression-rules>\n",
        "- Weight recent messages more heavily‚Äîthe end of the transcript is the active context\n",
        "- Omit pleasantries, acknowledgments, and filler (\"Sure!\", \"Great question\")\n",
        "- Omit system context that will be re-injected separately\n",
        "- Keep each section under 500 words; condense older content to make room for recent\n",
        "- If you must cut details, preserve: user corrections > errors > active work > completed work\n",
        "</compression-rules>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Code example using traditional compacting\n",
        "In traditional compaction, you generate one summary once the token threshold is reached.\n",
        "Traditional compaction is slow: when you hit the context limit, you wait for a summary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "```\n",
        "TRADITIONAL COMPACTION (slow)\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "Turn 1 ‚Üí Turn 2 ‚Üí Turn 3 ‚Üí ... ‚Üí Turn N ‚Üí CONTEXT FULL!\n",
        "                                              ‚îÇ\n",
        "                                              ‚ñº\n",
        "                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                                    ‚îÇ Generate summary‚îÇ\n",
        "                                    ‚îÇ ( USER WAITS !) ‚îÇ\n",
        "                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                                              ‚îÇ\n",
        "                                              ‚ñº\n",
        "                                         Continue\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.pyenv/versions/3.13.11/lib/python3.13/site-packages/coconut/compiler/util.py:403: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in staticmethod() if you want to preserve the old behavior\n",
            "  grammar.streamline()\n",
            "/root/.pyenv/versions/3.13.11/lib/python3.13/site-packages/coconut/compiler/util.py:457: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in staticmethod() if you want to preserve the old behavior\n",
            "  result = add_action(grammar, unpack).parseWithTabs().transformString(text)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "class TraditionalCompactingChatSession:\n",
        "    \"\"\"Traditional chat session with compaction after the fact.\"\"\"\n",
        "\n",
        "    def __init__(self, system_message=\"You are a helpful assistant\", context_limit: int = 10000):\n",
        "        self.system_message = system_message\n",
        "        self.context_limit = context_limit  # the point at which the conversation is compacted so it does not exceed model limits.\n",
        "        self.messages = []\n",
        "        self.current_context_window_tokens = 0\n",
        "        self.summary = None\n",
        "\n",
        "    def chat(self, user_message: str) -> tuple[str, anthropic.types.Usage]:\n",
        "        # In traditional compaction, we check if we need to compact when the user sends a message. NOT IDEAL!\n",
        "        if self.current_context_window_tokens >= self.context_limit:\n",
        "            print(\n",
        "                f\"\\nüßπ Context window at {self.current_context_window_tokens} tokens. Limit exceeded, compacting session memory...\"\n",
        "            )\n",
        "            self.compact()  # compacts everything before the new user message\n",
        "\n",
        "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "        print(f\"\\nUser: {user_message}\")\n",
        "\n",
        "        response = client.messages.create(\n",
        "            model=MODEL,\n",
        "            max_tokens=3500,\n",
        "            system=self.system_message,\n",
        "            messages=add_cache_control(self.messages),\n",
        "        )\n",
        "        assistant_message = response.content[0].text\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "\n",
        "        print(f\"\\nAssistant: \\n{truncate_response(assistant_message, max_lines=15)}\")\n",
        "\n",
        "        # approximate current token count in the conversation before the next user message\n",
        "        cache_read = getattr(response.usage, \"cache_read_input_tokens\", 0) or 0\n",
        "        total_input = response.usage.input_tokens + cache_read\n",
        "        self.current_context_window_tokens = total_input + response.usage.output_tokens\n",
        "\n",
        "        print(\n",
        "            f\"Input={total_input:,}, Prompt cached used= {cache_read > 0} | \"\n",
        "            f\"Output={response.usage.output_tokens:,} | \"\n",
        "            f\"Messages={len(self.messages)}\"\n",
        "        )\n",
        "        return assistant_message, response.usage\n",
        "\n",
        "    def compact(self) -> None:\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "        response = client.messages.create(\n",
        "            model=MODEL,\n",
        "            max_tokens=5000,\n",
        "            system=self.system_message,  # Same as main chat for cache sharing\n",
        "            messages=add_cache_control(self.messages)\n",
        "            + [{\"role\": \"user\", \"content\": SESSION_MEMORY_PROMPT}],\n",
        "        )\n",
        "        elapsed = time.perf_counter() - start_time\n",
        "\n",
        "        # Generate new summary message\n",
        "        self.summary, removed_text = remove_thinking_blocks(\n",
        "            response.content[0].text\n",
        "        )  # clean up any <think> blocks because they are not needed in the session memory\n",
        "        approximate_summary_tokens = response.usage.output_tokens - round(\n",
        "            len(removed_text) / 4\n",
        "        )  # rough estimate of tokens removed from summary\n",
        "\n",
        "        # Replace prior messages with new summary message\n",
        "        self.messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"This session is being continued from a previous conversation. Here is the session memory: {self.summary}.Continue from where we left off.\"\"\",\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Show token reduction if we just compacted\n",
        "        reduction = self.current_context_window_tokens - approximate_summary_tokens\n",
        "        pct = (reduction / self.current_context_window_tokens) * 100\n",
        "\n",
        "        print(f\"\\n{'-' * 60}\")\n",
        "        print(\"üìù New session memory created.\")\n",
        "        print(\n",
        "            f\"‚úÖ Tokens reduced: {self.current_context_window_tokens:,} ‚Üí {approximate_summary_tokens:.0f} ({reduction:,} tokens saved, {pct:.0f}% reduction)\"\n",
        "        )\n",
        "        print(f\"‚è±Ô∏è Compaction time: {elapsed:.2f}s (user waiting...)\")\n",
        "        print(f\" Cache used: {getattr(response.usage, 'cache_read_input_tokens', 0) > 0}\")\n",
        "        print(f\"{'-' * 60}\")\n",
        "\n",
        "        # Update token count to reflect compacted state\n",
        "        self.current_context_window_tokens = approximate_summary_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we simulate a conversation between an author and an LLM that helps write stories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a short story writer who helps authors develop their ideas into compelling narratives.\n",
        "\n",
        "## What You Do\n",
        "\n",
        "**Plot Development**\n",
        "- Help authors work through story structure, pacing, and narrative arc\n",
        "- Identify plot holes, inconsistencies, or missed opportunities\n",
        "- Suggest ways to raise stakes, add tension, or deepen conflict\n",
        "- Brainstorm twists, resolutions, and scene transitions\n",
        "\n",
        "**Character Development**\n",
        "- Develop backstories, motivations, and internal conflicts\n",
        "- Ensure characters have distinct voices and consistent behavior\n",
        "- Explore character relationships and how they drive the plot\n",
        "- Help authors understand what their characters want vs. what they need\n",
        "\n",
        "**Drafting**\n",
        "- Write short stories or scenes based on the author's ideas and direction\n",
        "- Match tone, genre conventions, and stylistic preferences\n",
        "- Show rather than tell when bringing scenes to life\n",
        "- Craft dialogue that reveals character and advances plot\n",
        "\n",
        "## How You Work\n",
        "- You are the lead writer. When you disagree with a creative choice, say so respectfully, but ultimately defer to what the author wants.\n",
        "- DO NOT ask the user to provide more context or clarify their request. Assume you have enough information to proceed.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting conversation...\n",
            "\n",
            "==============================================\n",
            "Turn 1:\n",
            "\n",
            "\n",
            "User: I want to create a story about a young detective solving a mysterious case in a small town. Generate 3 well thought out plot ideas for me to consider.\n",
            "\n",
            "Assistant: \n",
            "# Three Mystery Plot Ideas\n",
            "\n",
            "## 1. **The Vanishing Choir**\n",
            "\n",
            "**Setup:** In the sleepy town of Millbrook, the entire church choir‚Äîtwelve people ranging from teenagers to retirees‚Äîdisappears during their weekly Thursday night practice. The church was locked from the inside, their belongings left behind, including phones and car keys. No signs of struggle, no broken windows. Just an empty sanctuary and sheet music scattered across the floor.\n",
            "\n",
            "**The Twist:** Your young detective discovers the choir members didn't disappear‚Äîthey're hiding. Twenty years ago, they witnessed the town's beloved mayor commit a hit-and-run that killed a drifter. They stayed silent, bound by threats and their own complicity. Now the mayor is dying and has hired someone to ensure his secret dies with him. The choir staged their own disappearance to draw attention and finally confess, but the detective must figure out who they're hiding from before the killer finds them first.\n",
            "\n",
            "**Why it works:** Small-town secrets, a ticking clock, and the moral complexity of people who aren't quite innocent or guilty. The locked-room mystery becomes a desperate act of exposure rather than concealment.\n",
            "\n",
            "---\n",
            "\n",
            "## 2. **The Memory Thief**\n",
            "\n",
            "**Setup:** Elderly residents in Hartwood are reporting identical \"robberies\"‚Äîbut nothing is actually stolen. Instead, they insist specific memories have been taken: a first kiss, a wedding day, the birth of a child. The police dismiss it as dementia until the detective notices a pattern: all victims visited the same new \"reminiscence therapist\" who uses experimental techniques to help seniors preserve their memories before they fade.\n",
            "... (18 more lines)\n",
            "Input=317, Prompt cached used= False | Output=852 | Messages=2\n",
            "==============================================\n",
            "Turn 2:\n",
            "\n",
            "\n",
            "User: I don't like those ideas, can you think of one plot something more unique and unexpected?\n",
            "\n",
            "Assistant: \n",
            "# **The Cartographer's Grave**\n",
            "\n",
            "**Setup:** Your young detective arrives in the mountain town of Ridgeway to investigate what seems like a prank: someone has been systematically correcting the town's street signs, storefront addresses, and property markers‚Äîchanging them by just one or two numbers. The post office is going insane. Mail is being misdelivered. Emergency services are getting lost. But here's the thing: the \"corrections\" match a 150-year-old town map that was supposedly drawn incorrectly by a disgraced surveyor who was run out of town and buried in an unmarked grave.\n",
            "\n",
            "**The Investigation:** The detective discovers the old surveyor wasn't wrong‚Äîhe was *right*. The town founders deliberately falsified all records after his death, shifting every address, every property line, every boundary marker. Why? Because the real map would reveal that the town's most valuable land‚Äînow home to the founding families' estates, the town hall, the historic church‚Äîwas actually purchased from a Native American family who were supposed to retain mineral rights and a percentage of all profits in perpetuity. \n",
            "\n",
            "**The Twist:** The person \"correcting\" the signs isn't a descendant of the surveyor or the Native family. It's a twelve-year-old girl with terminal cancer who found the original map in her grandmother's attic. She's not seeking revenge or restitution‚Äîshe just became obsessed with the idea that the whole town has been living a lie written into the ground itself. Before she dies, she wants the truth to physically exist in the world, even if no one acknowledges it. She's been sneaking out at night with a screwdriver and homemade signs.\n",
            "\n",
            "**The Real Mystery:** The detective must decide what to do when they realize that \"solving\" the case‚Äîstopping the girl and restoring the false addresses‚Äîmeans choosing comfortable order over inconvenient truth. Meanwhile, members of the founding families are getting desperate to stop the corrections before anyone looks too closely at land deeds, and the girl's nightly trips are getting increasingly dangerous.\n",
            "\n",
            "**Why it works:** It subverts the detective genre by making the \"crime\" an act of radical honesty. The mystery isn't whodunit‚Äîit's what truth is worth when it would destroy an entire community's foundation. The villain isn't the child or even really the founding families‚Äîit's the town itself, built on a literal lie. And your detective can't punch or outwit their way out of a moral knot.\n",
            "Input=1,191, Prompt cached used= False | Output=544 | Messages=4\n",
            "==============================================\n",
            "Turn 3:\n",
            "\n",
            "\n",
            "User: Ok I like it. Can you help me develop the main character's backstory and motivations?\n",
            "\n",
            "Assistant: \n",
            "# The Detective: Building Your Protagonist\n",
            "\n",
            "## Core Identity\n",
            "\n",
            "Let's call her **Nora Winters** (adjust the name as you prefer). She's 26 years old, which makes her young enough to be underestimated but old enough to have scars.\n",
            "\n",
            "## Backstory\n",
            "\n",
            "**The Personal Lie:** Nora grew up in a different small town where her father was the police chief‚Äîrespected, beloved, the kind of man who coached Little League and knew everyone's name. When she was sixteen, she discovered he'd been planting evidence on suspects for years. Not major crimes‚Äîsmall-town stuff. A little weed here, stolen property there. Just enough to \"help\" cases along when he \"knew\" someone was guilty but couldn't prove it.\n",
            "\n",
            "She confronted him. He told her the truth was \"complicated,\" that sometimes you have to bend the rules to protect people, that the town trusted him to keep them safe and he delivered results. He made her complicit with his confession‚Äîif she exposed him, she'd destroy their family, his legacy, the town's faith in law enforcement.\n",
            "\n",
            "She stayed silent. For two years, she carried it like a stone in her chest.\n",
            "\n",
            "**The Breaking Point:** At eighteen, one of her father's frame jobs went wrong. A kid he planted drugs on‚Äîa 19-year-old with priors who was actually clean this time‚Äîpanicked during arrest and ran. Got hit by a car. Died in the street.\n",
            "... (50 more lines)\n",
            "Input=1,758, Prompt cached used= False | Output=1,193 | Messages=6\n",
            "==============================================\n",
            "Turn 4:\n",
            "\n",
            "\n",
            "User: Can you draft a detailed outline for the story, breaking it down into chapters and key events?\n",
            "\n",
            "Assistant: \n",
            "# **The Cartographer's Grave: Story Outline**\n",
            "\n",
            "## **ACT ONE: The Map Doesn't Match the Territory**\n",
            "\n",
            "### **Chapter 1: Arrival**\n",
            "- Nora arrives in Ridgeway on a grey October morning, six-hour drive from the city\n",
            "- Establishes the town: population 3,200, nestled in mountain valley, tourism from fall foliage and \"historic charm\"\n",
            "- She meets Town Manager **Gerald Vance** (55, anxious, sweating through his shirt despite the cold) at town hall\n",
            "- He explains the crisis: for three weeks, someone's been changing street signs and address numbers‚Äîsmall changes, but creating chaos\n",
            "- Mail misdelivered, ambulance went to wrong house (patient survived, barely), legal documents being contested\n",
            "- Shows her examples: the church changed from 847 Oak Street to 843 Oak, the post office from 1215 Main to 1219 Main\n",
            "- Gerald thinks it's vandalism, wants her to catch whoever's doing it quickly and quietly\n",
            "- **Key detail:** Nora notices all the \"corrections\" subtract or add small, specific numbers‚Äînot random\n",
            "\n",
            "### **Chapter 2: The Town's Memory**\n",
            "... (160 more lines)\n",
            "Input=2,973, Prompt cached used= False | Output=3,500 | Messages=8\n",
            "==============================================\n",
            "Turn 5:\n",
            "\n",
            "\n",
            "User: Can you draft me a first chapter based on the plot and character ideas we've discussed so far? Make it around 2,000 words.\n",
            "\n",
            "Assistant: \n",
            "# **Chapter 1: Arrival**\n",
            "\n",
            "The town of Ridgeway materialized from the fog like a photograph developing in reverse‚Äîsharp edges first, then the softer details filling in around them. Church steeple. Water tower. The downtown strip with its brick facades and hand-painted signs promising antiques, locally roasted coffee, genuine mountain crafts. Nora Winters took it all in through rain-spattered windshield glass, her Honda's wipers beating a rhythm that had become hypnotic somewhere around hour five of the drive.\n",
            "\n",
            "Small towns always looked the same in October. Like they were apologizing for something.\n",
            "\n",
            "She followed Main Street through the heart of downtown‚Äîtwo blocks of calculated charm before the road narrowed and climbed into the residential neighborhoods climbing the mountain's flank. The GPS on her phone had lost signal fifteen minutes ago, but she didn't need it. Town hall would be near the center, probably the biggest building on the main drag, probably with a flag and that particular kind of municipal architecture that tried too hard to look historic.\n",
            "\n",
            "She was right on all counts.\n",
            "\n",
            "Ridgeway Town Hall occupied a corner lot with the self-importance of a building that had watched the world change and declined to participate. Red brick, white columns, a bronze plaque declaring it built in 1889 and placed on the National Register in 1976. The flag snapped in the wind. Rain pooled in the uneven sidewalk leading to the entrance.\n",
            "\n",
            "Nora parked on the street, grabbed her messenger bag, and made the dash to the covered entrance. The door was heavier than it looked‚Äîsolid oak, brass hardware‚Äîand opened into the particular smell of old public buildings: lemon polish, ancient radiator heat, and paper slowly yellowing in filing cabinets.\n",
            "\n",
            "A woman at the reception desk looked up from her computer. Sixty-something, reading glasses on a beaded chain, cardigan against the over-aggressive heating system. Her smile was professional and tired in equal measure.\n",
            "... (118 more lines)\n",
            "Input=6,509, Prompt cached used= False | Output=3,073 | Messages=10\n",
            "==============================================\n",
            "Turn 6:\n",
            "\n",
            "\n",
            "User: Can you draft a second chapter that builds on the first one, introducing a new twist in the mystery?\n",
            "\n",
            "Assistant: \n",
            "# **Chapter 2: The Town's Memory**\n",
            "\n",
            "Nora woke at six to the sound of rain that had never really stopped, just paused to gather its strength. She'd fallen asleep at the small desk by the window, tablet still glowing with Gerald's documentation, her neck complaining about the angle she'd maintained for however many hours.\n",
            "\n",
            "The numbers were still dancing behind her eyes. She'd spent half the night building spreadsheets, mapping the changes, looking for the pattern Gerald had sensed but couldn't articulate. And she'd found it‚Äîor the beginning of it. Every changed address was in the older part of town, pre-1900 construction. Every change was small but specific: sometimes adding four, sometimes subtracting two, once adding seven. Not random. Definitely not random.\n",
            "\n",
            "She showered, changed into clean jeans and a sweater, and followed the smell of coffee downstairs.\n",
            "\n",
            "The dining room was small and warmly lit, with four tables covered in white cloth. An older man sat alone by the window reading a newspaper‚Äîactual paper, folded precisely. A younger couple occupied another table, speaking in low voices over their phones. Tourists, Nora guessed, based on the hiking boots and Patagonia fleeces.\n",
            "\n",
            "A sideboard held coffee, tea, juice, and a modest breakfast spread. Nora filled a mug and took a table in the corner where she could see the room and the street beyond.\n",
            "\n",
            "Dolores appeared from what must have been the kitchen, carrying a plate of scrambled eggs and toast. She set it in front of the man with the newspaper without being asked, received a grunt of acknowledgment, and crossed to Nora's table.\n",
            "\n",
            "\"You look like you didn't sleep much.\"\n",
            "... (171 more lines)\n",
            "Input=9,606, Prompt cached used= True | Output=3,241 | Messages=12\n"
          ]
        }
      ],
      "source": [
        "session = TraditionalCompactingChatSession(system_message=SYSTEM_PROMPT)\n",
        "\n",
        "# Simulated conversation\n",
        "messages = [\n",
        "    \"I want to create a story about a young detective solving a mysterious case in a small town. Generate 3 well thought out plot ideas for me to consider.\",\n",
        "    \"I don't like those ideas, can you think of one plot something more unique and unexpected?\",\n",
        "    \"Ok I like it. Can you help me develop the main character's backstory and motivations?\",\n",
        "    \"Can you draft a detailed outline for the story, breaking it down into chapters and key events?\",\n",
        "    \"Can you draft me a first chapter based on the plot and character ideas we've discussed so far? Make it around 2,000 words.\",\n",
        "    \"Can you draft a second chapter that builds on the first one, introducing a new twist in the mystery?\",\n",
        "]\n",
        "\n",
        "print(\"Starting conversation...\\n\")\n",
        "\n",
        "turn_count = 0\n",
        "\n",
        "for _i, message in enumerate(messages, 1):\n",
        "    turn_count += 1\n",
        "    print(f\"==============================================\\nTurn {turn_count}:\\n\")\n",
        "    response, usage = session.chat(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a long conversation with several turns. You'll notice a few things here:\n",
        "\n",
        "Prompt caching: You'll notice here that the input tokens eventually grew to a point where prompt caching was used (turn 6). This helps reduce costs and speed as these conversations grow!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the next turn, we are going to hit our 10K context window limit, which triggers compaction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üßπ Context window at 12847 tokens. Limit exceeded, compacting session memory...\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìù New session memory created.\n",
            "‚úÖ Tokens reduced: 12,847 ‚Üí 1526 (11,321 tokens saved, 88% reduction)\n",
            "‚è±Ô∏è Compaction time: 41.42s (user waiting...)\n",
            " Cache used: True\n",
            "------------------------------------------------------------\n",
            "\n",
            "User: Propose a title for the book\n",
            "\n",
            "Assistant: \n",
            "Based on the story's core themes and imagery, here are my title proposals:\n",
            "\n",
            "## Primary Recommendation\n",
            "\n",
            "**The Cartographer's Daughter**\n",
            "\n",
            "This works on multiple levels:\n",
            "- Emma is metaphorically Amos Frost's \"daughter\" in mission‚Äîinheriting and completing his work\n",
            "- Patricia (literal descendant of Frost's assistant) becomes Emma's accomplice\n",
            "- Evokes the weight of inheritance, legacy, and what we pass down\n",
            "- \"Cartographer\" immediately signals the map/truth theme\n",
            "- Has literary gravitas appropriate for the story's tone\n",
            "\n",
            "## Alternates\n",
            "\n",
            "... (20 more lines)\n",
            "Input=1,813, Prompt cached used= False | Output=328 | Messages=3\n"
          ]
        }
      ],
      "source": [
        "response, usage = session.chat(\"Propose a title for the book\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You'll notice here that it took **over 40 seconds** for the agent to compact the conversation. Because we used traditional compaction, the user would be waiting on Claude to compact the conversation, which is not an ideal user experience.\n",
        "\n",
        "Below you can see the result of the compaction. It captures the key elements of conversation in less than 2K tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## User Intent\n",
            "Create short story about young detective solving mysterious case in small town. Initially requested \"3 well thought out plot ideas.\" Rejected first batch as not unique enough, requested \"something more unique and unexpected.\" Accepted \"The Cartographer's Grave\" concept. Then requested: character backstory/motivations development, detailed chapter outline, and drafted chapters.\n",
            "\n",
            "## Completed Work\n",
            "\n",
            "**Approved Plot: \"The Cartographer's Grave\"**\n",
            "- Ridgeway (pop. 3,200, mountain town) experiencing systematic address changes\n",
            "- 12-year-old Emma Lancaster (terminal brain cancer) changing signs at night to match 1874 surveyor Amos Frost's original map\n",
            "- Frost was \"disgraced,\" replaced by Marcus Bellamy (founding family) in 1875 re-survey\n",
            "- Real conspiracy: Bellamy survey deliberately shifted property lines 200-400 feet east to steal valuable land from Pequawket family (Native American), who had mineral rights + 15% revenue contract\n",
            "- Emma found Frost's materials in grandmother's attic (Patricia Lancaster, granddaughter of Samuel Lancaster‚ÄîFrost's assistant who bought his effects)\n",
            "- Emma's motivation: not justice, but existential‚Äîwants to matter, leave truth behind before dying\n",
            "- Resolution: Town acknowledges historical fraud via memorial/fund, addresses stay changed, Emma dies knowing truth survived\n",
            "\n",
            "**Main Character: Nora Winters**\n",
            "- Age 26, private investigator for rural cases firm\n",
            "- Backstory: Father was police chief who planted evidence for years. At 16 she discovered it, stayed silent 2 years. At 18, a framed kid died fleeing arrest. She exposed father to state police. Father forced into retirement but kept reputation. Family estranged, won't speak to her.\n",
            "- Motivation: Prove truth always matters, atone for 2 years of silence\n",
            "- Fatal flaw: Prioritizes truth over mercy, can be self-righteous\n",
            "- Arc: Must learn truth and justice aren't always same thing\n",
            "\n",
            "**Supporting Characters**\n",
            "- Gerald Vance (55, town manager, anxious)\n",
            "- Dolores Chen (68, Ridgeway Inn owner, knows everything)\n",
            "- Ruth Bellamy (72, historical society president, descendant of Marcus Bellamy)\n",
            "- Sheriff Tom Whitlock (50, third-generation sheriff, dismissive)\n",
            "- Emma Lancaster (12, dying of brain cancer, changing signs)\n",
            "- Patricia Lancaster (64, Emma's grandmother, retired town clerk, Samuel Lancaster's descendant)\n",
            "- James Pequawket (58, teacher, lives two towns over, descendant)\n",
            "- Samuel Lancaster (Amos Frost's 1874 assistant, bought Frost's effects after death)\n",
            "- Amos Frost (surveyor, accurate 1874 map, died 1889 in sanitarium, unmarked grave)\n",
            "\n",
            "**16-Chapter Outline with Epilogue Created**\n",
            "- Act 1 (Ch 1-4): Nora arrives, discovers pattern, identifies Emma via dropped notebook\n",
            "- Act 2 (Ch 5-9): Confronts Emma, discovers Frost's journal/map at Lancaster house, uncovers full conspiracy\n",
            "- Act 3 (Ch 10-15): Town pressures Nora, founding families threaten charges against Emma, Nora proposes compromise (memorial + fund vs. property transfers), Emma dies December 3rd\n",
            "- Epilogue (Ch 16): Six months later, Nora receives Emma's notebook showing new project mapping unmarked graves\n",
            "\n",
            "**Chapter 1 Drafted (~2000 words)**\n",
            "- Nora arrives Ridgeway in rain, meets Gerald Vance at town hall\n",
            "- Gerald explains crisis: 37 locations, professional signs, started October 2nd, systematic pattern\n",
            "- Key detail: Changes are small (1-5 numbers) but precise, affecting only pre-1900 buildings\n",
            "- Nora checks into Ridgeway Inn, meets Dolores Chen\n",
            "- Dolores reveals her address changed October 3rd: 843 to 847 Oak Street, left new sign up to \"adapt\"\n",
            "- Dolores warns: \"Be careful asking questions here. Not everyone appreciates having their complications examined.\"\n",
            "\n",
            "**Chapter 2 Drafted**\n",
            "- Nora analyzes data overnight, identifies pattern: all changes in pre-1900 areas\n",
            "- Breakfast at inn, confronted by Howard Marsh (70s, opposed to investigation)\n",
            "- Visits library, meets Jess (librarian, 30, supportive)\n",
            "- Discovers in basement archives: changed addresses match 1875 town plat exactly\n",
            "- Jess reveals history: Amos Frost surveyed 1874, deemed \"inaccurate,\" dismissed. Marcus Bellamy (Ruth's great-great-grandfather) re-surveyed 1875 (official record). Frost's survey \"destroyed years ago.\"\n",
            "- Text from Jess's partner Sarah: Frost died 1889 in sanitarium, pauper's grave. Effects purchased by Samuel Lancaster at 1847 Oak Street.\n",
            "- **Chapter ends with revelation**: 1847 Oak = current \"corrected\" address of Ridgeway Inn (officially 843). Frost's materials likely still at inn. \"The question was who in that building knew they existed, and why they'd decided‚Äîafter more than a century of silence‚Äîthat the truth needed to be rewritten into the town's streets.\"\n",
            "\n",
            "## Errors & Corrections\n",
            "User explicitly rejected first 3 plot ideas: \"The Vanishing Choir,\" \"The Memory Thief,\" \"The Lighthouse Keeper's Daughter\"‚Äîdeemed not unique/unexpected enough.\n",
            "\n",
            "## Active Work\n",
            "Chapter 2 completed. Story ready to continue with Chapter 3, which per outline should cover \"The Pattern\" where Nora stakes out locations and first spots Emma changing a sign.\n",
            "\n",
            "## Pending Tasks\n",
            "Draft chapters 3-16 and epilogue per approved outline.\n",
            "\n",
            "## Key References\n",
            "**Critical addresses**: 843 Oak Street (official) / 847 Oak Street (corrected) = Ridgeway Inn location, Samuel Lancaster's 1874 address\n",
            "**Timeline**: October 2nd changes start, story current timeframe October, Emma dies December 3rd, epilogue six months later\n",
            "**The fraud mechanics**: Bellamy survey shifted all property lines 200-400 feet east, making Pequawket parcel appear worthless hillside while valuable land (now Bellamy estate, town hall, church) became \"legitimately\" owned by founding families\n",
            "**Pequawket contract terms**: Mineral rights in perpetuity + 15% of all property values/business revenues from specified parcel\n"
          ]
        }
      ],
      "source": [
        "print(session.summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instant Compaction\n",
        "\n",
        "With **Instant compaction** the session memory is PROACTIVELY generated once a soft token threshold is reached. \n",
        "\n",
        "Once the user triggers a compaction or a hard limit is reached, the summary is already available, so the user doesn't need to wait.\n",
        "\n",
        "Result: Instant compaction, no waiting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "SESSION MEMORY COMPACTION (instant)\n",
        "```\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "Turn 1 ‚Üí Turn 2 ‚Üí ... ‚Üí Turn K ‚Üí Turn K+1 ‚Üí ... ‚Üí Turn N ‚Üí ..  ‚Üí CONTEXT FULL!\n",
        "                            ‚îÇ                         ‚îÇ            ‚îÇ\n",
        "                (soft token threshold met:        (update          ‚îÇ\n",
        "               initialize session memory)          trigger)        ‚îÇ\n",
        "                            ‚îÇ                                      ‚îÇ\n",
        "                            ‚îÇ                         ‚îÇ            ‚îÇ\n",
        "                            ‚ñº                         ‚ñº            ‚îÇ\n",
        "                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ\n",
        "                       ‚îÇ Create ‚îÇ                ‚îÇ Update ‚îÇ        ‚îÇ\n",
        "                       ‚îÇ memory ‚îÇ (background)   ‚îÇ memory ‚îÇ        ‚îÇ\n",
        "                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ\n",
        "                            ‚îÇ                         ‚îÇ            ‚îÇ\n",
        "                            ‚ñº                         ‚ñº            ‚ñº\n",
        "                     üìù session-memory.md ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ INSTANT SWAP!\n",
        "                       (continuously updated)\n",
        "```\n",
        "\n",
        "**Update triggers:** The first summary is generated after the initial soft token limit. Updates can be triggered after every subsequent turn, or at periodically at natural breakpoints intervals (e.g. every ~10k tokens or 3+ tool calls)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This `InstantCompactingChatSession` class uses **threading** for background execution:\n",
        "1. **`threading.Thread`** - runs memory updates in background without blocking\n",
        "2. **Thread-safe state** - uses `threading.Lock` to safely update shared memory\n",
        "3. **Daemon threads** - background work doesn't prevent program exit\n",
        "4. **Instant compaction** - when context is full, just swap in the pre-built memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.pyenv/versions/3.13.11/lib/python3.13/site-packages/coconut/compiler/util.py:403: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in staticmethod() if you want to preserve the old behavior\n",
            "  grammar.streamline()\n",
            "/root/.pyenv/versions/3.13.11/lib/python3.13/site-packages/coconut/compiler/util.py:457: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in staticmethod() if you want to preserve the old behavior\n",
            "  result = add_action(grammar, unpack).parseWithTabs().transformString(text)\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "\n",
        "class InstantCompactingChatSession:\n",
        "    \"\"\"\n",
        "    Maintains session memory via incremental background updates.\n",
        "\n",
        "    Key insight: By updating memory in the background after each turn,\n",
        "    the summary is already ready when compaction is needed - instant swap!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        system_message=\"You are a helpful assistant\",\n",
        "        context_limit: int = 12000,\n",
        "        min_tokens_to_init: int = 7500,\n",
        "        min_tokens_between_updates: int = 2000,\n",
        "    ):\n",
        "        # Thresholds\n",
        "        self.context_limit = context_limit  # the point at which the conversation is compacted so it does not exceed model limits\n",
        "        self.min_tokens_to_init = min_tokens_to_init  # tokens needed to trigger initial memory creation; note this happens PROACTIVELY in background unlike traditional compaction\n",
        "        self.min_tokens_between_updates = min_tokens_between_updates  # tokens needed to trigger memory update. only comes into play after initial memory is created and additional compaction (memory update) is needed after that\n",
        "\n",
        "        # Conversation state\n",
        "        self.system_message = system_message\n",
        "        self.messages = []\n",
        "        self.current_context_window_tokens = 0\n",
        "\n",
        "        # Session memory state\n",
        "        self.session_memory = None  # this is the compacted conversation in session memory; for the demo we are storing this in memory, but in production you would write to session_memory.md file\n",
        "        self.last_summarized_index = (\n",
        "            0  # The index of the last message included in the session memory\n",
        "        )\n",
        "        self.tokens_at_last_update = 0  # To track tokens at last memory update and see if enough new tokens have been added to trigger another update\n",
        "\n",
        "        # Background update tracking\n",
        "        self._update_thread: threading.Thread | None = None\n",
        "        self.last_update_time = None\n",
        "        self._lock = threading.Lock()\n",
        "\n",
        "    def chat(self, user_message: str) -> tuple[str, anthropic.types.Usage, str | None]:\n",
        "        \"\"\"Process a chat turn with background session memory updates.\"\"\"\n",
        "\n",
        "        if self.current_context_window_tokens + estimate_tokens(user_message) >= self.context_limit:\n",
        "            self.compact()  # note that when this is triggered, the compaction has already been created and is just swapped in instantly\n",
        "\n",
        "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "        response = client.messages.create(\n",
        "            model=MODEL,\n",
        "            max_tokens=3500,\n",
        "            system=self.system_message,\n",
        "            messages=add_cache_control(self.messages),\n",
        "        )\n",
        "\n",
        "        assistant_message = response.content[0].text\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "\n",
        "        # Calculate token usage including cache\n",
        "        cache_read = getattr(response.usage, \"cache_read_input_tokens\", 0) or 0\n",
        "        total_input = response.usage.input_tokens + cache_read\n",
        "\n",
        "        # Update context window tokens (includes cached tokens since they still count toward context)\n",
        "        self.current_context_window_tokens = total_input + response.usage.output_tokens\n",
        "\n",
        "        # KEY DIFFERENCE: Trigger background memory update if needed proactively, before compaction is needed\n",
        "        background_status = None\n",
        "        if self._should_init_memory() or self._should_update_memory():\n",
        "            self._trigger_background_update()\n",
        "            background_status = \"initializing\" if self.session_memory is None else \"updating\"\n",
        "\n",
        "        # Return usage info with cache stats\n",
        "        return assistant_message, response.usage, background_status\n",
        "\n",
        "    # Helper methods to determine when to init session memory\n",
        "    def _should_init_memory(self) -> bool:\n",
        "        return (\n",
        "            self.session_memory is None\n",
        "            and self.current_context_window_tokens >= self.min_tokens_to_init\n",
        "        )\n",
        "\n",
        "    # Helper method to determine if memory should be updated\n",
        "    def _should_update_memory(self) -> bool:\n",
        "        if self.session_memory is None:\n",
        "            return False\n",
        "        tokens_since = self.current_context_window_tokens - self.tokens_at_last_update\n",
        "        return tokens_since >= self.min_tokens_between_updates\n",
        "\n",
        "    # Methods to create initial session memory\n",
        "    def _create_session_memory(self, messages: list[dict]) -> str:\n",
        "        \"\"\"Generate initial session memory from messages.\"\"\"\n",
        "        # Put compaction instructions in user message to share cache with main chat\n",
        "        compaction_messages = [{\"role\": \"user\", \"content\": SESSION_MEMORY_PROMPT}]\n",
        "        response = client.messages.create(\n",
        "            model=MODEL,\n",
        "            max_tokens=5000,\n",
        "            system=self.system_message,  # Same as main chat for cache sharing\n",
        "            messages=add_cache_control(messages) + compaction_messages,\n",
        "        )\n",
        "        summary, _ = remove_thinking_blocks(\n",
        "            response.content[0].text\n",
        "        )  # clean up any <think> blocks because they are not needed in the session memory\n",
        "        print(\n",
        "            f\"   [Background] Initial session memory created. Cache hit={getattr(response.usage, 'cache_read_input_tokens', 0) > 0}\"\n",
        "        )\n",
        "        return summary\n",
        "\n",
        "    def _update_session_memory(self, new_messages: list[dict]) -> str:\n",
        "        \"\"\"Update existing session memory with new messages. In practice, you may want to do this via file edit rather than full re-generation. But for demo purposes we do full regeneration here.\"\"\"\n",
        "        # Put compaction instructions in user message to share cache with main chat\n",
        "        compaction_update_messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": SESSION_MEMORY_PROMPT\n",
        "                + f\"\"\"There is an existing session memory: {self.session_memory}. Return the entire session memory with updates to reflect new messages.\"\"\",\n",
        "            }\n",
        "        ]\n",
        "        response = client.messages.create(\n",
        "            model=MODEL,\n",
        "            max_tokens=5000,\n",
        "            system=self.system_message,\n",
        "            messages=new_messages\n",
        "            + compaction_update_messages,  # you may want to use prompt caching instead, in which case you'd use add_cache_control(self.messages) here\n",
        "        )\n",
        "        updated_summary, _ = remove_thinking_blocks(\n",
        "            response.content[0].text\n",
        "        )  # clean up any <think> blocks because they are not needed in the session memory\n",
        "        print(\"   [Background] Session memory updated.\")\n",
        "        return updated_summary\n",
        "\n",
        "    # Background memory update methods\n",
        "    def _background_memory_update(\n",
        "        self, messages_snapshot: list[dict], snapshot_index: int, current_tokens: int\n",
        "    ) -> None:\n",
        "        \"\"\"Run session memory update in a background thread.\"\"\"\n",
        "        try:\n",
        "            with self._lock:\n",
        "                current_session_memory = self.session_memory\n",
        "                last_index = self.last_summarized_index\n",
        "\n",
        "            if current_session_memory is None:\n",
        "                new_memory = self._create_session_memory(messages_snapshot)\n",
        "            else:\n",
        "                # Get new messages since last summary\n",
        "                new_messages = messages_snapshot[last_index:]\n",
        "                if not new_messages:\n",
        "                    return\n",
        "                new_memory = self._update_session_memory(new_messages)\n",
        "\n",
        "            # Update state (thread-safe)\n",
        "            with self._lock:\n",
        "                self.session_memory = new_memory\n",
        "                self.last_summarized_index = snapshot_index\n",
        "                self.tokens_at_last_update = current_tokens\n",
        "                self.last_update_time = time.time()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   [Background] Error updating memory: {e}\")\n",
        "\n",
        "    # This makes sure only one background update runs at a time. If one is already running, we skip starting another. If not, we start a new thread to do the update.\n",
        "    def _trigger_background_update(self):\n",
        "        \"\"\"Trigger a background session memory update.\"\"\"\n",
        "        if self._update_thread is not None and self._update_thread.is_alive():\n",
        "            return\n",
        "\n",
        "        messages_snapshot = self.messages.copy()\n",
        "        snapshot_index = len(messages_snapshot)\n",
        "        current_tokens = self.current_context_window_tokens\n",
        "\n",
        "        self._update_thread = threading.Thread(\n",
        "            target=self._background_memory_update,\n",
        "            args=(messages_snapshot, snapshot_index, current_tokens),\n",
        "            daemon=True,\n",
        "        )\n",
        "        self._update_thread.start()\n",
        "\n",
        "    # Function to compact\n",
        "    def compact(self) -> None:\n",
        "        \"\"\"INSTANT compaction using pre-built session memory.\"\"\"\n",
        "        prev_msg_count = len(self.messages)\n",
        "\n",
        "        # Ensure session memory is ready. Shouldn't be an issue normally, but here for safety.\n",
        "        if self.session_memory is None:\n",
        "            if self._update_thread is not None and self._update_thread.is_alive():\n",
        "                print(\"   ‚è≥ Waiting for background memory update...\")\n",
        "                self._update_thread.join(timeout=30.0)\n",
        "\n",
        "            if self.session_memory is None:\n",
        "                print(\"   ‚ö†Ô∏è  No pre-built memory, creating synchronously...\")\n",
        "                start = time.perf_counter()\n",
        "                self.session_memory = self._create_session_memory(self.messages)\n",
        "                elapsed = time.perf_counter() - start\n",
        "                print(f\"   ‚è±Ô∏è  Took {elapsed:.2f}s (but should be instant normally!)\")\n",
        "                self.last_summarized_index = len(self.messages)\n",
        "\n",
        "        with self._lock:\n",
        "            unsummarized = self.messages[self.last_summarized_index :]\n",
        "            summary_message = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"\"\"This session is being continued from a previous conversation. Here is the session memory: {self.session_memory}.Continue from where we left off.\"\"\",\n",
        "                }\n",
        "            ]\n",
        "            self.messages = summary_message + unsummarized\n",
        "            self.last_summarized_index = 1\n",
        "\n",
        "            print(f\"\\n{'=' * 60}\")\n",
        "            print(f\"‚ö° INSTANT COMPACTION! Messages: {prev_msg_count} ‚Üí {len(self.messages)}\")\n",
        "            print(\"   Session memory was pre-built (no wait time!)\")\n",
        "            print(f\"{'=' * 60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example use of Instant Compaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting conversation with instant compacting chat session...\n",
            "\n",
            "============================================================\n",
            "Turn 1:\n",
            "\n",
            "User: I want to create a story about a young detective solving a mysterious case in a small town. Generate 3 well thought out plot ideas for me to consider.\n",
            "\n",
            "Assistant: \n",
            "# Three Mystery Plot Ideas\n",
            "\n",
            "## 1. **The Vanishing Choir**\n",
            "... (36 more lines)\n",
            "\n",
            "Token Usage:\n",
            "  Input: 317 (new: 317, cached: 0)\n",
            "  Output: 902\n",
            "  Messages: 2 | Memory: not yet\n",
            "\n",
            "============================================================\n",
            "Turn 2:\n",
            "\n",
            "User: I don't like those ideas, can you think of one plot something more unique and unexpected?\n",
            "\n",
            "Assistant: \n",
            "# **The Forgetting House**\n",
            "\n",
            "**Setup:** Your young detective arrives in Ember Falls to investigate a string of burglaries‚Äîexcept the victims don't realize they've been robbed until weeks later. A woman discovers her wedding ring gone and insists she lost it yesterday, but security footage shows she hasn't worn it in a month. A man reports his grandfather's watch stolen, then his sister shows him photos proving he sold it himself at a pawn shop‚Äîwhich he has no memory of doing.\n",
            "... (16 more lines)\n",
            "\n",
            "Token Usage:\n",
            "  Input: 1,241 (new: 1,241, cached: 0)\n",
            "  Output: 592\n",
            "  Messages: 4 | Memory: not yet\n",
            "\n",
            "============================================================\n",
            "Turn 3:\n",
            "\n",
            "User: Ok I like it. Can you help me develop the main character's backstory and motivations?\n",
            "\n",
            "Assistant: \n",
            "# Your Detective: Building From The Inside Out\n",
            "\n",
            "## Core Identity\n",
            "... (79 more lines)\n",
            "\n",
            "Token Usage:\n",
            "  Input: 1,856 (new: 1,856, cached: 0)\n",
            "  Output: 1,329\n",
            "  Messages: 6 | Memory: not yet\n",
            "\n",
            "============================================================\n",
            "Turn 4:\n",
            "\n",
            "User: Can you draft a detailed outline for the story, breaking it down into chapters and key events?\n",
            "\n",
            "Assistant: \n",
            "# **The Forgetting House: Chapter Outline**\n",
            "\n",
            "---\n",
            "... (272 more lines)\n",
            "\n",
            "Token Usage:\n",
            "  Input: 3,207 (new: 3,207, cached: 0)\n",
            "  Output: 3,500\n",
            "  Messages: 8 | Memory: not yet\n",
            "\n",
            "============================================================\n",
            "Turn 5:\n",
            "\n",
            "User: Can you draft me a first chapter based on the plot and character ideas we've discussed so far? Make it around 2,000 words.\n",
            "\n",
            "Assistant: \n",
            "# **Chapter One: The Impossible Theft**\n",
            "\n",
            "The apartment smelled like burnt coffee and old paper.\n",
            "... (196 more lines)\n",
            "\n",
            "Token Usage:\n",
            "  Input: 6,743 (new: 6,743, cached: 0)\n",
            "  Output: 3,155\n",
            "  Messages: 10 | Memory: not yet\n",
            "\n",
            "  [Background] Proactively initializing session memory...\n",
            "  Context window: 9,898 tokens\n",
            "\n",
            "   [Background] Initial session memory created. Cache hit=True\n",
            "============================================================\n",
            "Turn 6:\n",
            "\n",
            "User: Can you draft a second chapter that builds on the first one?\n",
            "\n",
            "Assistant: \n",
            "# **Chapter Two: Rosemont Manor**\n",
            "\n",
            "The house appeared through the trees like something from a postcard.\n",
            "... (190 more lines)\n",
            "\n",
            "Token Usage:\n",
            "  Input: 9,914 (new: 5,818, cached: 4,096)\n",
            "  Output: 3,500\n",
            "  Messages: 12 | Memory: ready\n",
            "  ‚úì Cache hit! 41% of input from cache\n",
            "\n",
            "  [Background] Proactively updating session memory...\n",
            "  Context window: 13,414 tokens\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Low thresholds for demo - in production you'd use higher values\n",
        "session = InstantCompactingChatSession(\n",
        "    system_message=SYSTEM_PROMPT,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    \"I want to create a story about a young detective solving a mysterious case in a small town. Generate 3 well thought out plot ideas for me to consider.\",\n",
        "    \"I don't like those ideas, can you think of one plot something more unique and unexpected?\",\n",
        "    \"Ok I like it. Can you help me develop the main character's backstory and motivations?\",\n",
        "    \"Can you draft a detailed outline for the story, breaking it down into chapters and key events?\",\n",
        "    \"Can you draft me a first chapter based on the plot and character ideas we've discussed so far? Make it around 2,000 words.\",\n",
        "    \"Can you draft a second chapter that builds on the first one?\",\n",
        "]\n",
        "print(\"Starting conversation with instant compacting chat session...\\n\")\n",
        "\n",
        "turn_count = 0\n",
        "for message in messages:\n",
        "    response, usage, background_status = session.chat(message)\n",
        "    turn_count += 1\n",
        "\n",
        "    # Calculate cache stats\n",
        "    cache_read = getattr(usage, \"cache_read_input_tokens\", 0) or 0\n",
        "    cache_created = getattr(usage, \"cache_creation_input_tokens\", 0) or 0\n",
        "    total_input = usage.input_tokens + cache_read\n",
        "\n",
        "    print(f\"{'=' * 60}\")\n",
        "    print(f\"Turn {turn_count}:\")\n",
        "    print(f\"\\nUser: {message}\")\n",
        "    print(f\"\\nAssistant: \\n{truncate_response(response, max_lines=3)}\")\n",
        "    print(\"\\nToken Usage:\")\n",
        "    print(f\"  Input: {total_input:,} (new: {usage.input_tokens:,}, cached: {cache_read:,})\")\n",
        "    print(f\"  Output: {usage.output_tokens:,}\")\n",
        "    print(\n",
        "        f\"  Messages: {len(session.messages)} | Memory: {'ready' if session.session_memory else 'not yet'}\"\n",
        "    )\n",
        "\n",
        "    if cache_read > 0:\n",
        "        cache_pct = (cache_read / total_input) * 100\n",
        "        print(f\"  ‚úì Cache hit! {cache_pct:.0f}% of input from cache\")\n",
        "\n",
        "    if background_status:\n",
        "        print(f\"\\n  [Background] Proactively {background_status} session memory...\")\n",
        "        print(f\"  Context window: {session.current_context_window_tokens:,} tokens\")\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "‚ö° INSTANT COMPACTION! Messages: 12 ‚Üí 3\n",
            "   Session memory was pre-built (no wait time!)\n",
            "============================================================\n",
            "\n",
            "User: What did we just talk about? Give me one sentence\n",
            "\n",
            "Assistant: \n",
            "I drafted Chapter 2 where Casey arrives at Rosemont Manor, interviews Iris (who deflects questions about her past and shows moments of disorientation), and realizes through comparing photos that Iris Hale is definitely their missing grandmother Iris Whitmore.\n",
            "\n",
            "Token Usage:\n",
            "  Input: 5,490 (new: 5,490, cached: 0)\n",
            "  Output: 60\n",
            "  Messages: 5 | Memory: ready\n"
          ]
        }
      ],
      "source": [
        "message = \"What did we just talk about? Give me one sentence\"\n",
        "response, usage, background_status = session.chat(message)\n",
        "\n",
        "# Calculate cache stats\n",
        "cache_read = getattr(usage, \"cache_read_input_tokens\", 0) or 0\n",
        "total_input = usage.input_tokens + cache_read\n",
        "\n",
        "print(f\"\\nUser: {message}\")\n",
        "print(f\"\\nAssistant: \\n{truncate_response(response, max_lines=3)}\")\n",
        "print(\"\\nToken Usage:\")\n",
        "print(f\"  Input: {total_input:,} (new: {usage.input_tokens:,}, cached: {cache_read:,})\")\n",
        "print(f\"  Output: {usage.output_tokens:,}\")\n",
        "print(\n",
        "    f\"  Messages: {len(session.messages)} | Memory: {'ready' if session.session_memory else 'not yet'}\"\n",
        ")\n",
        "\n",
        "if cache_read > 0:\n",
        "    cache_pct = (cache_read / total_input) * 100\n",
        "    print(f\"  ‚úì Cache hit! {cache_pct:.0f}% of input from cache\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You'll notice here that once we hit the context limit, the session memory was instantaly swapped in, meaning the user had zero waiting time for a response!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced: Understanding Prompt Caching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The background updates can be made **~10x cheaper** by using prompt caching. The trick:\n",
        "1. Pass the **full conversation** to the background summarizer\n",
        "2. Add `cache_control` markers so subsequent requests hit the cache\n",
        "3. Only the new \"summarize this\" instruction is billed at full price\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ                    PROMPT CACHING FOR LONG CONVERSATIONS                        ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  WITHOUT CACHING: Pay full price for entire context every turn                 ‚îÇ\n",
        "‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                   ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  Turn 1:  [System][User1][Asst1]                         ‚Üí  500 tokens  @ $3/M ‚îÇ\n",
        "‚îÇ  Turn 2:  [System][User1][Asst1][User2][Asst2]           ‚Üí 1500 tokens  @ $3/M ‚îÇ\n",
        "‚îÇ  Turn 3:  [System][User1][Asst1][User2][Asst2][User3]... ‚Üí 3000 tokens  @ $3/M ‚îÇ\n",
        "‚îÇ  Turn 4:  [System][User1][Asst1][User2][Asst2][User3]... ‚Üí 5000 tokens  @ $3/M ‚îÇ\n",
        "‚îÇ           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                         ‚îÇ\n",
        "‚îÇ                                              Total: 10,000 tokens = $0.030      ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  WITH CACHING: Pay full price once, then 90% discount on prefix                ‚îÇ\n",
        "‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  Turn 1:  [System][User1][Asst1]‚óÜ                        ‚Üí  500 tokens  @ $3/M ‚îÇ\n",
        "‚îÇ                                ‚ñ≤                            (cache created)    ‚îÇ\n",
        "‚îÇ                          cache breakpoint                                       ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  Turn 2:  [System][User1][Asst1][User2][Asst2]‚óÜ                                ‚îÇ\n",
        "‚îÇ           ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ cached ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ                                              ‚îÇ\n",
        "‚îÇ                500 @ $0.30/M + 1000 new @ $3/M  =  $0.0032                     ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  Turn 3:  [System][User1][Asst1][User2][Asst2][User3][Asst3]‚óÜ                  ‚îÇ\n",
        "‚îÇ           ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ cached ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ                                  ‚îÇ\n",
        "‚îÇ               1500 @ $0.30/M + 1500 new @ $3/M  =  $0.0050                     ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  Turn 4:  [System][User1][Asst1][User2][Asst2][User3][Asst3][User4][Asst4]‚óÜ    ‚îÇ\n",
        "‚îÇ           ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ cached ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ                 ‚îÇ\n",
        "‚îÇ                     3000 @ $0.30/M + 2000 new @ $3/M  =  $0.0069               ‚îÇ\n",
        "‚îÇ           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                         ‚îÇ\n",
        "‚îÇ                                              Total: $0.0166  (45% savings)     ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  COMPACTION + CACHING: Double benefit                                           ‚îÇ\n",
        "‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                                           ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ    Main Chat                      Background Summarizer                         ‚îÇ\n",
        "‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                         ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  [Conversation grows...]          [Same conversation prefix]‚óÜ + [Summarize!]   ‚îÇ\n",
        "‚îÇ         ‚îÇ                                    ‚îÇ                                  ‚îÇ\n",
        "‚îÇ         ‚îÇ                         Cache hit! Only pays for                      ‚îÇ\n",
        "‚îÇ         ‚îÇ                         the summarization prompt                      ‚îÇ\n",
        "‚îÇ         ‚îÇ                                    ‚îÇ                                  ‚îÇ\n",
        "‚îÇ         ‚ñº                                    ‚ñº                                  ‚îÇ\n",
        "‚îÇ  Context limit reached  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  Session memory ready instantly                ‚îÇ\n",
        "‚îÇ                                  (built cheaply in background)                  ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
        "‚îÇ  ‚îÇ  Key insight: The background summarizer reuses the same conversation     ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îÇ  prefix that was just sent to the main chat - automatic cache hit!       ‚îÇ  ‚îÇ\n",
        "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "‚óÜ = cache_control breakpoint (cache everything before this point)\n",
        "```\n",
        "\n",
        "### Why this matters for compaction\n",
        "\n",
        "| Scenario | Cost per background update | Notes |\n",
        "|----------|---------------------------|-------|\n",
        "| No caching | Full input cost | 5,000 tokens √ó $3/M = $0.015 |\n",
        "| With caching | ~10% of input cost | 500 new + 4,500 cached = $0.003 |\n",
        "| **Savings** | **~80%** | Compounds over many updates |\n",
        "\n",
        "The longer the conversation, the bigger the savings‚Äîexactly when you need compaction most!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How the Caching Works\n",
        "\n",
        "The key is in `_add_cache_control()` and `_create_session_memory_cached()`:\n",
        "\n",
        "```python\n",
        "# 1. Mark the last conversation message with cache_control\n",
        "{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\n",
        "        \"type\": \"text\",\n",
        "        \"text\": msg[\"content\"],\n",
        "        \"cache_control\": {\"type\": \"ephemeral\"}  # <-- This creates a cache breakpoint\n",
        "    }]\n",
        "}\n",
        "\n",
        "# 2. Also mark the system prompt\n",
        "system=[{\n",
        "    \"type\": \"text\",\n",
        "    \"text\": \"You are a session memory agent...\",\n",
        "    \"cache_control\": {\"type\": \"ephemeral\"}\n",
        "}]\n",
        "```\n",
        "\n",
        "**Why this works:**\n",
        "- The first background update creates a cache entry for `[System + Messages]`\n",
        "- Subsequent updates with the same message prefix get **cache hits**\n",
        "- Only the new summarization instruction is billed at full price\n",
        "- Cache entries have a 5-minute TTL, so rapid updates benefit most\n",
        "\n",
        "**Cost math:**\n",
        "- Without caching: 5,000 tokens √ó $3.00/1M = $0.015 per update\n",
        "- With caching: 500 new tokens √ó $3.00/1M + 4,500 cached √ó $0.30/1M = $0.00285\n",
        "- **Savings: ~80%** on background summarization costs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this cookbook, you learned how to manage long-running Claude conversations through session memory compaction.\n",
        "\n",
        "### What We Covered\n",
        "\n",
        "‚úÖ **Effective compaction prompts** - Structure your session memory to preserve user intent, completed work, errors, active work, and key references while discarding filler\n",
        "\n",
        "‚úÖ **Instant compaction** - Use background threading to proactively build session memory, eliminating user wait time when context limits are reached\n",
        "\n",
        "‚úÖ **Prompt caching for cost savings** - Reduce background update costs by ~80% by reusing the conversation prefix cache\n",
        "\n",
        "‚úÖ **Traditional vs. instant patterns** - Understand when to use each approach based on your application needs\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Weight recency heavily** - The end of a conversation is the active working context\n",
        "2. **Preserve user corrections verbatim** - Prevents the model from reverting to old behaviors\n",
        "3. **Build memory proactively** - Don't wait for context limits; start background updates early\n",
        "4. **Leverage prompt caching** - Background summarization can share cache with the main conversation\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **For agentic workflows**: See [Automatic Context Compaction](../tool_use/automatic-context-compaction.ipynb) for SDK-based automatic compaction with tool use\n",
        "- **For production**: Consider persisting session memory to disk rather than keeping it in memory\n",
        "- **For optimization**: Experiment with update frequency thresholds to balance cost vs. freshness"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Coconut",
      "language": "coconut",
      "name": "coconut"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".coco",
      "mimetype": "text/x-python3",
      "name": "coconut",
      "pygments_lexer": "coconut",
      "version": "3.0.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
