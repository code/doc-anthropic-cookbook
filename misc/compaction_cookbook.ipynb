{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compaction Cookbook: Incremental Session Memory Strategy\n",
    "\n",
    "This notebook demonstrates an efficient compaction strategy that uses **incremental background summarization** instead of summarizing everything at compaction time.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "Traditional compaction summarizes the entire conversation when context gets full. This is:\n",
    "- **Slow**: Requires a blocking API call at the moment the user is waiting\n",
    "- **Disruptive**: The user experiences latency at the worst possible time\n",
    "\n",
    "## The Solution: Session Memory\n",
    "\n",
    "Instead, we maintain a **running summary** that updates incrementally in the background:\n",
    "1. Periodically summarize new messages into a \"session memory\"\n",
    "2. Track which messages have been summarized\n",
    "3. At compaction time, just use the pre-computed summary + unsummarized messages\n",
    "\n",
    "**Key benefit**: The compaction itself is instant - no API call needed when context is full.\n",
    "\n",
    "**Trade-off**: This adds overhead from periodic summarization calls, so it doesn't reduce total API cost. The value is in eliminating user-facing latency.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "from anthropic.types import MessageParam\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "MODEL = \"claude-sonnet-4-5-20250929\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Memory Manager\n",
    "\n",
    "This class manages the incremental summarization strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SessionMemory:\n",
    "    \"\"\"Manages incremental session summarization for fast compaction.\"\"\"\n",
    "    min_tokens_to_init: int = 1000  # Tokens before first summarization\n",
    "    min_tokens_between_updates: int = 500  # Tokens between updates\n",
    "    summary: str = \"\"\n",
    "    last_summarized_count: int = 0  # Message count at last summarization\n",
    "    tokens_at_last_update: int = 0\n",
    "    current_tokens: int = 0\n",
    "\n",
    "    def update_tokens(self, tokens: int):\n",
    "        \"\"\"Update current token count (call after each API response).\"\"\"\n",
    "        self.current_tokens = tokens\n",
    "\n",
    "    def should_summarize(self) -> bool:\n",
    "        \"\"\"Check if we should run background summarization.\"\"\"\n",
    "        if self.current_tokens < self.min_tokens_to_init:\n",
    "            return False\n",
    "\n",
    "        tokens_since = self.current_tokens - self.tokens_at_last_update\n",
    "        return tokens_since >= self.min_tokens_between_updates\n",
    " \n",
    "    def compact_conversation(self, messages: list[MessageParam], summarize_fn):\n",
    "        \"\"\"Incrementally summarize new messages in the background.\"\"\"\n",
    "        new_messages = messages[self.last_summarized_count :]\n",
    "        if not new_messages:\n",
    "            return\n",
    "\n",
    "        self.summary = summarize_fn(new_messages, self.summary)\n",
    "        self.last_summarized_count = len(messages)\n",
    "        self.tokens_at_last_update = self.current_tokens\n",
    "\n",
    "        print(f\"  [Background] Summarized {len(new_messages)} messages at {self.current_tokens} tokens\")\n",
    "\n",
    "        return self.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization Function\n",
    "\n",
    "The summarization function calls Claude to extract key information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_messages(messages: list[MessageParam], existing_memory: str) -> str:\n",
    "    \"\"\"Use Claude to incrementally summarize conversation messages.\"\"\"\n",
    "    conversation_text = \"\\n\".join(f\"{msg['role'].upper()}: {msg['content']}\" for msg in messages)\n",
    "\n",
    "    if existing_memory:\n",
    "        prompt = f\"\"\"Update this session memory with new conversation turns.\n",
    "\n",
    "<existing_summary>\n",
    "{existing_memory}\n",
    "</existing_summary>\n",
    "\n",
    "<new_messages>\n",
    "{conversation_text}\n",
    "</new_messages>\n",
    "\n",
    "Return only the updated summary.\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Summarize this conversation concisely.\n",
    "\n",
    "<messages>\n",
    "{conversation_text}\n",
    "</messages>\n",
    "\n",
    "Capture: topics discussed, key decisions, important context. Return only the summary.\"\"\"\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=MODEL,\n",
    "        max_tokens=500,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Conversation Loop with Session Memory\n",
    "\n",
    "Now let's run a real conversation with Claude while demonstrating background summarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session memory with low thresholds for demo\n",
    "session_memory = SessionMemory(\n",
    "    min_tokens_to_init=200, \n",
    "    min_tokens_between_updates=500\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful coding assistant. Keep responses concise but informative.\"\"\"\n",
    "\n",
    "user_questions = [\n",
    "    \"What are Python decorators and why are they useful?\",\n",
    "    \"Show me a simple decorator example that logs function calls.\",\n",
    "    \"How do I create a decorator that accepts arguments?\",\n",
    "    \"Now explain Python's async/await syntax briefly.\",\n",
    "    \"What's the difference between asyncio.gather and asyncio.wait?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONVERSATION WITH BACKGROUND SUMMARIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "messages: list[MessageParam] = []\n",
    "for i, question in enumerate(user_questions, 1):\n",
    "    print(f\"\\n[Turn {i}] USER: {question}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=MODEL,\n",
    "        max_tokens=1024,\n",
    "        system=SYSTEM_PROMPT,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    assistant_msg: MessageParam = {\"role\": \"assistant\", \"content\": response.content[0].text}\n",
    "    messages.append(assistant_msg)\n",
    "    session_memory.update_tokens(response.usage.input_tokens + response.usage.output_tokens) # Update token count with each response\n",
    "\n",
    "    print(f\"ASSISTANT RESPONSE: {response.content[0].text}\")\n",
    "   \n",
    "    if not session_memory.should_summarize():\n",
    "        print(f\"\\nConversation at {response.usage.input_tokens + response.usage.output_tokens} tokens, no summarization needed yet.\")\n",
    "        continue\n",
    "\n",
    "    if session_memory.should_summarize():\n",
    "        print(f\"\\nConversation at {response.usage.input_tokens + response.usage.output_tokens} tokens, running background summarization...\")\n",
    "        # Create the summary of the conversation and update session memory\n",
    "        session_memory.compact_conversation(messages, summarize_messages)\n",
    "        \n",
    "        # Reset the messages to only keep the summary for future turns\n",
    "        summary = session_memory.compact_conversation(messages, summarize_messages)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SESSION MEMORY SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(session_memory.summary)\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": f\"You have been chatting with the user already. Here is the summary of the conversation so far:\\n{summary}\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COMPACTION DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nBefore compaction:\")\n",
    "print(f\"  Total messages: {len(messages)}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nAfter compaction:\")\n",
    "print(f\"  Messages kept (unsummarized): {len(kept_messages)}\")\n",
    "for msg in kept_messages:\n",
    "    content = msg[\"content\"]\n",
    "    preview = content[:50] + \"...\" if len(content) > 50 else content\n",
    "    print(f\"    - {msg['role']}: {preview}\")\n",
    "\n",
    "print(f\"\\nPre-computed summary:\")\n",
    "print(\"-\" * 40)\n",
    "print(summary)\n",
    "print(\"-\" * 40)\n",
    "print(\"\\n(Compaction was instant - no API call needed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Benefits\n",
    "\n",
    "1. **Instant compaction**: No API call needed at compaction time - summary already exists\n",
    "2. **Non-blocking**: Background summarization doesn't interrupt the user\n",
    "3. **No lost context**: Messages after the last summarization are preserved verbatim\n",
    "4. **Configurable thresholds**: Control when summarization happens based on token count\n",
    "\n",
    "## Production Considerations\n",
    "\n",
    "In a real implementation (like Claude Code's `sessionMemory.ts`):\n",
    "\n",
    "- **Background execution**: Summarization runs in a forked process to not block the main conversation\n",
    "- **Tool call awareness**: Don't summarize mid-tool-use to avoid orphaned tool results  \n",
    "- **File persistence**: Session memory is saved to disk (`.claude/session-memory.md`)\n",
    "- **Threshold tuning**: Default is 10K tokens to init, 5K between updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuing After Compaction\n",
    "\n",
    "After compaction, we can continue the conversation using the summary as context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the compacted message history\n",
    "compacted_messages: list[MessageParam] = [\n",
    "    {\"role\": \"user\", \"content\": f\"[Previous conversation summary]\\n{summary}\\n\\n[Continuing conversation]\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I understand. I have context from our previous discussion. How can I help?\"},\n",
    "]\n",
    "compacted_messages.extend(kept_messages)\n",
    "\n",
    "# Continue the conversation\n",
    "print(\"=\" * 60)\n",
    "print(\"CONTINUING CONVERSATION AFTER COMPACTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "follow_up = \"Based on what we discussed, how would I combine a decorator with an async function?\"\n",
    "print(f\"\\nUSER: {follow_up}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "response, tokens = chat(follow_up, compacted_messages)\n",
    "print(f\"ASSISTANT: {response['content']}\")\n",
    "\n",
    "print(f\"\\n[Context: {len(compacted_messages)} messages, {tokens} tokens instead of {session_memory.current_tokens}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the **incremental session memory** pattern for efficient context compaction:\n",
    "\n",
    "| Approach | At Compaction Time | Cost Distribution |\n",
    "|----------|-------------------|-------------------|\n",
    "| **Traditional** | Summarize all messages (slow) | All cost at once |\n",
    "| **Session Memory** | Use pre-computed summary (instant) | Cost spread over time |\n",
    "\n",
    "The key insight is that **summarization work can be done incrementally in the background**, making the actual compaction operation nearly instant. This pattern is particularly valuable for long-running conversations where context management is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: Response Time with Full vs Compacted Context\n",
    "\n",
    "Let's measure how much faster follow-up responses are when using the compacted context vs the full conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def timed_chat(user_message: str, messages: list[MessageParam]) -> tuple[str, float, int]:\n",
    "    \"\"\"Send a message and return response, elapsed time, and input tokens.\"\"\"\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=MODEL,\n",
    "        max_tokens=1024,\n",
    "        system=SYSTEM_PROMPT,\n",
    "        messages=messages + [{\"role\": \"user\", \"content\": user_message}],\n",
    "    )\n",
    "\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    return response.content[0].text, elapsed, response.usage.input_tokens\n",
    "\n",
    "\n",
    "# Build fresh message lists for fair comparison (before any follow-ups)\n",
    "\n",
    "# Full context: original conversation messages\n",
    "full_context_messages = messages.copy()\n",
    "\n",
    "# Compacted context: summary + unsummarized messages\n",
    "compacted_context_messages: list[MessageParam] = [\n",
    "    {\"role\": \"user\", \"content\": f\"[Previous conversation summary]\\n{summary}\\n\\n[Continuing conversation]\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I understand. I have context from our previous discussion. How can I help?\"},\n",
    "]\n",
    "compacted_context_messages.extend(kept_messages)\n",
    "\n",
    "# The follow-up question to test\n",
    "follow_up_question = \"Can you give me a quick example combining decorators with async?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESPONSE TIME COMPARISON: FULL vs COMPACTED CONTEXT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nQuestion: {follow_up_question}\")\n",
    "\n",
    "# Test 1: Full context\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"[1] FULL CONTEXT (original conversation)\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Messages: {len(full_context_messages)} | \", end=\"\")\n",
    "full_response, full_time, full_tokens = timed_chat(follow_up_question, full_context_messages)\n",
    "print(f\"Input tokens: {full_tokens} | Response time: {full_time:.2f}s\")\n",
    "print(f\"\\nAnswer:\\n{full_response}\")\n",
    "\n",
    "# Test 2: Compacted context\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"[2] COMPACTED CONTEXT (session memory)\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Messages: {len(compacted_context_messages)} | \", end=\"\")\n",
    "compact_response, compact_time, compact_tokens = timed_chat(\n",
    "    follow_up_question, compacted_context_messages\n",
    ")\n",
    "print(f\"Input tokens: {compact_tokens} | Response time: {compact_time:.2f}s\")\n",
    "print(f\"\\nAnswer:\\n{compact_response}\")\n",
    "\n",
    "# Results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "token_reduction = full_tokens - compact_tokens\n",
    "token_reduction_pct = (token_reduction / full_tokens) * 100\n",
    "time_saved = full_time - compact_time\n",
    "time_saved_pct = (time_saved / full_time) * 100 if full_time > 0 else 0\n",
    "\n",
    "print(f\"\\nToken reduction: {token_reduction:,} tokens ({token_reduction_pct:.1f}% smaller)\")\n",
    "print(f\"Time saved: {time_saved:.2f}s ({time_saved_pct:.1f}% faster)\")\n",
    "print(f\"\\nFull context:     {full_tokens:,} tokens → {full_time:.2f}s\")\n",
    "print(f\"Compacted context: {compact_tokens:,} tokens → {compact_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Results\n",
    "\n",
    "The comparison shows the response time benefit after compaction:\n",
    "\n",
    "| Metric | Full Context | Compacted Context |\n",
    "|--------|--------------|-------------------|\n",
    "| **Input Tokens** | All messages | Summary + recent |\n",
    "| **Response Time** | Baseline | Faster |\n",
    "\n",
    "**Why compaction speeds up responses:**\n",
    "- Fewer input tokens = faster time-to-first-token\n",
    "- Smaller context = lower cost *per subsequent turn*\n",
    "\n",
    "**Important trade-off**: Session memory does NOT reduce total API cost. It adds overhead from periodic background summarization calls. The benefits are:\n",
    "\n",
    "1. **No compaction latency** - user never waits for a summarization call\n",
    "2. **Faster subsequent responses** - smaller context after compaction\n",
    "3. **Lower cost per turn** - after compaction, each API call is cheaper\n",
    "\n",
    "The value is in **user experience** (no blocking) and **per-turn efficiency** after compaction, not total cost reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost evaluation\n",
    "\n",
    "# Pricing (per million tokens) - Sonnet 4.5\n",
    "INPUT_COST_PER_M = 3.00\n",
    "OUTPUT_COST_PER_M = 15.00\n",
    "\n",
    "\n",
    "def estimate_cost(input_tokens: int, output_tokens: int) -> float:\n",
    "    \"\"\"Estimate cost in dollars.\"\"\"\n",
    "    return (input_tokens * INPUT_COST_PER_M + output_tokens * OUTPUT_COST_PER_M) / 1_000_000\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COST COMPARISON: SESSION MEMORY vs TRADITIONAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Session Memory: background summarization overhead + compacted follow-up\n",
    "print(\"\\n[Session Memory]\")\n",
    "bg_summarize_calls = 2\n",
    "bg_input = bg_summarize_calls * 800\n",
    "bg_output = bg_summarize_calls * 200\n",
    "print(f\"  Background summaries: ~{bg_input:,} input, ~{bg_output:,} output\")\n",
    "print(f\"  Follow-up:            {compact_tokens:,} input\")\n",
    "\n",
    "sm_input = bg_input + compact_tokens\n",
    "sm_output = bg_output + len(compact_response) // 4\n",
    "sm_cost = estimate_cost(sm_input, sm_output)\n",
    "print(f\"  Total: ${sm_cost:.4f}\")\n",
    "\n",
    "# Traditional: compaction + full-context follow-up\n",
    "print(\"\\n[Traditional]\")\n",
    "print(f\"  Compaction:  {full_tokens:,} input\")\n",
    "print(f\"  Follow-up:   {full_tokens:,} input\")\n",
    "\n",
    "trad_input = full_tokens * 2\n",
    "trad_output = 300 + len(full_response) // 4\n",
    "trad_cost = estimate_cost(trad_input, trad_output)\n",
    "print(f\"  Total: ${trad_cost:.4f}\")\n",
    "\n",
    "# Result\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "cost_diff = sm_cost - trad_cost\n",
    "print(f\"Difference: ${cost_diff:+.4f}\")\n",
    "if cost_diff > 0:\n",
    "    print(\"→ Session memory costs more, but eliminates compaction latency\")\n",
    "else:\n",
    "    print(\"→ Session memory costs less due to smaller follow-up context\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
