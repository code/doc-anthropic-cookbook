{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb718d24",
   "metadata": {},
   "source": [
    "# Automatic Context Compaction\n",
    "\n",
    "Long-running agentic tasks can often exceed context limits. Tool heavy workflows or long conversations quickly consume the token context window. In [Effective Context Engineering for AI Agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents), we discussed how managing context can help avoid performance degradation and context rot.\n",
    "\n",
    "The Claude Agent Python SDK can help manage this context by automatically compressing conversation history when token usage exceeds a configurable threshold, allowing tasks to continue beyond the typical 200k token context limit.\n",
    "\n",
    "In this cookbook, we'll demonstrate context compaction through an **agentic customer service workflow**. Imagine you've built an AI customer service agent tasked with processing a queue of support tickets. For each ticket, you must classify the issue, search the knowledge base, set priority, route to the appropriate team, draft a response, and mark it complete. As you process ticket after ticket, the conversation history fills with classifications, knowledge base searches, and drafted responsesâ€”quickly consuming thousands of tokens.\n",
    "\n",
    "## What is Context Compaction?\n",
    "\n",
    "When building agentic workflows with tool use, conversations can grow very large as the agent iterates on complex tasks. The `compaction_control` parameter provides automatic context management by:\n",
    "\n",
    "1. Monitoring token usage across the conversation\n",
    "2. When a threshold is exceeded, injecting a summary prompt as a user turn\n",
    "3. Having the model generate a summary wrapped in `<summary></summary>` tags\n",
    "4. Clearing the conversation history and resuming with only the summary\n",
    "5. Continuing the task with the compressed context\n",
    "\n",
    "## By the end of this cookbook, you'll be able to:\n",
    " \n",
    " - Understand how to effectively manage context limits in iterative workflows\n",
    " - Write agents that leverage automatic context compaction\n",
    " - Design workflows that maintain focus across multiple iterations\n",
    "\n",
    "##  Prerequisites\n",
    "\n",
    "Before following this guide, ensure you have:\n",
    "\n",
    "**Required Knowledge**\n",
    "\n",
    "- Basic understanding of agentic patterns and tool calling\n",
    "\n",
    "**Required Tools**\n",
    "\n",
    "Python 3.11 or higher\n",
    "Anthropic API key\n",
    "Anthropic SDK >= 0.73"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e3b52",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d7f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU anthropic python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba96da5",
   "metadata": {},
   "source": [
    "Note: Ensure your .env file contains:\n",
    "\n",
    "`ANTHROPIC_API_KEY=your_key_here`\n",
    "\n",
    "Load your environment variables and configure the client. We also load a helper utility to visualize Claude message responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68f6d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from utils.visualize import visualize\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = \"claude-sonnet-4-5\"\n",
    "viz = visualize(auto_show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783cf8e3",
   "metadata": {},
   "source": [
    "## Setting the Stage\n",
    "\n",
    "In [utils/customer_service_tools.py](utils/customer_service_tools.py), we've defined several functions for processing customer support tickets:\n",
    "\n",
    "- `get_next_ticket()` - Retrieves the next unprocessed ticket from the queue\n",
    "- `classify_ticket(ticket_id, category)` - Categorizes issues as billing, technical, account, product, or shipping\n",
    "- `search_knowledge_base(query)` - Finds relevant help articles and solutions\n",
    "- `set_priority(ticket_id, priority)` - Assigns priority levels (low, medium, high, urgent)\n",
    "- `route_to_team(ticket_id, team)` - Routes tickets to the appropriate support team\n",
    "- `draft_response(ticket_id, response_text)` - Creates customer-facing responses\n",
    "- `mark_complete(ticket_id)` - Finalizes processed tickets\n",
    "\n",
    "For a customer service agent, these tools enable processing tickets systematically. Each ticket requires classification, research, prioritization, routing, and response drafting. When processing 20-30 tickets in sequence, the conversation history fills with tool results from every classification, every knowledge base search, and every drafted response, causing exponential token growth.\n",
    "\n",
    "We'll start by using the `beta_tool` decorator to import these functions as tools for our agent:\n",
    "\n",
    "```python\n",
    "import anthropic\n",
    "from anthropic import beta_tool\n",
    "\n",
    "@beta_tool\n",
    "def get_next_ticket() -> dict:\n",
    "    \"\"\"Retrieve the next unprocessed support ticket from the queue.\"\"\"\n",
    "    ...\n",
    "```\n",
    "\n",
    "Since our API already has the functions defined using the `beta_tool` decoractor, so we can import them and use them with the Claude API directly here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d2a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "from utils.customer_service_tools import (\n",
    "    classify_ticket,\n",
    "    draft_response,\n",
    "    get_next_ticket,\n",
    "    initialize_ticket_queue,\n",
    "    mark_complete,\n",
    "    route_to_team,\n",
    "    search_knowledge_base,\n",
    "    set_priority,\n",
    ")\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "tools = [\n",
    "    get_next_ticket,\n",
    "    classify_ticket,\n",
    "    search_knowledge_base,\n",
    "    set_priority,\n",
    "    route_to_team,\n",
    "    draft_response,\n",
    "    mark_complete,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fecfb8",
   "metadata": {},
   "source": [
    "## Baseline: Running Without Compaction\n",
    "\n",
    "Let's start with a realistic customer service scenario: Processing a queue of support tickets. The workflow looks like this:\n",
    "\n",
    "**For Each Ticket:**\n",
    "1. Fetch the ticket using `get_next_ticket()`\n",
    "2. Classify the issue category (billing, technical, account, product, shipping)\n",
    "3. Search the knowledge base for relevant information\n",
    "4. Set appropriate priority (low, medium, high, urgent)\n",
    "5. Route to the correct team\n",
    "6. Draft a customer response\n",
    "7. Mark the ticket complete\n",
    "8. Move to the next ticket\n",
    "\n",
    "**The Challenge**: With 5 tickets in the queue, and each requiring 7 tool calls, Claude will make 35 tool calls. The results from each step including classificationk knowledge base search, and drafted responses accumulate in the conversation history. Without compaction, all this data stays in memory for every ticket, by ticket #5, the context includes complete details from all 4 previous tickets.\n",
    "\n",
    "Let's run this workflow **without compaction** and observe what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "uef86nvtl4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn  1: Input=  1,537 tokens | Output=   61 tokens | Messages= 1 | Cumulative In=   1,537\n",
      "Turn  2: Input=  1,779 tokens | Output=  304 tokens | Messages= 3 | Cumulative In=   3,316\n",
      "Turn  3: Input=  2,360 tokens | Output=  338 tokens | Messages= 5 | Cumulative In=   5,676\n",
      "Turn  4: Input=  3,148 tokens | Output=   59 tokens | Messages= 7 | Cumulative In=   8,824\n",
      "Turn  5: Input=  3,381 tokens | Output=  291 tokens | Messages= 9 | Cumulative In=  12,205\n",
      "Turn  6: Input=  4,283 tokens | Output=  311 tokens | Messages=11 | Cumulative In=  16,488\n",
      "Turn  7: Input=  4,808 tokens | Output=   59 tokens | Messages=13 | Cumulative In=  21,296\n",
      "Turn  8: Input=  5,030 tokens | Output=  283 tokens | Messages=15 | Cumulative In=  26,326\n",
      "Turn  9: Input=  5,916 tokens | Output=  283 tokens | Messages=17 | Cumulative In=  32,242\n",
      "Turn 10: Input=  6,411 tokens | Output=   58 tokens | Messages=19 | Cumulative In=  38,653\n",
      "Turn 11: Input=  6,639 tokens | Output=  283 tokens | Messages=21 | Cumulative In=  45,292\n",
      "Turn 12: Input=  7,385 tokens | Output=  310 tokens | Messages=23 | Cumulative In=  52,677\n",
      "Turn 13: Input=  7,906 tokens | Output=   59 tokens | Messages=25 | Cumulative In=  60,583\n",
      "Turn 14: Input=  8,139 tokens | Output=  299 tokens | Messages=27 | Cumulative In=  68,722\n",
      "Turn 15: Input=  9,049 tokens | Output=  352 tokens | Messages=29 | Cumulative In=  77,771\n",
      "Turn 16: Input=  9,615 tokens | Output=   65 tokens | Messages=31 | Cumulative In=  87,386\n",
      "Turn 17: Input=  9,713 tokens | Output=  424 tokens | Messages=33 | Cumulative In=  97,099\n",
      "\n",
      "============================================================\n",
      "BASELINE RESULTS (NO COMPACTION)\n",
      "============================================================\n",
      "Total turns:   17\n",
      "Input tokens:  97,099\n",
      "Output tokens: 3,839\n",
      "Total tokens:  100,938\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from anthropic.types.beta import BetaMessageParam\n",
    "\n",
    "num_tickets = 5\n",
    "initialize_ticket_queue(num_tickets)\n",
    "\n",
    "messages: list[BetaMessageParam] = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"You are an AI customer service agent. Your task is to process support tickets from a queue.\n",
    "\n",
    "For EACH ticket, you must complete ALL these steps:\n",
    "\n",
    "1. **Fetch ticket**: Call get_next_ticket() to retrieve the next unprocessed ticket\n",
    "2. **Classify**: Call classify_ticket() to categorize the issue (billing/technical/account/product/shipping)\n",
    "3. **Research**: Call search_knowledge_base() to find relevant information for this ticket type\n",
    "4. **Prioritize**: Call set_priority() to assign priority (low/medium/high/urgent) based on severity\n",
    "5. **Route**: Call route_to_team() to assign to the appropriate team\n",
    "6. **Draft**: Call draft_response() to create a helpful customer response using KB information\n",
    "7. **Complete**: Call mark_complete() to finalize this ticket\n",
    "8. **Continue**: Immediately fetch the next ticket and repeat\n",
    "\n",
    "IMPORTANT RULES:\n",
    "- Process tickets ONE AT A TIME in sequence\n",
    "- Complete ALL 7 steps for each ticket before moving to the next\n",
    "- Keep fetching and processing tickets until you get an error that the queue is empty\n",
    "- There are {num_tickets} tickets total - process all of them\n",
    "- Be thorough but efficient\n",
    "\n",
    "Begin by fetching the first ticket.\"\"\",\n",
    "    }\n",
    "]\n",
    "\n",
    "MODEL = \"claude-haiku-4-5\"\n",
    "total_input = 0\n",
    "total_output = 0\n",
    "turn_count = 0\n",
    "\n",
    "runner = client.beta.messages.tool_runner(\n",
    "    model=MODEL,\n",
    "    max_tokens=4096,\n",
    "    tools=tools,\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "for message in runner:\n",
    "    turn_count += 1\n",
    "    total_input += message.usage.input_tokens\n",
    "    total_output += message.usage.output_tokens\n",
    "    print(\n",
    "            f\"Turn {turn_count:2d}: Input={message.usage.input_tokens:7,} tokens | \"\n",
    "            f\"Output={message.usage.output_tokens:5,} tokens | \"\n",
    "            f\"Messages={len(runner._params['messages']):2d} | \"\n",
    "            f\"Cumulative In={total_input:8,}\"\n",
    "        )\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"BASELINE RESULTS (NO COMPACTION)\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"Total turns:   {turn_count}\")\n",
    "print(f\"Input tokens:  {total_input:,}\")\n",
    "print(f\"Output tokens: {total_output:,}\")\n",
    "print(f\"Total tokens:  {total_input + total_output:,}\")\n",
    "print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd25c6f",
   "metadata": {},
   "source": [
    "Now that we have our baseline, we can see how token usage grows as we process more tickets. By ticket #5, the context is bloated with all previous ticket details, leading to high token consumption. Let's preview the final response from Claude after processing all tickets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8b51b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## âœ… All Tickets Processed Successfully!\n",
      "\n",
      "I have successfully completed processing all **5 tickets** in the queue. Here's a summary:\n",
      "\n",
      "### **Ticket Processing Summary:**\n",
      "\n",
      "1. **TICKET-1** - Jane Smith\n",
      "   - **Issue:** Package damaged/missing during shipping\n",
      "   - **Category:** Shipping | **Priority:** High | **Team:** Logistics Team\n",
      "   - **Status:** âœ… Complete\n",
      "\n",
      "2. **TICKET-2** - Chris Davis\n",
      "   - **Issue:** Slow performance after update to v1.8.19\n",
      "   - **Category:** Technical | **Priority:** High | **Team:** Tech Support\n",
      "   - **Status:** âœ… Complete\n",
      "\n",
      "3. **TICKET-3** - Alex Brown\n",
      "   - **Issue:** Feature request for dark mode\n",
      "   - **Category:** Product | **Priority:** Low | **Team:** Product Success\n",
      "   - **Status:** âœ… Complete\n",
      "\n",
      "4. **TICKET-4** - Chris Smith\n",
      "   - **Issue:** Wrong item delivered (received Purple Gadget instead of Blue Widget)\n",
      "   - **Category:** Shipping | **Priority:** High | **Team:** Logistics Team\n",
      "   - **Status:** âœ… Complete\n",
      "\n",
      "5. **TICKET-5** - Alex Williams\n",
      "   - **Issue:** Slow performance after update to v1.6.11\n",
      "   - **Category:** Technical | **Priority:** High | **Team:** Tech Support\n",
      "   - **Status:** âœ… Complete\n",
      "\n",
      "### **Processing Complete:**\n",
      "- âœ… All 5 tickets classified\n",
      "- âœ… All tickets prioritized appropriately\n",
      "- âœ… All tickets routed to correct teams\n",
      "- âœ… Professional draft responses created for each ticket\n",
      "- âœ… All tickets marked as resolved\n",
      "- âœ… Queue is now empty\n",
      "\n",
      "Each ticket has been thoroughly processed with all required steps completed in sequence.\n"
     ]
    }
   ],
   "source": [
    "print(message.content[-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "klok33ohsvn",
   "metadata": {},
   "source": [
    "### Understanding the Problem\n",
    "\n",
    "In the baseline workflow above, Claude had to:\n",
    "- Process **5 support tickets** sequentially\n",
    "- Complete **7 steps per ticket** (fetch, classify, research, prioritize, route, draft, complete)\n",
    "- Make **35 tool calls** with results accumulating in conversation history\n",
    "- Store **every classification, every knowledge base search, every drafted response** in memory\n",
    "\n",
    "**Why This Happens**:\n",
    "1. **Exponential token growth** - With each tool use, the entire conversation history (including all previous tool results) is sent to Claude\n",
    "2. **Context pollution** - Ticket A's classification and drafted response remain in context while processing Ticket B\n",
    "3. **Compounding costs** - By the time you're on Ticket #5, you're sending data from all 4 previous tickets on every API call\n",
    "4. **Slower responses** - Processing massive contexts takes longer\n",
    "5. **Risk of hitting limits** - Eventually you hit the 200k token context window\n",
    "\n",
    "\n",
    "**What We Actually Need**: After completing Ticket A, we only need a **brief summary** (ticket resolved, category, priority) - not the full classification result, knowledge base search, and complete drafted response. The detailed workflow should be discarded, keeping only completion summaries.\n",
    "\n",
    "Let's see how automatic context compaction solves this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "byut5h7hi3",
   "metadata": {},
   "source": [
    "## Enabling Automatic Context Compaction\n",
    "\n",
    "Now let's run the exact same customer service workflow, but with automatic context compaction enabled. We simply add the `compaction_control` parameter to our tool runner.\n",
    "\n",
    "The `compaction_control` parameter has one required field and several optional ones:\n",
    "\n",
    "- **`enabled`** (required): Boolean to turn compaction on/off\n",
    "- **`context_token_threshold`** (optional): Token count that triggers compaction (default: 150,000)\n",
    "- **`model`** (optional): Model to use for summarization (defaults to the main model)\n",
    "- **`summary_prompt`** (optional): Custom prompt for generating summaries\n",
    "\n",
    "For this customer service workflow, we'll use a **5,000 token threshold** - this means after processing several tickets (which generates classifications, KB searches, and responses), compaction will trigger. This allows Claude to:\n",
    "1. **Keep completion summaries** (tickets resolved, categories, outcomes)\n",
    "2. **Discard detailed tool results** (full KB articles, complete classifications, drafted response text)\n",
    "3. **Start fresh** when processing the next batch of tickets\n",
    "\n",
    "This mimics how a real support agent works: resolve the ticket, document it briefly, move to the next case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "x6lnx8d20fr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn  1: Input=  1,537 tokens | Output=   58 tokens | Messages= 1 | Cumulative In=   1,537\n",
      "Turn  2: Input=  1,763 tokens | Output=  268 tokens | Messages= 3 | Cumulative In=   3,300\n",
      "Turn  3: Input=  2,514 tokens | Output=  224 tokens | Messages= 5 | Cumulative In=   5,814\n",
      "Turn  4: Input=  2,787 tokens | Output=   58 tokens | Messages= 7 | Cumulative In=   8,601\n",
      "Turn  5: Input=  2,971 tokens | Output=   57 tokens | Messages= 9 | Cumulative In=  11,572\n",
      "ðŸ”„ Compaction occurred! Messages: 9 â†’ 1\n",
      "Turn  6: Input=  1,670 tokens | Output=  221 tokens | Messages= 1 | Cumulative In=  13,242\n",
      "Turn  7: Input=  2,275 tokens | Output=  331 tokens | Messages= 3 | Cumulative In=  15,517\n",
      "Turn  8: Input=  2,759 tokens | Output=   96 tokens | Messages= 5 | Cumulative In=  18,276\n",
      "Turn  9: Input=  3,184 tokens | Output=  213 tokens | Messages= 7 | Cumulative In=  21,460\n",
      "ðŸ”„ Compaction occurred! Messages: 7 â†’ 1\n",
      "Turn 10: Input=  1,989 tokens | Output=  431 tokens | Messages= 1 | Cumulative In=  23,449\n",
      "Turn 11: Input=  2,786 tokens | Output=   61 tokens | Messages= 3 | Cumulative In=  26,235\n",
      "Turn 12: Input=  3,022 tokens | Output=  242 tokens | Messages= 5 | Cumulative In=  29,257\n",
      "ðŸ”„ Compaction occurred! Messages: 5 â†’ 1\n",
      "Turn 13: Input=  2,241 tokens | Output=  412 tokens | Messages= 1 | Cumulative In=  31,498\n",
      "Turn 14: Input=  2,943 tokens | Output=   76 tokens | Messages= 3 | Cumulative In=  34,441\n",
      "ðŸ”„ Compaction occurred! Messages: 3 â†’ 1\n",
      "Turn 15: Input=  2,499 tokens | Output=  213 tokens | Messages= 1 | Cumulative In=  36,940\n",
      "Turn 16: Input=  2,911 tokens | Output=  409 tokens | Messages= 3 | Cumulative In=  39,851\n",
      "ðŸ”„ Compaction occurred! Messages: 3 â†’ 1\n",
      "Turn 17: Input=  2,486 tokens | Output=  103 tokens | Messages= 1 | Cumulative In=  42,337\n",
      "Turn 18: Input=  2,788 tokens | Output=  316 tokens | Messages= 3 | Cumulative In=  45,125\n",
      "\n",
      "============================================================\n",
      "OPTIMIZED RESULTS (WITH COMPACTION)\n",
      "============================================================\n",
      "Total turns:   18\n",
      "Compactions:   5\n",
      "Input tokens:  45,125\n",
      "Output tokens: 3,789\n",
      "Total tokens:  48,914\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Re-initialize queue and run with compaction\n",
    "initialize_ticket_queue(num_tickets)\n",
    "\n",
    "total_input_compact = 0\n",
    "total_output_compact = 0\n",
    "turn_count_compact = 0\n",
    "compaction_count = 0\n",
    "prev_msg_count = 0\n",
    "\n",
    "runner = client.beta.messages.tool_runner(\n",
    "    model=MODEL,\n",
    "    max_tokens=4096,\n",
    "    tools=tools,\n",
    "    messages=messages,\n",
    "    compaction_control={\n",
    "        \"enabled\": True,\n",
    "        \"context_token_threshold\": 3000,\n",
    "    },\n",
    ")\n",
    "\n",
    "for message in runner:\n",
    "    turn_count_compact += 1\n",
    "    total_input_compact += message.usage.input_tokens\n",
    "    total_output_compact += message.usage.output_tokens\n",
    "    messages_list = list(runner._params[\"messages\"])\n",
    "    curr_msg_count = len(messages_list)\n",
    "\n",
    "    if curr_msg_count < prev_msg_count:\n",
    "        compaction_count += 1\n",
    "        print(f\"ðŸ”„ Compaction occurred! Messages: {prev_msg_count} â†’ {curr_msg_count}\")\n",
    "\n",
    "    prev_msg_count = curr_msg_count\n",
    "    print(\n",
    "            f\"Turn {turn_count_compact:2d}: Input={message.usage.input_tokens:7,} tokens | \"\n",
    "            f\"Output={message.usage.output_tokens:5,} tokens | \"\n",
    "            f\"Messages={len(runner._params['messages']):2d} | \"\n",
    "            f\"Cumulative In={total_input_compact:8,}\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"OPTIMIZED RESULTS (WITH COMPACTION)\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"Total turns:   {turn_count_compact}\")\n",
    "print(f\"Compactions:   {compaction_count}\")\n",
    "print(f\"Input tokens:  {total_input_compact:,}\")\n",
    "print(f\"Output tokens: {total_output_compact:,}\")\n",
    "print(f\"Total tokens:  {total_input_compact + total_output_compact:,}\")\n",
    "print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24dd5c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect! **TICKET-5 has been successfully marked complete.**\n",
      "\n",
      "## Queue Processing Complete âœ“\n",
      "\n",
      "All 5 tickets in the support queue have been processed successfully:\n",
      "\n",
      "| Ticket | Customer | Issue Type | Priority | Team | Status |\n",
      "|--------|----------|-----------|----------|------|--------|\n",
      "| TICKET-1 | John Davis | Billing (Credit card update error) | High | Billing-team | âœ“ Resolved |\n",
      "| TICKET-2 | Morgan Johnson | Account (Email delivery failure) | High | Account-services | âœ“ Resolved |\n",
      "| TICKET-3 | Alex Jones | Product (Dark mode feature request) | Medium | Product-success | âœ“ Resolved |\n",
      "| TICKET-4 | Morgan Brown | Billing (Unexpected charge after cancellation) | High | Billing-team | âœ“ Resolved |\n",
      "| TICKET-5 | Jane Brown | Shipping (Undelivered package) | High | Logistics-team | âœ“ Resolved |\n",
      "\n",
      "**Summary:**\n",
      "- **Total Tickets Processed:** 5/5\n",
      "- **High Priority:** 4 tickets\n",
      "- **Medium Priority:** 1 ticket\n",
      "- **All 7 workflow steps completed for each ticket**\n",
      "\n",
      "The queue is now empty and all tickets have been routed to their appropriate teams with comprehensive responses and clear next steps for resolution.\n"
     ]
    }
   ],
   "source": [
    "print(message.content[-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vb4cesnmb8",
   "metadata": {},
   "source": [
    "### Comparing Results\n",
    "\n",
    "Notice the dramatic difference! With compaction enabled at 5,000 tokens:\n",
    "\n",
    "1. **Context resets after several tickets** - When processing 5-7 tickets generates 5k+ tokens of tool results, the SDK automatically:\n",
    "   - Injects a summary prompt\n",
    "   - Has Claude generate a completion summary wrapped in `<summary></summary>` tags\n",
    "   - Clears the conversation history and discards detailed classifications, KB searches, and responses\n",
    "   - Continues with only the completion summary\n",
    "\n",
    "2. **Input tokens stay bounded** - Instead of accumulating to 100k+ as we process more tickets, input tokens reset after each compaction. When processing Ticket #5, we're NOT carrying the full tool results from Tickets #1-4.\n",
    "\n",
    "3. **Task completes successfully** - The workflow continues smoothly through all tickets without hitting context limits\n",
    "\n",
    "4. **Quality is preserved** - The summaries retain critical information:\n",
    "   - Tickets processed with their IDs\n",
    "   - Categories and priorities assigned\n",
    "   - Teams routed to\n",
    "   - Overall progress status\n",
    "   \n",
    "   All tickets are still properly classified, prioritized, routed, and responded to.\n",
    "\n",
    "5. **Natural workflow** - This mirrors how real support agents work: resolve a ticket, document it briefly in the system, close it, move to the next one. You don't keep every knowledge base article and full response draft open while working on new tickets.\n",
    "\n",
    "Let's visualize the token savings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "z9lvigc94p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TOKEN USAGE COMPARISON\n",
      "======================================================================\n",
      "Metric                         Baseline             With Compaction     \n",
      "----------------------------------------------------------------------\n",
      "Input tokens:                               97,099              45,125\n",
      "Output tokens:                               3,839               3,789\n",
      "Total tokens:                              100,938              48,914\n",
      "Compactions:                                   N/A                   5\n",
      "======================================================================\n",
      "\n",
      "ðŸ’° Token Savings: 52,024 tokens (51.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Compare baseline vs compaction\n",
    "print(\"=\" * 70)\n",
    "print(\"TOKEN USAGE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<30} {'Baseline':<20} {'With Compaction':<20}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Input tokens:':<30} {total_input:>19,} {total_input_compact:>19,}\")\n",
    "print(f\"{'Output tokens:':<30} {total_output:>19,} {total_output_compact:>19,}\")\n",
    "print(\n",
    "    f\"{'Total tokens:':<30} {total_input + total_output:>19,} {total_input_compact + total_output_compact:>19,}\"\n",
    ")\n",
    "print(f\"{'Compactions:':<30} {'N/A':>19} {compaction_count:>19}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate savings\n",
    "token_savings = (total_input + total_output) - (total_input_compact + total_output_compact)\n",
    "savings_percent = (\n",
    "    (token_savings / (total_input + total_output)) * 100 if (total_input + total_output) > 0 else 0\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ’° Token Savings: {token_savings:,} tokens ({savings_percent:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lzvf1mw7o6",
   "metadata": {},
   "source": [
    "## How Compaction Works Under the Hood\n",
    "\n",
    "When the `tool_runner` detects that token usage has exceeded the threshold, it automatically:\n",
    "\n",
    "1. **Pauses the workflow** before making the next API call\n",
    "2. **Injects a summary request** as a user message asking Claude to summarize progress\n",
    "3. **Generates a summary** - Claude produces a summary wrapped in `<summary></summary>` tags containing:\n",
    "   - **Completed tickets**: Brief records of tickets resolved (IDs, categories, priorities, outcomes)\n",
    "   - **Progress status**: How many tickets processed, how many remain\n",
    "   - **Key patterns**: Any notable trends across tickets\n",
    "   - **Next steps**: What to do next (continue processing remaining tickets)\n",
    "4. **Clears history** - The entire conversation history (including all tool results) is replaced with just the summary\n",
    "5. **Resumes processing** - Claude continues working with the compressed context, processing the next batch of tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v64ljd0a79",
   "metadata": {},
   "source": [
    "## Customizing Compaction Configuration\n",
    "\n",
    "You can customize how compaction works to fit your specific use case. Here are the key configuration options:\n",
    "\n",
    "### Adjusting the Threshold\n",
    "\n",
    "The `context_token_threshold` determines when compaction triggers:\n",
    "\n",
    "```python\n",
    "compaction_control={\n",
    "    \"enabled\": True,\n",
    "    \"context_token_threshold\": 5000,  # Compact after processing 5-7 tickets\n",
    "}\n",
    "```\n",
    "\n",
    "**Guidelines:**\n",
    "- **Very low thresholds (5k-20k)**: \n",
    "  - Use for iterative task processing with clear boundaries\n",
    "  - Our customer service workflow uses 5k to compact after processing several tickets\n",
    "  - More frequent compaction, minimal context accumulation\n",
    "  - Best for sequential entity processing\n",
    "  \n",
    "- **Medium thresholds (50k-100k)**: \n",
    "  - Multi-phase workflows with fewer, larger natural checkpoints\n",
    "  - Balance between context retention and management\n",
    "  - Suitable for workflows with expensive tool calls\n",
    "  \n",
    "- **High thresholds (100k-150k)**: \n",
    "  - Tasks requiring substantial historical context\n",
    "  - Less frequent compaction preserves more raw details\n",
    "  - Higher per-call costs but fewer compactions\n",
    "  \n",
    "- **Default (150k)**: Good balance for general long-running tasks\n",
    "\n",
    "**For ticket processing**: The 5k threshold works well because each ticket's workflow generates substantial tool results, but tickets are independent. After resolving Ticket A, you don't need its detailed KB searches when processing Ticket B.\n",
    "\n",
    "### Using a Different Model for Summarization\n",
    "\n",
    "You can use a faster/cheaper model for generating summaries:\n",
    "\n",
    "```python\n",
    "compaction_control={\n",
    "    \"enabled\": True,\n",
    "    \"model\": \"claude-haiku-4-5\",  # Use Haiku for cost-effective summaries\n",
    "}\n",
    "```\n",
    "\n",
    "This is useful when you want to optimize costs - Haiku can generate good summaries much cheaper than Sonnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w4oyvorzkn",
   "metadata": {},
   "source": [
    "### Custom Summary Prompts\n",
    "\n",
    "You can provide a custom prompt to guide how summaries are generated. This is especially useful for customer service workflows where you need to preserve specific types of information.\n",
    "\n",
    "For our workflow, we want to ensure each compaction retains:\n",
    "- **Ticket summaries** for all completed tickets\n",
    "- **Categories and priorities** assigned\n",
    "- **Teams routed to**\n",
    "- **Progress status** (tickets completed, tickets remaining)\n",
    "- **Next steps** in the workflow\n",
    "\n",
    "```python\n",
    "compaction_control={\n",
    "    \"enabled\": True,\n",
    "    \"summary_prompt\": \"\"\"You are processing customer support tickets from a queue.\n",
    "\n",
    "Create a focused summary that preserves:\n",
    "\n",
    "1. **COMPLETED TICKETS**: For each ticket you've fully processed:\n",
    "   - Ticket ID and customer name\n",
    "   - Issue category and priority assigned\n",
    "   - Team routed to\n",
    "   - Brief outcome\n",
    "\n",
    "2. **PROGRESS STATUS**: \n",
    "   - How many tickets you've completed\n",
    "   - Approximately how many remain in the queue\n",
    "\n",
    "3. **NEXT STEPS**: Continue processing the next ticket\n",
    "\n",
    "Format with clear sections and wrap in <summary></summary> tags.\"\"\"\n",
    "}\n",
    "```\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "vdfwgq0ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Custom summaries completed in 12 turns\n",
      "   Each summary maintains structured records while discarding verbose tool results.\n"
     ]
    }
   ],
   "source": [
    "# Re-initialize queue with custom summary prompt\n",
    "initialize_ticket_queue(num_tickets)\n",
    "\n",
    "custom_summary_prompt = \"\"\"You are processing customer support tickets from a queue.\n",
    "\n",
    "Create a focused summary that preserves:\n",
    "\n",
    "1. **COMPLETED TICKETS**: For each ticket you've fully processed:\n",
    "   - Ticket ID and customer name\n",
    "   - Issue category and priority assigned\n",
    "   - Team routed to\n",
    "   - Brief outcome\n",
    "\n",
    "2. **PROGRESS STATUS**:\n",
    "   - How many tickets you've completed\n",
    "   - Approximately how many remain in the queue\n",
    "\n",
    "3. **NEXT STEPS**: Continue processing the next ticket\n",
    "\n",
    "Format with clear sections. Wrap in <summary></summary> tags.\"\"\"\n",
    "\n",
    "runner = client.beta.messages.tool_runner(\n",
    "    model=MODEL,\n",
    "    max_tokens=4096,\n",
    "    tools=tools,\n",
    "    messages=messages,\n",
    "    compaction_control={\n",
    "        \"enabled\": True,\n",
    "        \"context_token_threshold\": 5000,\n",
    "        \"summary_prompt\": custom_summary_prompt,\n",
    "    },\n",
    ")\n",
    "\n",
    "turn_count_custom = 0\n",
    "for message in runner:\n",
    "    turn_count_custom += 1\n",
    "\n",
    "print(f\"\\nâœ… Custom summaries completed in {turn_count_custom} turns\")\n",
    "print(\"   Each summary maintains structured records while discarding verbose tool results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3602dc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## TICKET-4 PROCESSING COMPLETE âœ“\n",
      "\n",
      "### Summary for Jane Brown\n",
      "- **Issue**: Unable to update payment method - receives error message; card expiring next month\n",
      "- **Category**: Billing\n",
      "- **Priority**: HIGH (time-sensitive - card expiring soon)\n",
      "- **Team Routed**: Billing-Team\n",
      "- **Outcome**: \n",
      "  - High-priority escalation with thorough troubleshooting steps (browser cache, different browser/device, card verification)\n",
      "  - Comprehensive response covering common payment update issues\n",
      "  - Billing team will investigate and potentially process manual update if needed\n",
      "  - 24-hour resolution SLA set\n",
      "\n",
      "---\n",
      "\n",
      "## UPDATED PROGRESS STATUS\n",
      "- **Tickets Completed**: 3 of 5 (60%) âœ“\n",
      "- **Tickets Remaining**: 2 in queue\n",
      "\n",
      "---\n",
      "\n",
      "## NEXT STEPS\n",
      "Ready to retrieve and process the next ticket from the queue.\n"
     ]
    }
   ],
   "source": [
    "print(message.content[-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7emc6ezm9",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "When using automatic context compaction, follow these guidelines:\n",
    "\n",
    "### 1. Choose the Right Threshold\n",
    "\n",
    "- **Start with the default (150k)** for most workflows\n",
    "- **Lower thresholds (5k-20k)** for:\n",
    "  - **Iterative processing** like our ticket workflow (compact after processing several items)\n",
    "  - **Multi-phase workflows** with natural checkpoints\n",
    "  - Very long-running tasks that need many iterations\n",
    "  - Cost-sensitive applications\n",
    "  - Tasks where older context becomes less relevant\n",
    "- **Higher thresholds (100k-150k)** for:\n",
    "  - Tasks requiring detailed historical context across all phases\n",
    "  - Complex reasoning that builds on previous steps\n",
    "  - Workflows with expensive tool calls where you want to minimize repetition\n",
    "\n",
    "**For ticket processing**: A threshold of 5k-10k works well because each ticket workflow is self-contained. After resolving Ticket A, you don't need its detailed tool results when processing Ticket B.\n",
    "\n",
    "### 2. Craft Effective Summary Prompts\n",
    "\n",
    "Good summary prompts should specify:\n",
    "- **What to preserve**: Critical information, IDs, categories, outcomes, progress\n",
    "- **Format**: Structured output helps maintain consistency\n",
    "- **Context**: Remind Claude of the task and current phase\n",
    "- **Next steps**: What remains to be done\n",
    "\n",
    "Example for ticket processing:\n",
    "```python\n",
    "summary_prompt = \"\"\"You are processing customer support tickets.\n",
    "\n",
    "Include in your summary:\n",
    "- COMPLETED: Brief record of each ticket (ID, category, priority, team, outcome)\n",
    "- PROGRESS: Tickets completed vs remaining\n",
    "- NEXT: Continue processing remaining tickets\n",
    "\n",
    "Wrap in <summary></summary> tags.\"\"\"\n",
    "```\n",
    "\n",
    "### 3. Use Appropriate Models\n",
    "\n",
    "- **Main task**: Use the model best suited for the task (Sonnet, Haiku)\n",
    "- **Summarization**: Consider using Haiku for cost savings if summaries don't require complex reasoning\n",
    "\n",
    "```python\n",
    "compaction_control={\n",
    "    \"enabled\": True,\n",
    "    \"model\": \"claude-haiku-4-5\",  # Cheaper summaries\n",
    "}\n",
    "```\n",
    "\n",
    "### 4. Monitor Compaction Behavior\n",
    "\n",
    "Track when compaction occurs to optimize your threshold:\n",
    "\n",
    "```python\n",
    "messages_list = list(runner._params[\"messages\"])\n",
    "curr_msg_count = len(messages_list)\n",
    "\n",
    "if curr_msg_count < prev_msg_count:\n",
    "    print(f\"Compaction at turn {turn_count}\")\n",
    "    \n",
    "prev_msg_count = curr_msg_count\n",
    "```\n",
    "\n",
    "In our ticket workflow, you should see compaction trigger after processing several tickets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71dwo1dayp",
   "metadata": {},
   "source": [
    "## Limitations and Considerations\n",
    "\n",
    "While automatic context compaction is powerful, there are important limitations to understand:\n",
    "\n",
    "### âš ï¸ Server-Side Sampling Loops\n",
    "\n",
    "**Current Limitation**: Compaction does not work optimally with server-side sampling loops, such as server-side web search tools.\n",
    "\n",
    "**Why**: Cache tokens accumulate across sampling loops, which can trigger compaction prematurely based on cached content rather than actual conversation history.\n",
    "\n",
    "**Workaround**: This feature works best with:\n",
    "- âœ… Client-side tools (like the customer service API in this cookbook)\n",
    "- âœ… Standard agentic workflows with regular tool use\n",
    "- âœ… File operations, database queries, API calls\n",
    "- âŒ Server-side Extended Thinking\n",
    "- âŒ Server-side web search tools\n",
    "\n",
    "### Information Loss\n",
    "\n",
    "**Trade-off**: Summaries inherently lose some information. While Claude is good at identifying key points, some details will be compressed or omitted.\n",
    "\n",
    "**In ticket processing**: \n",
    "- âœ… **Retained**: Ticket IDs, categories, priorities, teams, outcomes, progress status\n",
    "- âŒ **Lost**: Full knowledge base article text, complete drafted response text, detailed classification reasoning\n",
    "\n",
    "This is usually acceptableâ€”you don't need every KB article and full response text in perpetuity, just the completion records.\n",
    "\n",
    "**Mitigation**:\n",
    "- Use custom summary prompts to preserve critical information\n",
    "- Set higher thresholds for tasks requiring extensive historical context\n",
    "- Structure your tasks to be modular (each phase builds on summaries, not raw details)\n",
    "\n",
    "### When NOT to Use Compaction\n",
    "\n",
    "Avoid compaction for:\n",
    "\n",
    "1. **Short tasks**: If your task completes within 50k-100k tokens, compaction adds unnecessary overhead\n",
    "2. **Tasks requiring full audit trails**: Some tasks need access to ALL previous details\n",
    "3. **Server-side sampling workflows**: As mentioned above, wait for this limitation to be addressed\n",
    "4. **Highly iterative refinement**: Tasks where each step critically depends on exact details from all previous steps\n",
    "\n",
    "### When TO Use Compaction\n",
    "\n",
    "Compaction is ideal for:\n",
    "\n",
    "1. **Sequential processing**: Like our ticket workflowâ€”process multiple items one after another\n",
    "2. **Multi-phase workflows**: Where each phase can summarize progress before moving on\n",
    "3. **Iterative data processing**: Processing large datasets in chunks or entities one at a time\n",
    "4. **Extended analysis sessions**: Analyzing data across many entities\n",
    "5. **Batch operations**: Processing hundreds of items where each is independent\n",
    "\n",
    "**Ticket processing is a perfect use case** because:\n",
    "- Each ticket workflow is largely independent\n",
    "- You need completion summaries, not full tool results\n",
    "- Natural compaction points exist (after completing several tickets)\n",
    "- The workflow is iterative and sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4pz1jmdidi",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Automatic context compaction is a powerful feature that enables long-running agentic workflows to exceed typical context limits. In this cookbook, we've explored compaction through a customer service ticket processing workflow.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **The Problem**: Without compaction, iterative workflows face severe challenges:\n",
    "   - **Context accumulation**: Processing Ticket #10 means carrying tool results from Tickets #1-9\n",
    "   - **Exponential token growth**: Each new ticket adds to an ever-growing context\n",
    "   - **High costs**: Thousands of unnecessary tokens sent with each API call\n",
    "   - **Context limits**: Risk of hitting 200k token limit before completing all tickets\n",
    "\n",
    "2. **The Solution**: The `compaction_control` parameter automates context management:\n",
    "   ```python\n",
    "   compaction_control={\n",
    "       \"enabled\": True,\n",
    "       \"context_token_threshold\": 5000,  # Compact after processing several tickets\n",
    "       \"model\": \"claude-haiku-4-5\",      # Optional, defaults to main model\n",
    "       \"summary_prompt\": \"...\"            # Optional, customize for your use case\n",
    "   }\n",
    "   ```\n",
    "\n",
    "3. **How It Works**: When tokens exceed the threshold, the SDK:\n",
    "   - Pauses the workflow\n",
    "   - Generates a summary of progress and completed tickets\n",
    "   - Clears the conversation history (discarding detailed tool results)\n",
    "   - Continues seamlessly with compressed context (retaining completion summaries)\n",
    "\n",
    "4. **Real Impact**: In our ticket processing example, compaction enabled:\n",
    "   - Processing all 10 tickets with 70+ tool calls\n",
    "   - Managing classifications, KB searches, and drafted responses for each ticket\n",
    "   - **Significant token savings** (30-60%+ reduction in total tokens)\n",
    "   - **Successful completion** without hitting context limits\n",
    "   - **Quality preserved**: All tickets properly classified, prioritized, routed, and resolved\n",
    "\n",
    "5. **Natural Workflow**: Compaction mirrors how real support agents work:\n",
    "   - Process Ticket A thoroughly â†’ Document completion briefly\n",
    "   - Close that ticket and clear your workspace\n",
    "   - Move to Ticket B with a clean slate\n",
    "   - At the end, all tickets are documented in the system\n",
    "   \n",
    "   You don't keep every knowledge base article and full response draft open while working on new tickets.\n",
    "\n",
    "### When to Use Compaction\n",
    "\n",
    "- âœ… Sequential processing (customer service, data processing)\n",
    "- âœ… Iterative workflows (process hundreds of items)\n",
    "- âœ… Multi-phase workflows with natural checkpoints\n",
    "- âœ… Tasks processing large volumes of tool results\n",
    "- âœ… Cost-sensitive production applications\n",
    "- âŒ Tasks requiring full, uncompressed conversation history\n",
    "- âŒ Server-side sampling loop workflows (current limitation)\n",
    "\n",
    "### Key Configuration Guidelines\n",
    "\n",
    "- **5k-20k threshold**: For iterative processing with natural per-item compaction points\n",
    "- **50k-100k threshold**: For multi-phase workflows with fewer, larger compaction points\n",
    "- **150k threshold (default)**: For general long-running tasks\n",
    "- **Custom summary prompts**: Essential for preserving workflow-specific information\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Try implementing compaction in your own workflows:\n",
    "1. Identify natural compaction points (after processing each item, completing each phase, etc.)\n",
    "2. Start with an aggressive threshold (5k-10k) if you have clear per-item boundaries\n",
    "3. Use custom summary prompts to preserve critical information\n",
    "4. Monitor when compaction triggers and verify quality is maintained\n",
    "5. Adjust threshold based on your specific needs\n",
    "\n",
    "For more on effective context management, see [Effective Context Engineering for AI Agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anthropic-cookbook (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
