{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Editing & Memory for Long-Running Agents\n",
    "\n",
    "Learn how to build AI agents that learn and improve across conversations using Claude's memory tool and context editing capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction: Why Memory Matters](#introduction)\n",
    "2. [Use Cases](#use-cases)\n",
    "3. [Quick Start Examples](#quick-start)\n",
    "4. [How It Works](#how-it-works)\n",
    "5. [Code Review Assistant Demo](#demo)\n",
    "6. [Real-World Applications](#real-world)\n",
    "7. [Best Practices](#best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### For VSCode Users\n",
    "\n",
    "```bash\n",
    "# 1. Create virtual environment\n",
    "python -m venv .venv\n",
    "\n",
    "# 2. Activate it\n",
    "source .venv/bin/activate  # macOS/Linux\n",
    "# or: .venv\\Scripts\\activate  # Windows\n",
    "\n",
    "# 3. Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 4. In VSCode: Select .venv as kernel (top right)\n",
    "```\n",
    "\n",
    "### API Key\n",
    "\n",
    "```bash\n",
    "cp .env.example .env\n",
    "# Edit .env and add your ANTHROPIC_API_KEY\n",
    "```\n",
    "\n",
    "Get your API key from: https://console.anthropic.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: Why Memory Matters {#introduction}\n",
    "\n",
    "This cookbook demonstrates practical implementations of the context engineering patterns described in [Effective context engineering for AI agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents). That post covers why context is a finite resource, how attention budgets work, and strategies for building effective agents‚Äîthe techniques you'll see in action here.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Large language models have finite context windows (200k tokens for Claude 4). While this seems large, several challenges emerge:\n",
    "\n",
    "- **Context limits**: Long conversations or complex tasks can exceed available context\n",
    "- **Computational cost**: Processing large contexts is expensive - attention mechanisms scale quadratically\n",
    "- **Repeated patterns**: Similar tasks across conversations require re-explaining context every time\n",
    "- **Information loss**: When context fills up, earlier important information gets lost\n",
    "\n",
    "### The Solution\n",
    "\n",
    "Claude 4 models introduce powerful context management capabilities:\n",
    "\n",
    "1. **Memory Tool** (`memory_20250818`): Enables cross-conversation learning\n",
    "   - Claude can write down what it learns for future reference\n",
    "   - File-based system under `/memories` directory\n",
    "   - Client-side implementation gives you full control\n",
    "\n",
    "2. **Context Editing**: Automatically manages context with two strategies:\n",
    "   - **Tool use clearing** (`clear_tool_uses_20250919`): Clears old tool results when context grows large\n",
    "   - **Thinking management** (`clear_thinking_20251015`): Manages extended thinking blocks (requires thinking enabled)\n",
    "   - Configurable triggers and retention policies\n",
    "\n",
    "### The Benefit\n",
    "\n",
    "Build AI agents that **get better at your specific tasks over time**:\n",
    "\n",
    "- **Session 1**: Claude solves a problem, writes down the pattern\n",
    "- **Session 2**: Claude applies the learned pattern immediately (faster!)\n",
    "- **Long sessions**: Context editing keeps conversations manageable\n",
    "\n",
    "Think of it as giving Claude a notebook to take notes and refer back to - just like humans do.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "By the end of this cookbook, you will be able to:\n",
    "- **Implement** the memory tool for cross-conversation learning\n",
    "- **Configure** context editing to manage long-running sessions\n",
    "- **Apply** best practices for memory security and organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use Cases {#use-cases}\n",
    "\n",
    "Memory and context management enable powerful new workflows:\n",
    "\n",
    "### üîç Code Review Assistant\n",
    "- Learns debugging patterns from past reviews\n",
    "- Recognizes similar bugs instantly in future sessions\n",
    "- Builds team-specific code quality knowledge\n",
    "- **Production ready**: Integrate with [claude-code-action](https://github.com/anthropics/claude-code-action) for GitHub PR reviews\n",
    "\n",
    "### üìö Research Assistant\n",
    "- Accumulates knowledge on topics over multiple sessions\n",
    "- Connects insights across different research threads\n",
    "- Maintains bibliography and source tracking\n",
    "\n",
    "### üí¨ Customer Support Bot\n",
    "- Learns user preferences and communication style\n",
    "- Remembers common issues and solutions\n",
    "- Builds product knowledge base from interactions\n",
    "\n",
    "### üìä Data Analysis Helper\n",
    "- Remembers dataset patterns and anomalies\n",
    "- Stores analysis techniques that work well\n",
    "- Builds domain-specific insights over time\n",
    "\n",
    "**Supported Models**: Claude Opus 4.1 (`claude-opus-4-1`), Claude Opus 4 (`claude-opus-4`), Claude Sonnet 4.5 (`claude-sonnet-4-5`), Claude Sonnet 4 (`claude-sonnet-4`), and Claude Haiku 4.5 (`claude-haiku-4-5`)\n",
    "\n",
    "**This cookbook focuses on the Code Review Assistant** as it clearly demonstrates both memory (learning patterns) and context editing (handling long reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick Start Examples {#quick-start}\n",
    "\n",
    "Let's see memory and context management in action with simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, install dependencies and configure your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:40:46.546299Z",
     "iopub.status.busy": "2025-11-15T16:40:46.546237Z",
     "iopub.status.idle": "2025-11-15T16:40:50.344720Z",
     "shell.execute_reply": "2025-11-15T16:40:50.344353Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages\n",
    "# Option 1: From requirements.txt\n",
    "# %pip install -q -r requirements.txt\n",
    "\n",
    "# Option 2: Direct install\n",
    "%pip install -q anthropic python-dotenv ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ö†Ô∏è Important**: Create a `.env` file in this directory:\n",
    "\n",
    "```bash\n",
    "# Copy .env.example to .env and add your API key\n",
    "cp .env.example .env\n",
    "```\n",
    "\n",
    "Then edit `.env` to add your Anthropic API key from https://console.anthropic.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:40:50.346471Z",
     "iopub.status.busy": "2025-11-15T16:40:50.346354Z",
     "iopub.status.idle": "2025-11-15T16:40:50.613576Z",
     "shell.execute_reply": "2025-11-15T16:40:50.613333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì API key loaded\n",
      "‚úì Using model: claude-sonnet-4-5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any, cast\n",
    "\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Model configuration - use alias for automatic updates\n",
    "MODEL = \"claude-sonnet-4-5\"  # Can override via ANTHROPIC_MODEL env var\n",
    "if os.getenv(\"ANTHROPIC_MODEL\"):\n",
    "    MODEL = os.getenv(\"ANTHROPIC_MODEL\")\n",
    "\n",
    "API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\n",
    "        \"ANTHROPIC_API_KEY not found. \"\n",
    "        \"Copy .env.example to .env and add your API key.\"\n",
    "    )\n",
    "\n",
    "client = Anthropic(api_key=API_KEY)\n",
    "\n",
    "print(\"‚úì API key loaded\")\n",
    "print(f\"‚úì Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Basic Memory Usage\n",
    "\n",
    "Let's see Claude use memory to store information for future reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions**\n",
    "\n",
    "These examples use helper functions from `demo_helpers.py`:\n",
    "\n",
    "- **`run_conversation_loop()`**: Handles the API conversation loop\n",
    "  - Calls Claude's API with memory tool enabled\n",
    "  - Executes tool uses (memory operations)\n",
    "  - Continues until Claude stops using tools\n",
    "  - Returns the final response\n",
    "\n",
    "- **`run_conversation_turn()`**: Single turn (used in Example 3)\n",
    "  - Same as above but returns after one API call\n",
    "  - Useful when you need fine-grained control\n",
    "\n",
    "- **`print_context_management_info()`**: Displays context clearing stats\n",
    "  - Shows tokens saved, tool uses cleared\n",
    "  - Helps visualize when context editing triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ö†Ô∏è Note on Memory Clearing**\n",
    "\n",
    "The following cell clears all memory files to provide a clean slate for this demonstration. This is useful for running the notebook multiple times to see consistent results.\n",
    "\n",
    "**In production applications**, you should carefully consider whether to clear all memory, as it permanently removes learned patterns. Consider using selective deletion or organizing memory into project-specific directories instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:40:50.628009Z",
     "iopub.status.busy": "2025-11-15T16:40:50.627894Z",
     "iopub.status.idle": "2025-11-15T16:41:26.556663Z",
     "shell.execute_reply": "2025-11-15T16:41:26.556414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Clearing previous memories...\n",
      "‚úì Memory cleared\n",
      "\n",
      "============================================================\n",
      "üìù SESSION 1: Learning from a bug\n",
      "============================================================\n",
      "\n",
      "üîÑ Turn 1:\n",
      "üí¨ Claude: I'll review this code for the race condition issues. Let me start by checking my memory.\n",
      "\n",
      "  üîß Memory tool: view /memories\n",
      "  ‚úì Result: Directory: /memories\n",
      "(empty)\n",
      "\n",
      "üîÑ Turn 2:\n",
      "  üîß Memory tool: create /memories/review_progress.md\n",
      "  ‚úì Result: File created successfully at /memories/review_progress.md\n",
      "\n",
      "üîÑ Turn 3:\n",
      "üí¨ Claude: ## Code Review: Multi-threaded Web Scraper\n",
      "\n",
      "### **Critical Issues Found** üî¥\n",
      "\n",
      "I've identified the race condition bugs causing inconsistent results. Here's my detailed analysis:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Primary Race Condition: Unsafe List Operations** \n",
      "**Severity: CRITICAL**\n",
      "\n",
      "**Location:** Lines in `scrape_urls()` method\n",
      "```python\n",
      "self.results.append(result)  # RACE CONDITION\n",
      "self.failed_urls.append(result[\"url\"])  # RACE CONDITION\n",
      "```\n",
      "\n",
      "**Problem:**\n",
      "- Python's `list.append()` is **NOT thread-safe** despite the GIL (Global Interpreter Lock)\n",
      "- While the GIL prevents true parallelism, it doesn't guarantee atomicity of operations\n",
      "- `append()` involves multiple steps: checking size, resizing (if needed), inserting element\n",
      "- When multiple threads call `append()` simultaneously, these operations can interleave, causing:\n",
      "  - **Lost updates** (most common - items disappear)\n",
      "  - **Data corruption** (less common but possible)\n",
      "  - **Inconsistent counts**\n",
      "\n",
      "**Why Results Vary Between Runs:**\n",
      "- Thread scheduling is non-deterministic\n",
      "- Race conditions manifest probabilistically based on timing\n",
      "- With 10 workers and 50 URLs, collision probability is high\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Additional Issues**\n",
      "\n",
      "#### **a) Instance-Level State Reuse**\n",
      "```python\n",
      "self.results = []  # Accumulates across multiple calls\n",
      "```\n",
      "- If `scrape_urls()` is called multiple times, results accumulate\n",
      "- Old results pollute new scrapes\n",
      "- Stats become inaccurate\n",
      "\n",
      "#### **b) Potential Division by Zero** (Minor)\n",
      "The stats calculation handles this, but it's convoluted.\n",
      "\n",
      "---\n",
      "\n",
      "### **Solutions** ‚úÖ\n",
      "\n",
      "#### **Option 1: Use Thread-Safe Data Structures with Locks** (Recommended for this pattern)\n",
      "```python\n",
      "from threading import Lock\n",
      "\n",
      "class WebScraper:\n",
      "    def __init__(self, max_workers: int = 10):\n",
      "        self.max_workers = max_workers\n",
      "        self.results = []\n",
      "        self.failed_urls = []\n",
      "        self.results_lock = Lock()  # Add lock for synchronization\n",
      "        self.failed_lock = Lock()   # Separate lock for failed URLs\n",
      "\n",
      "    def scrape_urls(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
      "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
      "            futures = [executor.submit(self.fetch_url, url) for url in urls]\n",
      "\n",
      "            for future in as_completed(futures):\n",
      "                result = future.result()\n",
      "\n",
      "                if \"error\" in result:\n",
      "                    with self.failed_lock:  # Acquire lock before modifying\n",
      "                        self.failed_urls.append(result[\"url\"])\n",
      "                else:\n",
      "                    with self.results_lock:  # Acquire lock before modifying\n",
      "                        self.results.append(result)\n",
      "\n",
      "        return self.results\n",
      "```\n",
      "\n",
      "#### **Option 2: Use Queue (Better for Producer-Consumer Pattern)**\n",
      "```python\n",
      "from queue import Queue\n",
      "\n",
      "class WebScraper:\n",
      "    def __init__(self, max_workers: int = 10):\n",
      "        self.max_workers = max_workers\n",
      "        self.results_queue = Queue()  # Thread-safe by design\n",
      "        self.failed_queue = Queue()\n",
      "\n",
      "    def scrape_urls(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
      "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
      "            futures = [executor.submit(self.fetch_url, url) for url in urls]\n",
      "\n",
      "            for future in as_completed(futures):\n",
      "                result = future.result()\n",
      "\n",
      "                if \"error\" in result:\n",
      "                    self.failed_queue.put(result[\"url\"])\n",
      "                else:\n",
      "                    self.results_queue.put(result)\n",
      "\n",
      "        # Convert queue to list for return\n",
      "        results = []\n",
      "        while not self.results_queue.empty():\n",
      "            results.append(self.results_queue.get())\n",
      "        \n",
      "        return results\n",
      "```\n",
      "\n",
      "#### **Option 3: Collect Results from Futures (Cleanest)**\n",
      "```python\n",
      "def scrape_urls(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
      "    \"\"\"No shared state - collect results after completion.\"\"\"\n",
      "    results = []\n",
      "    failed_urls = []\n",
      "    \n",
      "    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
      "        futures = [executor.submit(self.fetch_url, url) for url in urls]\n",
      "\n",
      "        for future in as_completed(futures):\n",
      "            result = future.result()\n",
      "\n",
      "            if \"error\" in result:\n",
      "                failed_urls.append(result[\"url\"])\n",
      "            else:\n",
      "                results.append(result)\n",
      "\n",
      "    # Update instance state after all threads complete\n",
      "    self.results = results\n",
      "    self.failed_urls = failed_urls\n",
      "    \n",
      "    return results\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Recommended Fix** (Option 3 - Simplest and Most Robust)\n",
      "\n",
      "```python\n",
      "\"\"\"\n",
      "Fixed concurrent web scraper - no race conditions.\n",
      "\"\"\"\n",
      "\n",
      "import time\n",
      "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
      "from typing import List, Dict, Any\n",
      "\n",
      "import requests\n",
      "\n",
      "\n",
      "class WebScraper:\n",
      "    \"\"\"Web scraper that fetches multiple URLs concurrently.\"\"\"\n",
      "\n",
      "    def __init__(self, max_workers: int = 10):\n",
      "        self.max_workers = max_workers\n",
      "        self.results = []\n",
      "        self.failed_urls = []\n",
      "\n",
      "    def fetch_url(self, url: str) -> Dict[str, Any]:\n",
      "        \"\"\"Fetch a single URL and return the result.\"\"\"\n",
      "        try:\n",
      "            response = requests.get(url, timeout=5)\n",
      "            response.raise_for_status()\n",
      "            return {\n",
      "                \"url\": url,\n",
      "                \"status\": response.status_code,\n",
      "                \"content_length\": len(response.content),\n",
      "            }\n",
      "        except requests.exceptions.RequestException as e:\n",
      "            return {\"url\": url, \"error\": str(e)}\n",
      "\n",
      "    def scrape_urls(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
      "        \"\"\"\n",
      "        Scrape multiple URLs concurrently.\n",
      "        \n",
      "        FIXED: Results are collected locally, then assigned to instance\n",
      "        variables after all threads complete. No race conditions.\n",
      "        \"\"\"\n",
      "        results = []  # Local variable - each call gets its own\n",
      "        failed_urls = []  # Local variable - thread-safe\n",
      "        \n",
      "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
      "            futures = [executor.submit(self.fetch_url, url) for url in urls]\n",
      "\n",
      "            for future in as_completed(futures):\n",
      "                result = future.result()\n",
      "\n",
      "                # Safe: Only main thread modifies these local lists\n",
      "                if \"error\" in result:\n",
      "                    failed_urls.append(result[\"url\"])\n",
      "                else:\n",
      "                    results.append(result)\n",
      "\n",
      "        # Update instance state after threads complete\n",
      "        self.results = results\n",
      "        self.failed_urls = failed_urls\n",
      "        \n",
      "        return results\n",
      "\n",
      "    def get_stats(self) -> Dict[str, int]:\n",
      "        \"\"\"Get scraping statistics.\"\"\"\n",
      "        total = len(self.results) + len(self.failed_urls)\n",
      "        return {\n",
      "            \"total_results\": len(self.results),\n",
      "            \"failed_urls\": len(self.failed_urls),\n",
      "            \"success_rate\": len(self.results) / total if total > 0 else 0.0,\n",
      "        }\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    urls = [\n",
      "        \"https://httpbin.org/delay/1\",\n",
      "        \"https://httpbin.org/status/200\",\n",
      "        \"https://httpbin.org/status/404\",\n",
      "        \"https://httpbin.org/delay/2\",\n",
      "        \"https://httpbin.org/status/500\",\n",
      "    ] * 10  # 50 URLs total\n",
      "\n",
      "    scraper = WebScraper(max_workers=10)\n",
      "    results = scraper.scrape_urls(urls)\n",
      "\n",
      "    print(f\"Expected: 50 results\")\n",
      "    print(f\"Got: {len(results)} results\")  # Now always 50!\n",
      "    print(f\"Stats: {scraper.get_stats()}\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Takeaways**\n",
      "\n",
      "1. **Python lists are NOT thread-safe** - always synchronize access from multiple threads\n",
      "2. **GIL ‚â† Thread Safety** - The GIL prevents parallel execution but not race conditions\n",
      "3. **Best practice**: Avoid shared mutable state in concurrent code when possible\n",
      "4. **Alternative solutions\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚úÖ Session 1 complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Import helper functions\n",
    "from memory_demo.demo_helpers import run_conversation_loop, run_conversation_turn, print_context_management_info\n",
    "from memory_tool import MemoryToolHandler\n",
    "\n",
    "# Initialize\n",
    "client = Anthropic(api_key=API_KEY)\n",
    "memory = MemoryToolHandler(base_path=\"./demo_memory\")\n",
    "\n",
    "# Clear any existing memories to start fresh\n",
    "print(\"üßπ Clearing previous memories...\")\n",
    "memory.clear_all_memory()\n",
    "print(\"‚úì Memory cleared\\n\")\n",
    "\n",
    "# Load example code with a race condition bug\n",
    "with open(\"memory_demo/sample_code/web_scraper_v1.py\", \"r\") as f:\n",
    "    code_to_review = f.read()\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"I'm reviewing a multi-threaded web scraper that sometimes returns fewer results than expected. The count is inconsistent across runs. Can you find the issue?\\n\\n```python\\n{code_to_review}\\n```\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìù SESSION 1: Learning from a bug\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run conversation loop\n",
    "response = run_conversation_loop(\n",
    "    client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    memory_handler=memory,\n",
    "    system=\"You are a code reviewer.\",\n",
    "    max_tokens=2048,\n",
    "    max_turns=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Session 1 complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened?**\n",
    "\n",
    "1. Claude checked its memory (empty on first run)\n",
    "2. Identified the bug: **race condition** - multiple threads modifying shared state (`self.results` and `self.failed_urls`) without synchronization\n",
    "3. Stored the concurrency pattern in memory for future reference\n",
    "\n",
    "Now let's see the magic - Claude applying this learned pattern in a **new conversation**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Cross-Conversation Learning\n",
    "\n",
    "Start a completely new conversation - memory persists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:41:26.558064Z",
     "iopub.status.busy": "2025-11-15T16:41:26.557991Z",
     "iopub.status.idle": "2025-11-15T16:42:16.178791Z",
     "shell.execute_reply": "2025-11-15T16:42:16.178572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ SESSION 2: Applying learned pattern\n",
      "============================================================\n",
      "\n",
      "üîÑ Turn 1:\n",
      "  üîß Memory tool: view /memories\n",
      "  ‚úì Result: Directory: /memories\n",
      "- review_progress.md\n",
      "\n",
      "üîÑ Turn 2:\n",
      "  üîß Memory tool: view /memories/review_progress.md\n",
      "  ‚úì Result:    1: # Code Review: Multi-threaded Web Scraper\n",
      "   2: \n",
      "   3: ## Task\n",
      "   4: Revie...\n",
      "\n",
      "üîÑ Turn 3:\n",
      "  üîß Memory tool: str_replace /memories/review_progress.md\n",
      "  ‚úì Result: File /memories/review_progress.md has been edited successfully\n",
      "\n",
      "üîÑ Turn 4:\n",
      "üí¨ Claude: Now let me provide a comprehensive code review:\n",
      "\n",
      "## Code Review: AsyncAPIClient\n",
      "\n",
      "### üî¥ **Critical Issues**\n",
      "\n",
      "#### 1. **Race Conditions on Shared State** (High Severity)\n",
      "**Location**: Lines 34-41 in `fetch_all()` method\n",
      "\n",
      "```python\n",
      "# PROBLEM CODE:\n",
      "if \"error\" in result:\n",
      "    self.error_count += 1  # Not atomic!\n",
      "else:\n",
      "    self.responses.append(result)  # Not thread-safe in async context!\n",
      "```\n",
      "\n",
      "**Issue**: While Python's GIL prevents true concurrent execution in threads, **async code CAN still have race conditions** because:\n",
      "- Coroutines can be suspended at `await` points\n",
      "- Multiple coroutines interleave execution\n",
      "- Operations like `+=` and `append()` are **not atomic** even in async contexts\n",
      "- Between checking and modifying state, another coroutine can execute\n",
      "\n",
      "**Impact**: \n",
      "- Inconsistent counts (results may be lost)\n",
      "- `self.responses` list may have incorrect length\n",
      "- `self.error_count` may be incorrect\n",
      "- Summary statistics will be unreliable\n",
      "\n",
      "---\n",
      "\n",
      "#### 2. **Instance Variables as Accumulators** (Design Flaw)\n",
      "**Location**: Lines 13-14\n",
      "\n",
      "```python\n",
      "def __init__(self, base_url: str):\n",
      "    self.base_url = base_url\n",
      "    self.responses = []  # BUG: Shared state\n",
      "    self.error_count = 0  # BUG: Race condition\n",
      "```\n",
      "\n",
      "**Issue**: Using instance variables for accumulating results makes the client:\n",
      "- **Not reusable**: Can't call `fetch_all()` multiple times without reset\n",
      "- **Not safe**: State persists between calls, causing accumulation\n",
      "- **Not thread-safe**: If used in multi-threaded context\n",
      "\n",
      "---\n",
      "\n",
      "### üü° **Medium Severity Issues**\n",
      "\n",
      "#### 3. **Misleading Comments**\n",
      "**Location**: Lines 28-29\n",
      "\n",
      "```python\n",
      "# BUG: Similar to the threading issue, multiple coroutines\n",
      "# modify self.responses and self.error_count without coordination!\n",
      "```\n",
      "\n",
      "**Issue**: The comment says \"similar to threading\" but the dynamics are different:\n",
      "- Async: Single-threaded, cooperative multitasking, context switches at `await`\n",
      "- Threading: Multiple OS threads, preemptive multitasking, switches anytime\n",
      "- **However**, both can have race conditions!\n",
      "\n",
      "---\n",
      "\n",
      "#### 4. **Missing Error Results**\n",
      "**Location**: Lines 38-41\n",
      "\n",
      "```python\n",
      "if \"error\" in result:\n",
      "    self.error_count += 1  # Not atomic!\n",
      "else:\n",
      "    self.responses.append(result)  # Only success responses stored!\n",
      "```\n",
      "\n",
      "**Issue**: Errors are counted but **not stored**, making it impossible to:\n",
      "- Know which endpoints failed\n",
      "- Retry failed requests\n",
      "- Debug specific errors\n",
      "\n",
      "---\n",
      "\n",
      "### üü¢ **Minor Issues**\n",
      "\n",
      "#### 5. **No Retry Logic**\n",
      "The client has no retry mechanism for failed requests, which is common in production API clients.\n",
      "\n",
      "#### 6. **Hardcoded Timeout**\n",
      "The 5-second timeout is hardcoded and not configurable.\n",
      "\n",
      "---\n",
      "\n",
      "## üõ†Ô∏è **Recommended Fixes**\n",
      "\n",
      "### **Solution 1: Return Results Directly (Simplest)**\n",
      "\n",
      "```python\n",
      "async def fetch_all(self, endpoints: List[str]) -> List[Dict[str, Any]]:\n",
      "    \"\"\"Fetch multiple endpoints concurrently.\"\"\"\n",
      "    async with aiohttp.ClientSession() as session:\n",
      "        tasks = [self.fetch_endpoint(session, endpoint) for endpoint in endpoints]\n",
      "        # Gather returns results in order, maintaining consistency\n",
      "        results = await asyncio.gather(*tasks, return_exceptions=False)\n",
      "        return results\n",
      "\n",
      "def get_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
      "    \"\"\"Get summary statistics from results.\"\"\"\n",
      "    errors = sum(1 for r in results if \"error\" in r)\n",
      "    successes = len(results) - errors\n",
      "    \n",
      "    return {\n",
      "        \"total_responses\": successes,\n",
      "        \"errors\": errors,\n",
      "        \"success_rate\": successes / len(results) if results else 0,\n",
      "    }\n",
      "```\n",
      "\n",
      "**Benefits**:\n",
      "- No shared state\n",
      "- No race conditions\n",
      "- Stateless and reusable\n",
      "- Results correspond 1:1 with endpoints\n",
      "\n",
      "---\n",
      "\n",
      "### **Solution 2: Use asyncio.Lock (If You Need Instance State)**\n",
      "\n",
      "```python\n",
      "import asyncio\n",
      "from typing import List, Dict, Optional, Any\n",
      "\n",
      "class AsyncAPIClient:\n",
      "    def __init__(self, base_url: str):\n",
      "        self.base_url = base_url\n",
      "        self.responses = []\n",
      "        self.error_count = 0\n",
      "        self._lock = asyncio.Lock()  # Synchronization primitive\n",
      "\n",
      "    async def fetch_all(self, endpoints: List[str]) -> List[Dict[str, Any]]:\n",
      "        \"\"\"Fetch multiple endpoints concurrently.\"\"\"\n",
      "        self.responses = []  # Reset state\n",
      "        self.error_count = 0\n",
      "        \n",
      "        async with aiohttp.ClientSession() as session:\n",
      "            tasks = [self.fetch_endpoint(session, endpoint) for endpoint in endpoints]\n",
      "\n",
      "            for coro in asyncio.as_completed(tasks):\n",
      "                result = await coro\n",
      "                \n",
      "                # Protect shared state with lock\n",
      "                async with self._lock:\n",
      "                    if \"error\" in result:\n",
      "                        self.error_count += 1\n",
      "                    else:\n",
      "                        self.responses.append(result)\n",
      "\n",
      "        return self.responses\n",
      "```\n",
      "\n",
      "**Note**: This adds overhead and complexity. Only use if you have a good reason to maintain instance state.\n",
      "\n",
      "---\n",
      "\n",
      "### **Solution 3: Use Local Variables (Best Practice)**\n",
      "\n",
      "```python\n",
      "async def fetch_all(self, endpoints: List[str]) -> List[Dict[str, Any]]:\n",
      "    \"\"\"Fetch multiple endpoints concurrently.\"\"\"\n",
      "    responses = []  # Local variable, no sharing\n",
      "    error_count = 0\n",
      "    \n",
      "    async with aiohttp.ClientSession() as session:\n",
      "        tasks = [self.fetch_endpoint(session, endpoint) for endpoint in endpoints]\n",
      "\n",
      "        for coro in asyncio.as_completed(tasks):\n",
      "            result = await coro\n",
      "            \n",
      "            # No race condition - single coroutine owns these variables\n",
      "            if \"error\" in result:\n",
      "                error_count += 1\n",
      "            else:\n",
      "                responses.append(result)\n",
      "\n",
      "    # Store final results if needed\n",
      "    self.responses = responses\n",
      "    self.error_count = error_count\n",
      "    \n",
      "    return responses\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## üìä **Comparison of Approaches**\n",
      "\n",
      "| Approach | Pros | Cons | Use When |\n",
      "|----------|------|------|----------|\n",
      "| **Direct Return** | Simple, no race conditions, functional | No state tracking | Default choice |\n",
      "| **asyncio.Lock** | Maintains instance state safely | Overhead, complexity | Multiple coroutines need shared state |\n",
      "| **Local Variables** | Balance of simplicity and state | Still uses instance variables | Need to track last run stats |\n",
      "\n",
      "---\n",
      "\n",
      "## üéØ **Best Practice Recommendations**\n",
      "\n",
      "1. **Prefer functional/stateless designs** - Return data instead of storing it\n",
      "2. **Use `asyncio.gather()`** instead of `as_completed()` when order matters\n",
      "3. **Separate data from client** - Client should be stateless, results should be returned\n",
      "4. **Add proper error handling** - Store errors with details for debugging\n",
      "5. **Make timeouts configurable** - Different endpoints may need different timeouts\n",
      "6. **Consider adding retry logic** with exponential backoff\n",
      "7. **Add rate limiting** to avoid overwhelming APIs\n",
      "\n",
      "---\n",
      "\n",
      "## üîç **Why Async Code Has Race Conditions**\n",
      "\n",
      "Despite running in a single thread, async code can still have race conditions because:\n",
      "\n",
      "```python\n",
      "# This is NOT atomic in async:\n",
      "self.error_count += 1\n",
      "\n",
      "# It actually does:\n",
      "# 1. Read self.error_count\n",
      "# 2. Add 1\n",
      "# 3. Write back to self.error_count\n",
      "\n",
      "# Another coroutine can run between steps 1-3!\n",
      "```\n",
      "\n",
      "**Example interleaving**:\n",
      "```\n",
      "Coroutine A: Read error_count (0)\n",
      "Coroutine B: Read error_count (0)  # Still 0!\n",
      "Coroutine A: Write error_count (1)\n",
      "Coroutine B: Write error_count (1)  # Should be 2!\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "  üîß Memory tool: str_replace \n",
      "  ‚úì Result: Missing required parameters: path, old_str\n",
      "\n",
      "üîÑ Turn 5:\n",
      "  üîß Memory tool: str_replace /memories/review_progress.md\n",
      "  ‚úì Result: File /memories/review_progress.md has been edited successfully\n",
      "\n",
      "============================================================\n",
      "‚úÖ Session 2 complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# NEW conversation (empty messages)\n",
    "# Load API client code with similar concurrency issue\n",
    "with open(\"memory_demo/sample_code/api_client_v1.py\", \"r\") as f:\n",
    "    code_to_review = f.read()\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Review this API client code:\\n\\n```python\\n{code_to_review}\\n```\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ SESSION 2: Applying learned pattern\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run conversation loop\n",
    "response = run_conversation_loop(\n",
    "    client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    memory_handler=memory,\n",
    "    system=\"You are a code reviewer.\",\n",
    "    max_tokens=2048,\n",
    "    max_turns=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Session 2 complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice the difference:**\n",
    "\n",
    "- Claude **immediately checked memory** and found the thread-safety/concurrency pattern\n",
    "- Recognized the similar issue in async code **instantly** without re-learning\n",
    "- Response was **faster** because it applied stored knowledge about shared mutable state\n",
    "\n",
    "This is **cross-conversation learning** in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Context Clearing While Preserving Memory\n",
    "\n",
    "What happens during a **long review session** with many code files?\n",
    "\n",
    "- Context fills up with tool results from previous reviews\n",
    "- But memory (learned patterns) must persist!\n",
    "\n",
    "Let's trigger **context editing** to see how Claude manages this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:42:16.180214Z",
     "iopub.status.busy": "2025-11-15T16:42:16.180126Z",
     "iopub.status.idle": "2025-11-15T16:42:36.700578Z",
     "shell.execute_reply": "2025-11-15T16:42:36.700079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìö SESSION 3: Long review session with context clearing\n",
      "============================================================\n",
      "\n",
      "üìù Review 1: Data processor\n",
      "üß† Thinking: Let me review this data processor code. I should first check my memory, then provide a comprehensive...\n",
      "  üîß Memory tool: view /memories\n",
      "  ‚úì Result: Directory: /memories\n",
      "- review_progress.md\n",
      "  üìä Input tokens: 6,651\n",
      "  ‚ÑπÔ∏è  Context below threshold - no clearing triggered\n",
      "\n",
      "üìù Review 2: SQL query builder\n",
      "üß† Thinking: The user is now asking me to review a SQL query builder with SQL injection vulnerabilities. Let me c...\n",
      "  üîß Memory tool: str_replace /memories/review_progress.md\n",
      "  ‚úì Result: File /memories/review_progress.md has been edited successfully\n",
      "  üìä Input tokens: 7,740\n",
      "  ‚úÇÔ∏è  Context editing triggered!\n",
      "      ‚Ä¢ Cleared 0 tool uses\n",
      "      ‚Ä¢ Saved 38 tokens\n",
      "      ‚Ä¢ After clearing: 7,740 tokens\n",
      "\n",
      "üìù Review 3: Web scraper (should trigger clearing)\n",
      "üß† Thinking: The user is asking for a quick check on this web scraper code. Let me identify the issues:\n",
      "\n",
      "1. **Rac...\n",
      "  üîß Memory tool: str_replace /memories/review_progress.md\n",
      "  ‚úì Result: File /memories/review_progress.md has been edited successfully\n",
      "  üìä Input tokens: 8,926\n",
      "  ‚úÇÔ∏è  Context editing triggered!\n",
      "      ‚Ä¢ Cleared 0 tool uses\n",
      "      ‚Ä¢ Saved 113 tokens\n",
      "      ‚Ä¢ After clearing: 8,926 tokens\n",
      "\n",
      "============================================================\n",
      "‚úÖ Session 3 complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure context management with BOTH clearing strategies\n",
    "# Low thresholds for demo - in production use 30-40k tokens\n",
    "CONTEXT_MANAGEMENT = {\n",
    "    \"edits\": [\n",
    "        # Thinking management MUST come first when combining strategies\n",
    "        {\n",
    "            \"type\": \"clear_thinking_20251015\",\n",
    "            \"keep\": {\"type\": \"thinking_turns\", \"value\": 1}  # Keep only last turn's thinking\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"clear_tool_uses_20250919\",\n",
    "            \"trigger\": {\"type\": \"input_tokens\", \"value\": 5000},  # Low threshold for demo\n",
    "            \"keep\": {\"type\": \"tool_uses\", \"value\": 2},  # Keep last 2 tool uses\n",
    "            \"clear_at_least\": {\"type\": \"input_tokens\", \"value\": 2000}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Extended thinking config (required for clear_thinking strategy)\n",
    "THINKING = {\n",
    "    \"type\": \"enabled\",\n",
    "    \"budget_tokens\": 1024  # Budget for thinking per turn\n",
    "}\n",
    "\n",
    "# Continue from previous session - memory persists!\n",
    "print(\"=\" * 60)\n",
    "print(\"üìö SESSION 3: Long review session with context clearing\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Review 1: Data processor (larger file)\n",
    "with open(\"memory_demo/sample_code/data_processor_v1.py\", \"r\") as f:\n",
    "    data_processor_code = f.read()\n",
    "\n",
    "messages.extend([\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Review this data processor:\\n\\n```python\\n{data_processor_code}\\n```\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"üìù Review 1: Data processor\")\n",
    "response = run_conversation_turn(\n",
    "    client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    memory_handler=memory,\n",
    "    system=\"You are a code reviewer.\",\n",
    "    context_management=CONTEXT_MANAGEMENT,\n",
    "    thinking=THINKING,\n",
    "    max_tokens=4096,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Add response to messages\n",
    "messages.append({\"role\": \"assistant\", \"content\": response[1]})\n",
    "if response[2]:\n",
    "    messages.append({\"role\": \"user\", \"content\": response[2]})\n",
    "\n",
    "print(f\"  üìä Input tokens: {response[0].usage.input_tokens:,}\")\n",
    "context_cleared, saved = print_context_management_info(response[0])\n",
    "print()\n",
    "\n",
    "# Review 2: SQL code\n",
    "with open(\"memory_demo/sample_code/sql_query_builder.py\", \"r\") as f:\n",
    "    sql_code = f.read()\n",
    "\n",
    "messages.extend([\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Review this SQL query builder:\\n\\n```python\\n{sql_code}\\n```\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"üìù Review 2: SQL query builder\")\n",
    "response = run_conversation_turn(\n",
    "    client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    memory_handler=memory,\n",
    "    system=\"You are a code reviewer.\",\n",
    "    context_management=CONTEXT_MANAGEMENT,\n",
    "    thinking=THINKING,\n",
    "    max_tokens=4096,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response[1]})\n",
    "if response[2]:\n",
    "    messages.append({\"role\": \"user\", \"content\": response[2]})\n",
    "\n",
    "print(f\"  üìä Input tokens: {response[0].usage.input_tokens:,}\")\n",
    "context_cleared, saved = print_context_management_info(response[0])\n",
    "print()\n",
    "\n",
    "# Review 3: Add one more review to ensure we trigger clearing\n",
    "with open(\"memory_demo/sample_code/web_scraper_v1.py\", \"r\") as f:\n",
    "    scraper_code = f.read()\n",
    "\n",
    "messages.extend([\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Quick check - any issues here?\\n\\n```python\\n{scraper_code}\\n```\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"üìù Review 3: Web scraper (should trigger clearing)\")\n",
    "response = run_conversation_turn(\n",
    "    client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    memory_handler=memory,\n",
    "    system=\"You are a code reviewer.\",\n",
    "    context_management=CONTEXT_MANAGEMENT,\n",
    "    thinking=THINKING,\n",
    "    max_tokens=4096,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response[1]})\n",
    "if response[2]:\n",
    "    messages.append({\"role\": \"user\", \"content\": response[2]})\n",
    "\n",
    "print(f\"  üìä Input tokens: {response[0].usage.input_tokens:,}\")\n",
    "context_cleared, saved = print_context_management_info(response[0])\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Session 3 complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "\n",
    "As context grew during multiple reviews with extended thinking enabled, context editing was applied:\n",
    "1. **Thinking blocks cleared** - Old thinking from previous turns removed first\n",
    "2. **Tool results cleared** - Old memory tool results removed when threshold exceeded\n",
    "3. **Memory files intact** - Claude can still query learned patterns\n",
    "4. **Token usage managed** - Saved tokens from both thinking and tool results\n",
    "\n",
    "This demonstrates the key benefit:\n",
    "- **Short-term memory** (conversation context + thinking) ‚Üí Cleared to save space\n",
    "- **Long-term memory** (stored patterns) ‚Üí Persists across sessions\n",
    "\n",
    "Let's verify memory survived the clearing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:42:36.703208Z",
     "iopub.status.busy": "2025-11-15T16:42:36.703056Z",
     "iopub.status.idle": "2025-11-15T16:42:36.707578Z",
     "shell.execute_reply": "2025-11-15T16:42:36.707209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Memory files in demo_memory/:\n",
      "\n",
      "demo_memory/\n",
      "  memories/\n",
      "    ‚îú‚îÄ‚îÄ review_progress.md (255 bytes)\n",
      "\n",
      "‚úÖ All learned patterns preserved despite context clearing!\n"
     ]
    }
   ],
   "source": [
    "# Verify memory persists after context clearing\n",
    "import os\n",
    "\n",
    "print(\"üìÇ Memory files in demo_memory/:\")\n",
    "print()\n",
    "\n",
    "for root, dirs, files in os.walk(\"./demo_memory\"):\n",
    "    # Calculate relative path for display\n",
    "    level = root.replace(\"./demo_memory\", \"\").count(os.sep)\n",
    "    indent = \"  \" * level\n",
    "    folder_name = os.path.basename(root) or \"demo_memory\"\n",
    "    print(f\"{indent}{folder_name}/\")\n",
    "    \n",
    "    sub_indent = \"  \" * (level + 1)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"{sub_indent}‚îú‚îÄ‚îÄ {file} ({size} bytes)\")\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ All learned patterns preserved despite context clearing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How It Works {#how-it-works}\n",
    "\n",
    "### Memory Tool Architecture\n",
    "\n",
    "The memory tool is **client-side** - you control the storage. Claude makes tool calls, your application executes them.\n",
    "\n",
    "#### Memory Tool Commands\n",
    "\n",
    "| Command | Description | Example |\n",
    "|---------|-------------|---------|\n",
    "| `view` | Show directory or file contents | `{\"command\": \"view\", \"path\": \"/memories\"}` |\n",
    "| `create` | Create or overwrite a file | `{\"command\": \"create\", \"path\": \"/memories/notes.md\", \"file_text\": \"...\"}` |\n",
    "| `str_replace` | Replace text in a file | `{\"command\": \"str_replace\", \"path\": \"...\", \"old_str\": \"...\", \"new_str\": \"...\"}` |\n",
    "| `insert` | Insert text at line number | `{\"command\": \"insert\", \"path\": \"...\", \"insert_line\": 2, \"insert_text\": \"...\"}` |\n",
    "| `delete` | Delete a file or directory | `{\"command\": \"delete\", \"path\": \"/memories/old.txt\"}` |\n",
    "| `rename` | Rename or move a file | `{\"command\": \"rename\", \"old_path\": \"...\", \"new_path\": \"...\"}` |\n",
    "\n",
    "See `memory_tool.py` for the complete implementation with path validation and security measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking Management (`clear_thinking_20251015`)\n",
    "\n",
    "When using extended thinking, thinking blocks accumulate and consume tokens. The `clear_thinking` strategy manages these automatically.\n",
    "\n",
    "**Important**: This strategy requires `thinking` to be enabled in your API call.\n",
    "\n",
    "**API Call Pattern** (with extended thinking enabled):\n",
    "\n",
    "```python\n",
    "response = client.beta.messages.create(\n",
    "    betas=[\"context-management-2025-06-27\"],  # Required beta flag\n",
    "    model=\"claude-sonnet-4-5\",\n",
    "    messages=messages,\n",
    "    tools=[{\"type\": \"memory_20250818\", \"name\": \"memory\"}],\n",
    "    thinking={\"type\": \"enabled\", \"budget_tokens\": 10000},  # Enable thinking\n",
    "    context_management={  # Context editing config\n",
    "        \"edits\": [\n",
    "            {\n",
    "                \"type\": \"clear_thinking_20251015\",\n",
    "                \"keep\": {\"type\": \"thinking_turns\", \"value\": 1}  # Keep last turn only\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"clear_tool_uses_20250919\",\n",
    "                \"trigger\": {\"type\": \"input_tokens\", \"value\": 35000},\n",
    "                \"keep\": {\"type\": \"tool_uses\", \"value\": 5}\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    max_tokens=2048\n",
    ")\n",
    "```\n",
    "\n",
    "**Key points:**\n",
    "- `clear_thinking` must come **first** when combining strategies\n",
    "- Requires extended thinking to be enabled (`thinking={\"type\": \"enabled\", ...}`)\n",
    "- Use `\"keep\": \"all\"` to preserve all thinking blocks for maximum cache hits\n",
    "- Trigger is optional for thinking (clears based on `keep` value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Demo Code\n",
    "\n",
    "Key implementation details from `code_review_demo.py`:\n",
    "\n",
    "```python\n",
    "class CodeReviewAssistant:\n",
    "    def __init__(self, memory_storage_path=\"./memory_storage\"):\n",
    "        self.client = Anthropic(api_key=API_KEY)\n",
    "        self.memory_handler = MemoryToolHandler(base_path=memory_storage_path)\n",
    "        self.messages = []\n",
    "    \n",
    "    def review_code(self, code, filename, description=\"\"):\n",
    "        # 1. Add user message\n",
    "        self.messages.append({...})\n",
    "        \n",
    "        # 2. Conversation loop with tool execution\n",
    "        while True:\n",
    "            response = self.client.beta.messages.create(\n",
    "                model=MODEL,\n",
    "                system=self._create_system_prompt(),\n",
    "                messages=self.messages,\n",
    "                tools=[{\"type\": \"memory_20250818\", \"name\": \"memory\"}],\n",
    "                betas=[\"context-management-2025-06-27\"],\n",
    "                context_management=CONTEXT_MANAGEMENT\n",
    "            )\n",
    "            \n",
    "            # 3. Execute tool uses\n",
    "            tool_results = []\n",
    "            for content in response.content:\n",
    "                if content.type == \"tool_use\":\n",
    "                    result = self._execute_tool_use(content)\n",
    "                    tool_results.append({...})\n",
    "            \n",
    "            # 4. Continue if there are tool uses, otherwise done\n",
    "            if tool_results:\n",
    "                self.messages.append({\"role\": \"user\", \"content\": tool_results})\n",
    "            else:\n",
    "                break\n",
    "```\n",
    "\n",
    "**The key pattern**: Keep calling the API while there are tool uses, executing them and feeding results back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Claude Actually Learns\n",
    "\n",
    "This is what makes memory powerful - **semantic pattern recognition**, not just syntax:\n",
    "\n",
    "**Session 1: Thread-Based Web Scraper**\n",
    "\n",
    "```python\n",
    "# Bug: Race condition\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.results = []  # Shared state!\n",
    "    \n",
    "    def scrape_urls(self, urls):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            for future in as_completed(futures):\n",
    "                self.results.append(future.result())  # RACE!\n",
    "```\n",
    "\n",
    "**What Claude Stores in Memory** (example file: `/memories/concurrency_patterns/thread_safety.md`):\n",
    "\n",
    "When Claude encounters this pattern, it stores the following insights to its memory files:\n",
    "- **Symptom**: Inconsistent results in concurrent operations\n",
    "- **Cause**: Shared mutable state (lists/dicts) modified from multiple threads\n",
    "- **Solution**: Use locks, thread-safe data structures, or return results instead\n",
    "- **Red flags**: Instance variables in thread callbacks, unused locks, counter increments\n",
    "\n",
    "---\n",
    "\n",
    "**Session 2: Async API Client** (New conversation!)\n",
    "\n",
    "Claude checks memory FIRST, finds the thread-safety pattern, then:\n",
    "1. **Recognizes** similar pattern in async code (coroutines can interleave too)\n",
    "2. **Applies** the solution immediately (no re-learning needed)\n",
    "3. **Explains** with reference to stored knowledge\n",
    "\n",
    "```python\n",
    "# Claude spots this immediately:\n",
    "async def fetch_all(self, endpoints):\n",
    "    for coro in asyncio.as_completed(tasks):\n",
    "        self.responses.append(await coro)  # Same pattern!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Why This Matters:**\n",
    "\n",
    "- ‚ùå **Syntax checkers** miss race conditions entirely\n",
    "- ‚úÖ **Claude learns** architectural patterns and applies them across contexts\n",
    "- ‚úÖ **Cross-language**: Pattern applies to Go, Java, Rust concurrency too\n",
    "- ‚úÖ **Gets better**: Each review adds to the knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Code Files\n",
    "\n",
    "The demo uses these sample files (all have concurrency/thread-safety bugs):\n",
    "\n",
    "- `memory_demo/sample_code/web_scraper_v1.py` - Race condition: threads modifying shared state\n",
    "- `memory_demo/sample_code/api_client_v1.py` - Similar concurrency bug in async context\n",
    "- `memory_demo/sample_code/data_processor_v1.py` - Multiple concurrency issues for long session demo\n",
    "\n",
    "Let's look at one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`memory_demo/sample_code/web_scraper_v1.py`**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Concurrent web scraper with a race condition bug.\n",
    "Multiple threads modify shared state without synchronization.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "class WebScraper:\n",
    "    \"\"\"Web scraper that fetches multiple URLs concurrently.\"\"\"\n",
    "\n",
    "    def __init__(self, max_workers: int = 10):\n",
    "        self.max_workers = max_workers\n",
    "        self.results = []  # BUG: Shared mutable state accessed by multiple threads!\n",
    "        self.failed_urls = []  # BUG: Another race condition!\n",
    "\n",
    "    def fetch_url(self, url: str) -> Dict[str, any]:\n",
    "        \"\"\"Fetch a single URL and return the result.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            response.raise_for_status()\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"status\": response.status_code,\n",
    "                \"content_length\": len(response.content),\n",
    "            }\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return {\"url\": url, \"error\": str(e)}\n",
    "\n",
    "    def scrape_urls(self, urls: List[str]) -> List[Dict[str, any]]:\n",
    "        \"\"\"\n",
    "        Scrape multiple URLs concurrently.\n",
    "\n",
    "        BUG: self.results is accessed from multiple threads without locking!\n",
    "        This causes race conditions where results can be lost or corrupted.\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = [executor.submit(self.fetch_url, url) for url in urls]\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "\n",
    "                # RACE CONDITION: Multiple threads append to self.results simultaneously\n",
    "                if \"error\" in result:\n",
    "                    self.failed_urls.append(result[\"url\"])  # RACE CONDITION\n",
    "                else:\n",
    "                    self.results.append(result)  # RACE CONDITION\n",
    "\n",
    "        return self.results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bug**: Multiple threads modify `self.results` and `self.failed_urls` without locking!\n",
    "\n",
    "Claude will:\n",
    "1. Identify the race conditions\n",
    "2. Store the pattern in `/memories/concurrency_patterns/thread_safety.md`\n",
    "3. Apply this concurrency pattern to async code in Session 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo Overview\n",
    "\n",
    "We've built a complete Code Review Assistant. The implementation is in `memory_demo/code_review_demo.py`.\n",
    "\n",
    "**To run the interactive demo:**\n",
    "```bash\n",
    "python memory_demo/code_review_demo.py\n",
    "```\n",
    "\n",
    "The demo demonstrates:\n",
    "1. **Session 1**: Review Python code with a bug ‚Üí Claude learns the pattern\n",
    "2. **Session 2**: Review similar code (new conversation) ‚Üí Claude applies the pattern\n",
    "3. **Session 3**: Long review session ‚Üí Context editing keeps it manageable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices & Security {#best-practices}\n",
    "\n",
    "### Memory Management\n",
    "\n",
    "**Do:**\n",
    "- ‚úÖ Store task-relevant patterns, not conversation history\n",
    "- ‚úÖ Organize with clear directory structure\n",
    "- ‚úÖ Use descriptive file names\n",
    "- ‚úÖ Periodically review and clean up memory\n",
    "\n",
    "**Don't:**\n",
    "- ‚ùå Store sensitive information (passwords, API keys, PII)\n",
    "- ‚ùå Let memory grow unbounded\n",
    "- ‚ùå Store everything indiscriminately\n",
    "\n",
    "### Security: Path Traversal Protection\n",
    "\n",
    "**Critical**: Always validate paths to prevent directory traversal attacks. See `memory_tool.py` for implementation.\n",
    "\n",
    "### Security: Memory Poisoning\n",
    "\n",
    "**‚ö†Ô∏è Critical Risk**: Memory files are read back into Claude's context, making them a potential vector for prompt injection.\n",
    "\n",
    "**Mitigation strategies:**\n",
    "1. **Content Sanitization**: Filter dangerous patterns before storing\n",
    "2. **Memory Scope Isolation**: Per-user/per-project isolation  \n",
    "3. **Memory Auditing**: Log and scan all memory operations\n",
    "4. **Prompt Engineering**: Instruct Claude to ignore instructions in memory\n",
    "\n",
    "See `memory_tool.py` for complete security implementation and tests in `tests/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **API docs**: [Claude API reference](https://docs.claude.com/en/api/messages)\n",
    "- **Usage docs**: [Memory tool](https://docs.claude.com/en/docs/agents-and-tools/tool-use/memory-tool)\n",
    "- **GitHub Action**: [claude-code-action](https://github.com/anthropics/claude-code-action)\n",
    "- **Support**: [support.claude.com](https://support.claude.com)\n",
    "\n",
    "### Feedback\n",
    "\n",
    "Memory and context management are in **beta**. Share your feedback to help us improve!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
