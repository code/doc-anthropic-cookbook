{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Editing & Memory for Long-Running Agents\n",
    "\n",
    "AI agents that run across multiple sessions or handle long-running tasks face two key challenges: they lose learned patterns between conversations, and context windows fill up during extended interactions.\n",
    "\n",
    "This cookbook demonstrates how to address these challenges using Claude's memory tool and context editing capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction: Why Memory Matters](#introduction)\n",
    "2. [Use Cases](#use-cases)\n",
    "3. [Quick Start Examples](#quick-start)\n",
    "4. [How It Works](#how-it-works)\n",
    "5. [Code Review Assistant Demo](#demo)\n",
    "6. [Real-World Applications](#real-world)\n",
    "7. [Best Practices](#best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Required Knowledge:**\n",
    "- Python fundamentals (functions, classes, async/await basics)\n",
    "- Basic understanding of REST APIs and JSON\n",
    "\n",
    "**Required Tools:**\n",
    "- Python 3.10 or higher\n",
    "- Anthropic API key ([get one here](https://console.anthropic.com/))\n",
    "\n",
    "**Recommended:**\n",
    "- Familiarity with concurrent programming concepts (threads, async)\n",
    "- Basic understanding of context windows in LLMs\n",
    "\n",
    "## Setup\n",
    "\n",
    "### For VSCode Users\n",
    "\n",
    "```bash\n",
    "# 1. Create virtual environment\n",
    "python -m venv .venv\n",
    "\n",
    "# 2. Activate it\n",
    "source .venv/bin/activate  # macOS/Linux\n",
    "# or: .venv\\Scripts\\activate  # Windows\n",
    "\n",
    "# 3. Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 4. In VSCode: Select .venv as kernel (top right)\n",
    "```\n",
    "\n",
    "### API Key\n",
    "\n",
    "```bash\n",
    "cp .env.example .env\n",
    "# Edit .env and add your ANTHROPIC_API_KEY\n",
    "```\n",
    "\n",
    "Get your API key from: https://console.anthropic.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: Why Memory Matters {#introduction}\n",
    "\n",
    "This cookbook demonstrates practical implementations of the context engineering patterns described in [Effective context engineering for AI agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents). That post covers why context is a finite resource, how attention budgets work, and strategies for building effective agents‚Äîthe techniques you'll see in action here.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Large language models have finite context windows (200k tokens for Claude 4). While this seems large, several challenges emerge:\n",
    "\n",
    "- **Context limits**: Long conversations or complex tasks can exceed available context\n",
    "- **Computational cost**: Processing large contexts is expensive - attention mechanisms scale quadratically\n",
    "- **Repeated patterns**: Similar tasks across conversations require re-explaining context every time\n",
    "- **Information loss**: When context fills up, earlier important information gets lost\n",
    "\n",
    "### The Solution\n",
    "\n",
    "Claude 4 models introduce powerful context management capabilities:\n",
    "\n",
    "1. **Memory Tool** (`memory_20250818`): Enables cross-conversation learning\n",
    "   - Claude can write down what it learns for future reference\n",
    "   - File-based system under `/memories` directory\n",
    "   - Client-side implementation gives you full control\n",
    "\n",
    "2. **Context Editing**: Automatically manages context with two strategies:\n",
    "   - **Tool use clearing** (`clear_tool_uses_20250919`): Clears old tool results when context grows large\n",
    "   - **Thinking management** (`clear_thinking_20251015`): Manages extended thinking blocks (requires thinking enabled)\n",
    "   - Configurable triggers and retention policies\n",
    "\n",
    "### The Benefit\n",
    "\n",
    "Build AI agents that **get better at your specific tasks over time**:\n",
    "\n",
    "- **Session 1**: Claude solves a problem, writes down the pattern\n",
    "- **Session 2**: Claude applies the learned pattern immediately (faster!)\n",
    "- **Long sessions**: Context editing keeps conversations manageable\n",
    "\n",
    "Think of it as giving Claude a notebook to take notes and refer back to - just like humans do.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "By the end of this cookbook, you will be able to:\n",
    "- **Implement** the memory tool for cross-conversation learning\n",
    "- **Configure** context editing to manage long-running sessions\n",
    "- **Apply** best practices for memory security and organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use Cases {#use-cases}\n",
    "\n",
    "Memory and context management enable powerful new workflows:\n",
    "\n",
    "### üîç Code Review Assistant\n",
    "- Learns debugging patterns from past reviews\n",
    "- Recognizes similar bugs instantly in future sessions\n",
    "- Builds team-specific code quality knowledge\n",
    "- **Production ready**: Integrate with [claude-code-action](https://github.com/anthropics/claude-code-action) for GitHub PR reviews\n",
    "\n",
    "### üìö Research Assistant\n",
    "- Accumulates knowledge on topics over multiple sessions\n",
    "- Connects insights across different research threads\n",
    "- Maintains bibliography and source tracking\n",
    "\n",
    "### üí¨ Customer Support Bot\n",
    "- Learns user preferences and communication style\n",
    "- Remembers common issues and solutions\n",
    "- Builds product knowledge base from interactions\n",
    "\n",
    "### üìä Data Analysis Helper\n",
    "- Remembers dataset patterns and anomalies\n",
    "- Stores analysis techniques that work well\n",
    "- Builds domain-specific insights over time\n",
    "\n",
    "**Supported Models**: Claude Opus 4.1 (`claude-opus-4-1`), Claude Opus 4 (`claude-opus-4`), Claude Sonnet 4.5 (`claude-sonnet-4-5`), Claude Sonnet 4 (`claude-sonnet-4`), and Claude Haiku 4.5 (`claude-haiku-4-5`)\n",
    "\n",
    "**This cookbook focuses on the Code Review Assistant** as it clearly demonstrates both memory (learning patterns) and context editing (handling long reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick Start Examples {#quick-start}\n",
    "\n",
    "Let's see memory and context management in action with simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, install dependencies and configure your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T17:11:26.527705Z",
     "iopub.status.busy": "2025-11-15T17:11:26.527494Z",
     "iopub.status.idle": "2025-11-15T17:11:30.063615Z",
     "shell.execute_reply": "2025-11-15T17:11:30.063106Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages\n",
    "# Option 1: From requirements.txt\n",
    "# %pip install -q -r requirements.txt\n",
    "\n",
    "# Option 2: Direct install\n",
    "%pip install -q anthropic python-dotenv ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ö†Ô∏è Important**: Create a `.env` file in this directory:\n",
    "\n",
    "```bash\n",
    "# Copy .env.example to .env and add your API key\n",
    "cp .env.example .env\n",
    "```\n",
    "\n",
    "Then edit `.env` to add your Anthropic API key from https://console.anthropic.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T17:11:30.066093Z",
     "iopub.status.busy": "2025-11-15T17:11:30.065829Z",
     "iopub.status.idle": "2025-11-15T17:11:30.318563Z",
     "shell.execute_reply": "2025-11-15T17:11:30.318322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì API key loaded\n",
      "‚úì Using model: claude-sonnet-4-5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any, cast\n",
    "\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Model configuration - use alias for automatic updates\n",
    "MODEL = \"claude-sonnet-4-5\"  # Can override via ANTHROPIC_MODEL env var\n",
    "if os.getenv(\"ANTHROPIC_MODEL\"):\n",
    "    MODEL = os.getenv(\"ANTHROPIC_MODEL\")\n",
    "\n",
    "if not os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "    raise ValueError(\n",
    "        \"ANTHROPIC_API_KEY not found. \"\n",
    "        \"Copy .env.example to .env and add your API key.\"\n",
    "    )\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "print(\"‚úì API key loaded\")\n",
    "print(f\"‚úì Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Basic Memory Usage\n",
    "\n",
    "Let's see Claude use memory to store information for future reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions**\n",
    "\n",
    "These examples use helper functions from `demo_helpers.py`:\n",
    "\n",
    "- **`run_conversation_loop()`**: Handles the API conversation loop\n",
    "  - Calls Claude's API with memory tool enabled\n",
    "  - Executes tool uses (memory operations)\n",
    "  - Continues until Claude stops using tools\n",
    "  - Returns the final response\n",
    "\n",
    "- **`run_conversation_turn()`**: Single turn (used in Example 3)\n",
    "  - Same as above but returns after one API call\n",
    "  - Useful when you need fine-grained control\n",
    "\n",
    "- **`print_context_management_info()`**: Displays context clearing stats\n",
    "  - Shows tokens saved, tool uses cleared\n",
    "  - Helps visualize when context editing triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ö†Ô∏è Note on Memory Clearing**\n",
    "\n",
    "The following cell clears all memory files to provide a clean slate for this demonstration. This is useful for running the notebook multiple times to see consistent results.\n",
    "\n",
    "**In production applications**, you should carefully consider whether to clear all memory, as it permanently removes learned patterns. Consider using selective deletion or organizing memory into project-specific directories instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T17:11:30.331182Z",
     "iopub.status.busy": "2025-11-15T17:11:30.331084Z",
     "iopub.status.idle": "2025-11-15T17:12:09.831980Z",
     "shell.execute_reply": "2025-11-15T17:12:09.831417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Clearing previous memories...\n",
      "‚úì Memory cleared\n",
      "\n",
      "============================================================\n",
      "üìù SESSION 1: Learning from a bug\n",
      "============================================================\n",
      "\n",
      "üîÑ Turn 1:\n",
      "üí¨ Claude: I'll review this multi-threaded web scraper to identify the race condition issues. Let me start by checking my memory.\n",
      "\n",
      "  üîß Memory tool: view /memories\n",
      "  ‚úì Result: Directory: /memories\n",
      "(empty)\n",
      "\n",
      "üîÑ Turn 2:\n",
      "  üîß Memory tool: create /memories/review_progress.md\n",
      "  ‚úì Result: File created successfully at /memories/review_progress.md\n",
      "\n",
      "üîÑ Turn 3:\n",
      "üí¨ Claude: ## Code Review: Multi-threaded Web Scraper Race Condition\n",
      "\n",
      "I've identified **multiple critical race condition bugs** in this code. Here's my detailed analysis:\n",
      "\n",
      "---\n",
      "\n",
      "### üêõ **PRIMARY ISSUES**\n",
      "\n",
      "#### **1. Race Condition on `self.results` (Line 51)**\n",
      "```python\n",
      "self.results.append(result)  # RACE CONDITION\n",
      "```\n",
      "\n",
      "**Problem:** Multiple threads are calling `list.append()` on the shared `self.results` list simultaneously. While Python's `list.append()` is *technically* atomic at the bytecode level due to the GIL, **the real issue is that results from multiple threads in `as_completed()` are being processed in the main thread**, so this isn't actually the race condition causing lost results.\n",
      "\n",
      "#### **2. Race Condition on `self.failed_urls` (Line 49)**\n",
      "```python\n",
      "self.failed_urls.append(result[\"url\"])  # RACE CONDITION\n",
      "```\n",
      "\n",
      "**Problem:** Same issue as above with shared mutable state.\n",
      "\n",
      "#### **3. THE ACTUAL BUG: Instance Reuse** ‚ö†Ô∏è\n",
      "The **real problem** causing inconsistent counts is:\n",
      "- `self.results` and `self.failed_urls` are **never cleared** between runs\n",
      "- If you call `scrape_urls()` multiple times on the same instance, results accumulate\n",
      "- But more critically: the lists are **class instance variables** that persist\n",
      "\n",
      "---\n",
      "\n",
      "### üîç **WHY RESULTS ARE LOST**\n",
      "\n",
      "Looking more carefully at the code structure:\n",
      "\n",
      "```python\n",
      "for future in as_completed(futures):\n",
      "    result = future.result()\n",
      "    if \"error\" in result:\n",
      "        self.failed_urls.append(result[\"url\"])\n",
      "    else:\n",
      "        self.results.append(result)\n",
      "```\n",
      "\n",
      "**The actual race condition:** While `as_completed()` runs in the main thread, there's still a subtle issue. The comment says results can be \"lost or corrupted.\" Here's why:\n",
      "\n",
      "1. **List operations in CPython:** While `append()` is atomic, if the list needs to resize, there's a brief window where corruption could occur\n",
      "2. **Memory visibility:** Without proper synchronization, threads might see stale values\n",
      "3. **Non-atomic read-modify-write:** If list internals are being modified, there could be issues\n",
      "\n",
      "However, the **most likely cause** of inconsistent counts is:\n",
      "- **Exception handling:** Some futures might raise exceptions that aren't caught\n",
      "- **Instance state reuse:** Running the scraper multiple times without clearing state\n",
      "\n",
      "---\n",
      "\n",
      "### ‚úÖ **FIXES**\n",
      "\n",
      "  üîß Memory tool: str_replace /memories/review_progress.md\n",
      "  ‚úì Result: File /memories/review_progress.md has been edited successfully\n",
      "\n",
      "üîÑ Turn 4:\n",
      "üí¨ Claude: #### **Solution 1: Use Threading Locks** üîí\n",
      "\n",
      "```python\n",
      "import threading\n",
      "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
      "from typing import List, Dict, Any\n",
      "\n",
      "class WebScraper:\n",
      "    def __init__(self, max_workers: int = 10):\n",
      "        self.max_workers = max_workers\n",
      "        self.results = []\n",
      "        self.failed_urls = []\n",
      "        self.lock = threading.Lock()  # Add synchronization\n",
      "\n",
      "    def scrape_urls(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
      "        # Clear previous results\n",
      "        self.results = []\n",
      "        self.failed_urls = []\n",
      "        \n",
      "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
      "            futures = [executor.submit(self.fetch_url, url) for url in urls]\n",
      "\n",
      "            for future in as_completed(futures):\n",
      "                result = future.result()\n",
      "\n",
      "                # Use lock to protect shared state\n",
      "                with self.lock:\n",
      "                    if \"error\" in result:\n",
      "                        self.failed_urls.append(result[\"url\"])\n",
      "                    else:\n",
      "                        self.results.append(result)\n",
      "\n",
      "        return self.results\n",
      "```\n",
      "\n",
      "#### **Solution 2: Avoid Shared State (BEST PRACTICE)** ‚≠ê\n",
      "\n",
      "```python\n",
      "def scrape_urls(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
      "    \"\"\"\n",
      "    Scrape multiple URLs concurrently.\n",
      "    Returns results directly without modifying instance state.\n",
      "    \"\"\"\n",
      "    results = []\n",
      "    failed_urls = []\n",
      "    \n",
      "    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
      "        futures = [executor.submit(self.fetch_url, url) for url in urls]\n",
      "\n",
      "        for future in as_completed(futures):\n",
      "            result = future.result()\n",
      "            \n",
      "            if \"error\" in result:\n",
      "                failed_urls.append(result[\"url\"])\n",
      "            else:\n",
      "                results.append(result)\n",
      "\n",
      "    # Store for stats if needed\n",
      "    self.results = results\n",
      "    self.failed_urls = failed_urls\n",
      "    \n",
      "    return results\n",
      "```\n",
      "\n",
      "#### **Solution 3: Use Thread-Safe Collections** üì¶\n",
      "\n",
      "```python\n",
      "from queue import Queue\n",
      "\n",
      "class WebScraper:\n",
      "    def __init__(self, max_workers: int = 10):\n",
      "        self.max_workers = max_workers\n",
      "        self.results = Queue()  # Thread-safe\n",
      "        self.failed_urls = Queue()  # Thread-safe\n",
      "\n",
      "    def scrape_urls(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
      "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
      "            futures = [executor.submit(self.fetch_url, url) for url in urls]\n",
      "\n",
      "            for future in as_completed(futures):\n",
      "                result = future.result()\n",
      "                \n",
      "                if \"error\" in result:\n",
      "                    self.failed_urls.put(result[\"url\"])\n",
      "                else:\n",
      "                    self.results.put(result)\n",
      "\n",
      "        # Convert queue to list for return\n",
      "        results_list = []\n",
      "        while not self.results.empty():\n",
      "            results_list.append(self.results.get())\n",
      "        \n",
      "        return results_list\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### üìä **SUMMARY**\n",
      "\n",
      "| Issue | Severity | Impact |\n",
      "|-------|----------|--------|\n",
      "| Unsynchronized list access | **HIGH** | Lost/corrupted results |\n",
      "| Instance state not cleared | **MEDIUM** | Accumulating results across runs |\n",
      "| No exception handling | **LOW** | Potential silent failures |\n",
      "\n",
      "**Recommended Fix:** Use **Solution 2** (avoid shared state) as it's the cleanest and most Pythonic approach. The `as_completed()` loop runs in a single thread anyway, so the real issue is architectural rather than a true concurrent access problem.\n",
      "\n",
      "The inconsistent counts are most likely due to:\n",
      "1. Exception handling issues in some edge cases\n",
      "2. HTTP timeouts/failures not being properly counted\n",
      "3. Potential list resizing during concurrent access (rare but possible)\n",
      "\n",
      "Would you like me to provide a complete, production-ready version with proper error handling and logging?\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚úÖ Session 1 complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Import helper functions\n",
    "from memory_demo.demo_helpers import run_conversation_loop, run_conversation_turn, print_context_management_info\n",
    "from memory_tool import MemoryToolHandler\n",
    "\n",
    "# Initialize\n",
    "client = Anthropic()\n",
    "memory = MemoryToolHandler(base_path=\"./demo_memory\")\n",
    "\n",
    "# Clear any existing memories to start fresh\n",
    "print(\"üßπ Clearing previous memories...\")\n",
    "memory.clear_all_memory()\n",
    "print(\"‚úì Memory cleared\\n\")\n",
    "\n",
    "# Load example code with a race condition bug\n",
    "with open(\"memory_demo/sample_code/web_scraper_v1.py\", \"r\") as f:\n",
    "    code_to_review = f.read()\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"I'm reviewing a multi-threaded web scraper that sometimes returns fewer results than expected. The count is inconsistent across runs. Can you find the issue?\\n\\n```python\\n{code_to_review}\\n```\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìù SESSION 1: Learning from a bug\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run conversation loop\n",
    "response = run_conversation_loop(\n",
    "    client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    memory_handler=memory,\n",
    "    system=\"You are a code reviewer.\",\n",
    "    max_tokens=2048,\n",
    "    max_turns=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Session 1 complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened?**\n",
    "\n",
    "1. Claude checked its memory (empty on first run)\n",
    "2. Identified the bug: **race condition** - multiple threads modifying shared state (`self.results` and `self.failed_urls`) without synchronization\n",
    "3. Stored the concurrency pattern in memory for future reference\n",
    "\n",
    "Now let's see the magic - Claude applying this learned pattern in a **new conversation**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Cross-Conversation Learning\n",
    "\n",
    "Start a completely new conversation - memory persists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T17:12:09.834796Z",
     "iopub.status.busy": "2025-11-15T17:12:09.834455Z",
     "iopub.status.idle": "2025-11-15T17:12:49.692649Z",
     "shell.execute_reply": "2025-11-15T17:12:49.692286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ SESSION 2: Applying learned pattern\n",
      "============================================================\n",
      "\n",
      "üîÑ Turn 1:\n",
      "üí¨ Claude: I'll review this async API client code. Let me first check my memory for any context.\n",
      "\n",
      "  üîß Memory tool: view /memories\n",
      "  ‚úì Result: Directory: /memories\n",
      "- review_progress.md\n",
      "\n",
      "üîÑ Turn 2:\n",
      "  üîß Memory tool: view /memories/review_progress.md\n",
      "  ‚úì Result:    1: # Code Review: Multi-threaded Web Scraper\n",
      "   2: \n",
      "   3: ## Task\n",
      "   4: Revie...\n",
      "\n",
      "üîÑ Turn 3:\n",
      "üí¨ Claude: Now let me update my memory and conduct the review:\n",
      "\n",
      "  üîß Memory tool: str_replace /memories/review_progress.md\n",
      "  ‚úì Result: File /memories/review_progress.md has been edited successfully\n",
      "\n",
      "üîÑ Turn 4:\n",
      "üí¨ Claude: ## Code Review: Async API Client\n",
      "\n",
      "### üî¥ Critical Issues\n",
      "\n",
      "#### 1. **Race Conditions on Shared State** (Severity: HIGH)\n",
      "**Location:** Lines 11-12, 49-53\n",
      "\n",
      "```python\n",
      "self.responses = []  # Multiple coroutines append to this\n",
      "self.error_count = 0  # Multiple coroutines increment this\n",
      "```\n",
      "\n",
      "**Problem:** While Python's GIL provides *some* protection for threads, **async coroutines can still interleave operations**. When a coroutine yields (e.g., at `await`), another coroutine can run and modify the same data structures.\n",
      "\n",
      "**Impact:**\n",
      "- List operations like `append()` *might* be atomic in CPython, but relying on this is implementation-dependent and fragile\n",
      "- Counter increments (`self.error_count += 1`) involve read-modify-write and are **NOT atomic**\n",
      "- Can lead to lost updates and incorrect counts\n",
      "\n",
      "#### 2. **Misleading Comment** (Severity: MEDIUM)\n",
      "**Location:** Line 50\n",
      "\n",
      "```python\n",
      "# Not thread-safe in async context!\n",
      "```\n",
      "\n",
      "**Problem:** The comment mentions \"thread-safe\" but this is async code, not threaded code. The issue is about **coroutine-safety** or **concurrency-safety**, not thread-safety per se.\n",
      "\n",
      "#### 3. **Missing Error Responses in Results** (Severity: MEDIUM)\n",
      "**Location:** Lines 49-53\n",
      "\n",
      "```python\n",
      "if \"error\" in result:\n",
      "    self.error_count += 1  # Not atomic!\n",
      "else:\n",
      "    self.responses.append(result)  # Only success responses saved\n",
      "```\n",
      "\n",
      "**Problem:** Error responses are counted but not stored, so you lose valuable debugging information. The returned list only contains successful responses.\n",
      "\n",
      "#### 4. **State Accumulation Bug** (Severity: HIGH)\n",
      "**Location:** Throughout class\n",
      "\n",
      "**Problem:** If `fetch_all()` is called multiple times on the same instance, results accumulate:\n",
      "```python\n",
      "client = AsyncAPIClient(\"...\")\n",
      "await client.fetch_all(endpoints1)  # Gets 10 results\n",
      "await client.fetch_all(endpoints2)  # Now has 10 + 20 = 30 results!\n",
      "```\n",
      "\n",
      "### ‚ö†Ô∏è Design Issues\n",
      "\n",
      "#### 5. **Unnecessary Instance State**\n",
      "The class stores results as instance variables when they could be local variables returned by the method. This violates the principle of minimal mutable state.\n",
      "\n",
      "#### 6. **Inconsistent Error Handling**\n",
      "The `get_summary()` method calculates success rate but might divide by zero if no requests were made (though there's a guard for this).\n",
      "\n",
      "---\n",
      "\n",
      "## üîß Recommended Solutions\n",
      "\n",
      "### **Solution 1: Use asyncio.Lock** (If you must keep instance state)\n",
      "\n",
      "```python\n",
      "import asyncio\n",
      "from typing import List, Dict, Optional, Any\n",
      "import aiohttp\n",
      "\n",
      "\n",
      "class AsyncAPIClient:\n",
      "    \"\"\"Async API client for fetching data from multiple endpoints.\"\"\"\n",
      "\n",
      "    def __init__(self, base_url: str):\n",
      "        self.base_url = base_url\n",
      "        self.responses: List[Dict[str, Any]] = []\n",
      "        self.error_count: int = 0\n",
      "        self._lock = asyncio.Lock()  # Protect shared state\n",
      "\n",
      "    async def fetch_endpoint(\n",
      "        self, session: aiohttp.ClientSession, endpoint: str\n",
      "    ) -> Dict[str, Any]:\n",
      "        \"\"\"Fetch a single endpoint.\"\"\"\n",
      "        url = f\"{self.base_url}/{endpoint}\"\n",
      "        try:\n",
      "            async with session.get(\n",
      "                url, timeout=aiohttp.ClientTimeout(total=5)\n",
      "            ) as response:\n",
      "                data = await response.json()\n",
      "                return {\n",
      "                    \"endpoint\": endpoint,\n",
      "                    \"status\": response.status,\n",
      "                    \"data\": data,\n",
      "                }\n",
      "        except Exception as e:\n",
      "            return {\n",
      "                \"endpoint\": endpoint,\n",
      "                \"error\": str(e),\n",
      "            }\n",
      "\n",
      "    async def fetch_all(self, endpoints: List[str]) -> List[Dict[str, Any]]:\n",
      "        \"\"\"Fetch multiple endpoints concurrently.\"\"\"\n",
      "        # Clear state at the beginning\n",
      "        async with self._lock:\n",
      "            self.responses.clear()\n",
      "            self.error_count = 0\n",
      "\n",
      "        async with aiohttp.ClientSession() as session:\n",
      "            tasks = [self.fetch_endpoint(session, endpoint) for endpoint in endpoints]\n",
      "\n",
      "            for coro in asyncio.as_completed(tasks):\n",
      "                result = await coro\n",
      "\n",
      "                # Use lock to protect shared state\n",
      "                async with self._lock:\n",
      "                    if \"error\" in result:\n",
      "                        self.error_count += 1\n",
      "                    self.responses.append(result)  # Store all results\n",
      "\n",
      "        # Return a copy to prevent external modification\n",
      "        async with self._lock:\n",
      "            return self.responses.copy()\n",
      "\n",
      "    def get_summary(self) -> Dict[str, Any]:\n",
      "        \"\"\"Get summary statistics.\"\"\"\n",
      "        error_count = sum(1 for r in self.responses if \"error\" in r)\n",
      "        success_count = len(self.responses) - error_count\n",
      "        \n",
      "        return {\n",
      "            \"total_responses\": len(self.responses),\n",
      "            \"successful\": success_count,\n",
      "            \"errors\": error_count,\n",
      "            \"success_rate\": (\n",
      "                success_count / len(self.responses)\n",
      "                if len(self.responses) > 0\n",
      "                else 0\n",
      "            ),\n",
      "        }\n",
      "```\n",
      "\n",
      "### **Solution 2: Eliminate Instance State** (RECOMMENDED ‚úÖ)\n",
      "\n",
      "This is cleaner and more functional:\n",
      "\n",
      "```python\n",
      "import asyncio\n",
      "from typing import List, Dict, Any\n",
      "import aiohttp\n",
      "\n",
      "\n",
      "class AsyncAPIClient:\n",
      "    \"\"\"Async API client for fetching data from multiple endpoints.\"\"\"\n",
      "\n",
      "    def __init__(self, base_url: str):\n",
      "        self.base_url = base_url\n",
      "        # No shared state!\n",
      "\n",
      "    async def fetch_endpoint(\n",
      "        self, session: aiohttp.ClientSession, endpoint: str\n",
      "    ) -> Dict[str, Any]:\n",
      "        \"\"\"Fetch a single endpoint.\"\"\"\n",
      "        url = f\"{self.base_url}/{endpoint}\"\n",
      "        try:\n",
      "            async with session.get(\n",
      "                url, timeout=aiohttp.ClientTimeout(total=5)\n",
      "            ) as response:\n",
      "                data = await response.json()\n",
      "                return {\n",
      "                    \"endpoint\": endpoint,\n",
      "                    \"status\": response.status,\n",
      "                    \"data\": data,\n",
      "                }\n",
      "        except Exception as e:\n",
      "            return {\n",
      "                \"endpoint\": endpoint,\n",
      "                \"error\": str(e),\n",
      "            }\n",
      "\n",
      "    async def fetch_all(self, endpoints: List[str]) -> List[Dict[str, Any]]:\n",
      "        \"\"\"Fetch multiple endpoints concurrently.\"\"\"\n",
      "        async with aiohttp.ClientSession() as session:\n",
      "            tasks = [self.fetch_endpoint(session, endpoint) for endpoint in endpoints]\n",
      "            # Use asyncio.gather to collect all results\n",
      "            results = await asyncio.gather(*tasks)\n",
      "            return list(results)\n",
      "\n",
      "    @staticmethod\n",
      "    def get_summary(results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
      "        \"\"\"Get summary statistics from results.\"\"\"\n",
      "        error_count = sum(1 for r in results if \"error\" in r)\n",
      "        success_count = len(results) - error_count\n",
      "        \n",
      "        return {\n",
      "            \"total_responses\": len(results),\n",
      "            \"successful\": success_count,\n",
      "            \"errors\": error_count,\n",
      "            \"success_rate\": (\n",
      "                success_count / len(results)\n",
      "                if len(results) > 0\n",
      "                else 0\n",
      "            ),\n",
      "        }\n",
      "\n",
      "\n",
      "async def main():\n",
      "    \"\"\"Test the async API client.\"\"\"\n",
      "    client = AsyncAPIClient(\"https://jsonplaceholder.typicode.com\")\n",
      "\n",
      "    endpoints = [\n",
      "        \"posts/1\",\n",
      "        \"posts/2\",\n",
      "        \"posts/3\",\n",
      "        \"users/1\",\n",
      "        \"users/2\",\n",
      "        \"invalid/endpoint\",\n",
      "    ] * 20  # 120 requests total\n",
      "\n",
      "    results = await client.fetch_all(endpoints)\n",
      "    summary = AsyncAPIClient.get_summary(results)\n",
      "\n",
      "    print(f\"Expected: 120 total responses (100 success, 20 errors)\")\n",
      "    print(f\"Got: {summary['total_responses']} responses\")\n",
      "    print(f\"Summary: {summary}\")\n",
      "```\n",
      "\n",
      "### **Solution 3: Using asyncio.Queue** (Alternative approach)\n",
      "\n",
      "For streaming results or more complex scenarios:\n",
      "\n",
      "```python\n",
      "async def fetch_all(self, endpoints: List[str]) -> List[Dict[str, Any]]:\n",
      "    \"\"\"Fetch multiple endpoints concurrently using a queue.\"\"\"\n",
      "    results_queue = asyncio.Queue()\n",
      "    \n",
      "\n",
      "\n",
      "============================================================\n",
      "‚úÖ Session 2 complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# NEW conversation (empty messages)\n",
    "# Load API client code with similar concurrency issue\n",
    "with open(\"memory_demo/sample_code/api_client_v1.py\", \"r\") as f:\n",
    "    code_to_review = f.read()\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Review this API client code:\\n\\n```python\\n{code_to_review}\\n```\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ SESSION 2: Applying learned pattern\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run conversation loop\n",
    "response = run_conversation_loop(\n",
    "    client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    memory_handler=memory,\n",
    "    system=\"You are a code reviewer.\",\n",
    "    max_tokens=2048,\n",
    "    max_turns=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Session 2 complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice the difference:**\n",
    "\n",
    "- Claude **immediately checked memory** and found the thread-safety/concurrency pattern\n",
    "- Recognized the similar issue in async code **instantly** without re-learning\n",
    "- Response was **faster** because it applied stored knowledge about shared mutable state\n",
    "\n",
    "This is **cross-conversation learning** in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Context Clearing While Preserving Memory\n",
    "\n",
    "What happens during a **long review session** with many code files?\n",
    "\n",
    "- Context fills up with tool results from previous reviews\n",
    "- But memory (learned patterns) must persist!\n",
    "\n",
    "Let's trigger **context editing** to see how Claude manages this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T17:12:49.694674Z",
     "iopub.status.busy": "2025-11-15T17:12:49.694494Z",
     "iopub.status.idle": "2025-11-15T17:13:06.605373Z",
     "shell.execute_reply": "2025-11-15T17:13:06.604667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìö SESSION 3: Long review session with context clearing\n",
      "============================================================\n",
      "\n",
      "üìù Review 1: Data processor\n",
      "üß† Thinking: The user wants me to review a new piece of code - a data processor with thread-safety issues. Let me...\n",
      "  üîß Memory tool: str_replace /memories/review_progress.md\n",
      "  ‚úì Result: File /memories/review_progress.md has been edited successfully\n",
      "  üìä Input tokens: 6,512\n",
      "  ‚ÑπÔ∏è  Context below threshold - no clearing triggered\n",
      "\n",
      "üìù Review 2: SQL query builder\n",
      "üß† Thinking: The user wants me to review this SQL query builder code. Let me first check my memory, then update i...\n",
      "  üîß Memory tool: str_replace /memories/review_progress.md\n",
      "  ‚úì Result: File /memories/review_progress.md has been edited successfully\n",
      "  üìä Input tokens: 7,723\n",
      "  ‚úÇÔ∏è  Context editing triggered!\n",
      "      ‚Ä¢ Cleared 1 thinking turn(s), saved 121 tokens\n",
      "      ‚Ä¢ After clearing: 7,723 tokens\n",
      "\n",
      "üìù Review 3: Web scraper (should trigger clearing)\n",
      "üß† Thinking: The user wants a quick check of this web scraper code. Let me update my memory and provide a concise...\n",
      "  üîß Memory tool: str_replace /memories/review_progress.md\n",
      "  ‚úì Result: File /memories/review_progress.md has been edited successfully\n",
      "  üìä Input tokens: 8,770\n",
      "  ‚úÇÔ∏è  Context editing triggered!\n",
      "      ‚Ä¢ Cleared 2 thinking turn(s), saved 172 tokens\n",
      "      ‚Ä¢ After clearing: 8,770 tokens\n",
      "\n",
      "============================================================\n",
      "‚úÖ Session 3 complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure context management with BOTH clearing strategies\n",
    "# Low thresholds for demo - in production use 30-40k tokens\n",
    "CONTEXT_MANAGEMENT = {\n",
    "    \"edits\": [\n",
    "        # Thinking management MUST come first when combining strategies\n",
    "        {\n",
    "            \"type\": \"clear_thinking_20251015\",\n",
    "            \"keep\": {\"type\": \"thinking_turns\", \"value\": 1}  # Keep only last turn's thinking\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"clear_tool_uses_20250919\",\n",
    "            \"trigger\": {\"type\": \"input_tokens\", \"value\": 5000},  # Low threshold for demo\n",
    "            \"keep\": {\"type\": \"tool_uses\", \"value\": 2},  # Keep last 2 tool uses\n",
    "            \"clear_at_least\": {\"type\": \"input_tokens\", \"value\": 2000}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Extended thinking config (required for clear_thinking strategy)\n",
    "THINKING = {\n",
    "    \"type\": \"enabled\",\n",
    "    \"budget_tokens\": 1024  # Budget for thinking per turn\n",
    "}\n",
    "\n",
    "# Continue from previous session - memory persists!\n",
    "print(\"=\" * 60)\n",
    "print(\"üìö SESSION 3: Long review session with context clearing\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Clean up messages - remove any empty content from previous session\n",
    "# This ensures we have a valid message state to continue from\n",
    "cleaned_messages = []\n",
    "for msg in messages:\n",
    "    if isinstance(msg.get(\"content\"), list):\n",
    "        # Filter out empty content blocks\n",
    "        content = [c for c in msg[\"content\"] if c]\n",
    "        if content:\n",
    "            cleaned_messages.append({\"role\": msg[\"role\"], \"content\": content})\n",
    "    elif msg.get(\"content\"):\n",
    "        cleaned_messages.append(msg)\n",
    "\n",
    "messages = cleaned_messages\n",
    "\n",
    "# Review 1: Data processor (larger file)\n",
    "with open(\"memory_demo/sample_code/data_processor_v1.py\", \"r\") as f:\n",
    "    data_processor_code = f.read()\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"Review this data processor:\\n\\n```python\\n{data_processor_code}\\n```\"\n",
    "})\n",
    "\n",
    "print(\"üìù Review 1: Data processor\")\n",
    "response = run_conversation_turn(\n",
    "    client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    memory_handler=memory,\n",
    "    system=\"You are a code reviewer.\",\n",
    "    context_management=CONTEXT_MANAGEMENT,\n",
    "    thinking=THINKING,\n",
    "    max_tokens=4096,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Add response to messages\n",
    "messages.append({\"role\": \"assistant\", \"content\": response[1]})\n",
    "if response[2]:\n",
    "    messages.append({\"role\": \"user\", \"content\": response[2]})\n",
    "\n",
    "print(f\"  üìä Input tokens: {response[0].usage.input_tokens:,}\")\n",
    "context_cleared, saved = print_context_management_info(response[0])\n",
    "print()\n",
    "\n",
    "# Review 2: SQL code\n",
    "with open(\"memory_demo/sample_code/sql_query_builder.py\", \"r\") as f:\n",
    "    sql_code = f.read()\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"Review this SQL query builder:\\n\\n```python\\n{sql_code}\\n```\"\n",
    "})\n",
    "\n",
    "print(\"üìù Review 2: SQL query builder\")\n",
    "response = run_conversation_turn(\n",
    "    client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    memory_handler=memory,\n",
    "    system=\"You are a code reviewer.\",\n",
    "    context_management=CONTEXT_MANAGEMENT,\n",
    "    thinking=THINKING,\n",
    "    max_tokens=4096,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response[1]})\n",
    "if response[2]:\n",
    "    messages.append({\"role\": \"user\", \"content\": response[2]})\n",
    "\n",
    "print(f\"  üìä Input tokens: {response[0].usage.input_tokens:,}\")\n",
    "context_cleared, saved = print_context_management_info(response[0])\n",
    "print()\n",
    "\n",
    "# Review 3: Add one more review to ensure we trigger clearing\n",
    "with open(\"memory_demo/sample_code/web_scraper_v1.py\", \"r\") as f:\n",
    "    scraper_code = f.read()\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"Quick check - any issues here?\\n\\n```python\\n{scraper_code}\\n```\"\n",
    "})\n",
    "\n",
    "print(\"üìù Review 3: Web scraper (should trigger clearing)\")\n",
    "response = run_conversation_turn(\n",
    "    client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    memory_handler=memory,\n",
    "    system=\"You are a code reviewer.\",\n",
    "    context_management=CONTEXT_MANAGEMENT,\n",
    "    thinking=THINKING,\n",
    "    max_tokens=4096,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response[1]})\n",
    "if response[2]:\n",
    "    messages.append({\"role\": \"user\", \"content\": response[2]})\n",
    "\n",
    "print(f\"  üìä Input tokens: {response[0].usage.input_tokens:,}\")\n",
    "context_cleared, saved = print_context_management_info(response[0])\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Session 3 complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "\n",
    "As context grew during multiple reviews with extended thinking enabled, context editing was applied:\n",
    "1. **Thinking blocks cleared** - Old thinking from previous turns removed first\n",
    "2. **Tool results cleared** - Old memory tool results removed when threshold exceeded\n",
    "3. **Memory files intact** - Claude can still query learned patterns\n",
    "4. **Token usage managed** - Saved tokens from both thinking and tool results\n",
    "\n",
    "This demonstrates the key benefit:\n",
    "- **Short-term memory** (conversation context + thinking) ‚Üí Cleared to save space\n",
    "- **Long-term memory** (stored patterns) ‚Üí Persists across sessions\n",
    "\n",
    "Let's verify memory survived the clearing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T17:13:06.608657Z",
     "iopub.status.busy": "2025-11-15T17:13:06.608274Z",
     "iopub.status.idle": "2025-11-15T17:13:06.613302Z",
     "shell.execute_reply": "2025-11-15T17:13:06.612891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Memory files in demo_memory/:\n",
      "\n",
      "demo_memory/\n",
      "  memories/\n",
      "    ‚îú‚îÄ‚îÄ review_progress.md (673 bytes)\n",
      "\n",
      "‚úÖ All learned patterns preserved despite context clearing!\n"
     ]
    }
   ],
   "source": [
    "# Verify memory persists after context clearing\n",
    "import os\n",
    "\n",
    "print(\"üìÇ Memory files in demo_memory/:\")\n",
    "print()\n",
    "\n",
    "for root, dirs, files in os.walk(\"./demo_memory\"):\n",
    "    # Calculate relative path for display\n",
    "    level = root.replace(\"./demo_memory\", \"\").count(os.sep)\n",
    "    indent = \"  \" * level\n",
    "    folder_name = os.path.basename(root) or \"demo_memory\"\n",
    "    print(f\"{indent}{folder_name}/\")\n",
    "    \n",
    "    sub_indent = \"  \" * (level + 1)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"{sub_indent}‚îú‚îÄ‚îÄ {file} ({size} bytes)\")\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ All learned patterns preserved despite context clearing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How It Works {#how-it-works}\n",
    "\n",
    "### Memory Tool Architecture\n",
    "\n",
    "The memory tool is **client-side** - you control the storage. Claude makes tool calls, your application executes them.\n",
    "\n",
    "#### Memory Tool Commands\n",
    "\n",
    "| Command | Description | Example |\n",
    "|---------|-------------|---------|\n",
    "| `view` | Show directory or file contents | `{\"command\": \"view\", \"path\": \"/memories\"}` |\n",
    "| `create` | Create or overwrite a file | `{\"command\": \"create\", \"path\": \"/memories/notes.md\", \"file_text\": \"...\"}` |\n",
    "| `str_replace` | Replace text in a file | `{\"command\": \"str_replace\", \"path\": \"...\", \"old_str\": \"...\", \"new_str\": \"...\"}` |\n",
    "| `insert` | Insert text at line number | `{\"command\": \"insert\", \"path\": \"...\", \"insert_line\": 2, \"insert_text\": \"...\"}` |\n",
    "| `delete` | Delete a file or directory | `{\"command\": \"delete\", \"path\": \"/memories/old.txt\"}` |\n",
    "| `rename` | Rename or move a file | `{\"command\": \"rename\", \"old_path\": \"...\", \"new_path\": \"...\"}` |\n",
    "\n",
    "See `memory_tool.py` for the complete implementation with path validation and security measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking Management (`clear_thinking_20251015`)\n",
    "\n",
    "When using extended thinking, thinking blocks accumulate and consume tokens. The `clear_thinking` strategy manages these automatically.\n",
    "\n",
    "**Important**: This strategy requires `thinking` to be enabled in your API call.\n",
    "\n",
    "**API Call Pattern** (with extended thinking enabled):\n",
    "\n",
    "```python\n",
    "response = client.beta.messages.create(\n",
    "    betas=[\"context-management-2025-06-27\"],  # Required beta flag\n",
    "    model=\"claude-sonnet-4-5\",\n",
    "    messages=messages,\n",
    "    tools=[{\"type\": \"memory_20250818\", \"name\": \"memory\"}],\n",
    "    thinking={\"type\": \"enabled\", \"budget_tokens\": 10000},  # Enable thinking\n",
    "    context_management={  # Context editing config\n",
    "        \"edits\": [\n",
    "            {\n",
    "                \"type\": \"clear_thinking_20251015\",\n",
    "                \"keep\": {\"type\": \"thinking_turns\", \"value\": 1}  # Keep last turn only\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"clear_tool_uses_20250919\",\n",
    "                \"trigger\": {\"type\": \"input_tokens\", \"value\": 35000},\n",
    "                \"keep\": {\"type\": \"tool_uses\", \"value\": 5}\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    max_tokens=2048\n",
    ")\n",
    "```\n",
    "\n",
    "**Key points:**\n",
    "- `clear_thinking` must come **first** when combining strategies\n",
    "- Requires extended thinking to be enabled (`thinking={\"type\": \"enabled\", ...}`)\n",
    "- Use `\"keep\": \"all\"` to preserve all thinking blocks for maximum cache hits\n",
    "- Trigger is optional for thinking (clears based on `keep` value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Demo Code\n",
    "\n",
    "Key implementation details from `code_review_demo.py`:\n",
    "\n",
    "```python\n",
    "class CodeReviewAssistant:\n",
    "    def __init__(self, memory_storage_path=\"./memory_storage\"):\n",
    "        self.client = Anthropic()\n",
    "        self.memory_handler = MemoryToolHandler(base_path=memory_storage_path)\n",
    "        self.messages = []\n",
    "    \n",
    "    def review_code(self, code, filename, description=\"\"):\n",
    "        # 1. Add user message\n",
    "        self.messages.append({...})\n",
    "        \n",
    "        # 2. Conversation loop with tool execution\n",
    "        while True:\n",
    "            response = self.client.beta.messages.create(\n",
    "                model=MODEL,\n",
    "                system=self._create_system_prompt(),\n",
    "                messages=self.messages,\n",
    "                tools=[{\"type\": \"memory_20250818\", \"name\": \"memory\"}],\n",
    "                betas=[\"context-management-2025-06-27\"],\n",
    "                context_management=CONTEXT_MANAGEMENT\n",
    "            )\n",
    "            \n",
    "            # 3. Execute tool uses\n",
    "            tool_results = []\n",
    "            for content in response.content:\n",
    "                if content.type == \"tool_use\":\n",
    "                    result = self._execute_tool_use(content)\n",
    "                    tool_results.append({...})\n",
    "            \n",
    "            # 4. Continue if there are tool uses, otherwise done\n",
    "            if tool_results:\n",
    "                self.messages.append({\"role\": \"user\", \"content\": tool_results})\n",
    "            else:\n",
    "                break\n",
    "```\n",
    "\n",
    "**The key pattern**: Keep calling the API while there are tool uses, executing them and feeding results back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Claude Actually Learns\n",
    "\n",
    "This is what makes memory powerful - **semantic pattern recognition**, not just syntax:\n",
    "\n",
    "**Session 1: Thread-Based Web Scraper**\n",
    "\n",
    "```python\n",
    "# Bug: Race condition\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.results = []  # Shared state!\n",
    "    \n",
    "    def scrape_urls(self, urls):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            for future in as_completed(futures):\n",
    "                self.results.append(future.result())  # RACE!\n",
    "```\n",
    "\n",
    "**What Claude Stores in Memory** (example file: `/memories/concurrency_patterns/thread_safety.md`):\n",
    "\n",
    "When Claude encounters this pattern, it stores the following insights to its memory files:\n",
    "- **Symptom**: Inconsistent results in concurrent operations\n",
    "- **Cause**: Shared mutable state (lists/dicts) modified from multiple threads\n",
    "- **Solution**: Use locks, thread-safe data structures, or return results instead\n",
    "- **Red flags**: Instance variables in thread callbacks, unused locks, counter increments\n",
    "\n",
    "---\n",
    "\n",
    "**Session 2: Async API Client** (New conversation!)\n",
    "\n",
    "Claude checks memory FIRST, finds the thread-safety pattern, then:\n",
    "1. **Recognizes** similar pattern in async code (coroutines can interleave too)\n",
    "2. **Applies** the solution immediately (no re-learning needed)\n",
    "3. **Explains** with reference to stored knowledge\n",
    "\n",
    "```python\n",
    "# Claude spots this immediately:\n",
    "async def fetch_all(self, endpoints):\n",
    "    for coro in asyncio.as_completed(tasks):\n",
    "        self.responses.append(await coro)  # Same pattern!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Why This Matters:**\n",
    "\n",
    "- ‚ùå **Syntax checkers** miss race conditions entirely\n",
    "- ‚úÖ **Claude learns** architectural patterns and applies them across contexts\n",
    "- ‚úÖ **Cross-language**: Pattern applies to Go, Java, Rust concurrency too\n",
    "- ‚úÖ **Gets better**: Each review adds to the knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Code Files\n",
    "\n",
    "The demo uses these sample files (all have concurrency/thread-safety bugs):\n",
    "\n",
    "- `memory_demo/sample_code/web_scraper_v1.py` - Race condition: threads modifying shared state\n",
    "- `memory_demo/sample_code/api_client_v1.py` - Similar concurrency bug in async context\n",
    "- `memory_demo/sample_code/data_processor_v1.py` - Multiple concurrency issues for long session demo\n",
    "\n",
    "Let's look at one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`memory_demo/sample_code/web_scraper_v1.py`**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Concurrent web scraper with a race condition bug.\n",
    "Multiple threads modify shared state without synchronization.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "class WebScraper:\n",
    "    \"\"\"Web scraper that fetches multiple URLs concurrently.\"\"\"\n",
    "\n",
    "    def __init__(self, max_workers: int = 10):\n",
    "        self.max_workers = max_workers\n",
    "        self.results = []  # BUG: Shared mutable state accessed by multiple threads!\n",
    "        self.failed_urls = []  # BUG: Another race condition!\n",
    "\n",
    "    def fetch_url(self, url: str) -> Dict[str, any]:\n",
    "        \"\"\"Fetch a single URL and return the result.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            response.raise_for_status()\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"status\": response.status_code,\n",
    "                \"content_length\": len(response.content),\n",
    "            }\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return {\"url\": url, \"error\": str(e)}\n",
    "\n",
    "    def scrape_urls(self, urls: List[str]) -> List[Dict[str, any]]:\n",
    "        \"\"\"\n",
    "        Scrape multiple URLs concurrently.\n",
    "\n",
    "        BUG: self.results is accessed from multiple threads without locking!\n",
    "        This causes race conditions where results can be lost or corrupted.\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = [executor.submit(self.fetch_url, url) for url in urls]\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "\n",
    "                # RACE CONDITION: Multiple threads append to self.results simultaneously\n",
    "                if \"error\" in result:\n",
    "                    self.failed_urls.append(result[\"url\"])  # RACE CONDITION\n",
    "                else:\n",
    "                    self.results.append(result)  # RACE CONDITION\n",
    "\n",
    "        return self.results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bug**: Multiple threads modify `self.results` and `self.failed_urls` without locking!\n",
    "\n",
    "Claude will:\n",
    "1. Identify the race conditions\n",
    "2. Store the pattern in `/memories/concurrency_patterns/thread_safety.md`\n",
    "3. Apply this concurrency pattern to async code in Session 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo Overview\n",
    "\n",
    "We've built a complete Code Review Assistant. The implementation is in `memory_demo/code_review_demo.py`.\n",
    "\n",
    "**To run the interactive demo:**\n",
    "```bash\n",
    "python memory_demo/code_review_demo.py\n",
    "```\n",
    "\n",
    "The demo demonstrates:\n",
    "1. **Session 1**: Review Python code with a bug ‚Üí Claude learns the pattern\n",
    "2. **Session 2**: Review similar code (new conversation) ‚Üí Claude applies the pattern\n",
    "3. **Session 3**: Long review session ‚Üí Context editing keeps it manageable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices & Security {#best-practices}\n",
    "\n",
    "### Memory Management\n",
    "\n",
    "**Do:**\n",
    "- ‚úÖ Store task-relevant patterns, not conversation history\n",
    "- ‚úÖ Organize with clear directory structure\n",
    "- ‚úÖ Use descriptive file names\n",
    "- ‚úÖ Periodically review and clean up memory\n",
    "\n",
    "**Don't:**\n",
    "- ‚ùå Store sensitive information (passwords, API keys, PII)\n",
    "- ‚ùå Let memory grow unbounded\n",
    "- ‚ùå Store everything indiscriminately\n",
    "\n",
    "### Security: Path Traversal Protection\n",
    "\n",
    "**Critical**: Always validate paths to prevent directory traversal attacks. See `memory_tool.py` for implementation.\n",
    "\n",
    "### Security: Memory Poisoning\n",
    "\n",
    "**‚ö†Ô∏è Critical Risk**: Memory files are read back into Claude's context, making them a potential vector for prompt injection.\n",
    "\n",
    "**Mitigation strategies:**\n",
    "1. **Content Sanitization**: Filter dangerous patterns before storing\n",
    "2. **Memory Scope Isolation**: Per-user/per-project isolation  \n",
    "3. **Memory Auditing**: Log and scan all memory operations\n",
    "4. **Prompt Engineering**: Instruct Claude to ignore instructions in memory\n",
    "\n",
    "See `memory_tool.py` for complete security implementation and tests in `tests/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "In this cookbook, you learned to:\n",
    "- ‚úÖ **Implement the memory tool** for cross-conversation learning (Sessions 1 & 2 showed pattern recognition persisting)\n",
    "- ‚úÖ **Configure context editing** with token triggers and retention policies (Session 3 demonstrated automatic clearing)\n",
    "- ‚úÖ **Apply security best practices** including path validation and memory poisoning prevention\n",
    "\n",
    "### Applying These Patterns\n",
    "\n",
    "**For your projects:**\n",
    "1. Start with a single memory file for patterns (e.g., `/memories/patterns.md`)\n",
    "2. Set context editing triggers at 30-40k tokens for production use\n",
    "3. Implement per-project memory isolation to prevent cross-contamination\n",
    "\n",
    "**Other applications:**\n",
    "- **Customer support**: Store user preferences and common issue resolutions\n",
    "- **Research assistants**: Accumulate domain knowledge across sessions\n",
    "- **Data analysis**: Remember dataset characteristics and successful techniques\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Production deployment**: Use [claude-code-action](https://github.com/anthropics/claude-code-action) for GitHub PR reviews\n",
    "- **Security hardening**: Review the memory poisoning mitigations in `memory_tool.py`\n",
    "- **Extended thinking**: Explore thinking management for compute-intensive tasks\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Memory tool documentation](https://docs.claude.com/en/docs/agents-and-tools/tool-use/memory-tool)\n",
    "- [Claude API reference](https://docs.claude.com/en/api/messages)\n",
    "- [Support](https://support.claude.com)\n",
    "\n",
    "Memory and context management are in **beta**. Share your feedback to help us improve!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
